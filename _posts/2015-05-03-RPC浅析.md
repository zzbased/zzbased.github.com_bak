---
layout: post
title: "RPC浅析"
---

# RPC浅析

## Protobuf简介

### 自描述消息

生产者：产生消息，填充内容，并序列化保存

消费者：读取数据，反序列化得到消息，使用消息

目的：解除这种耦合，让消费者能动态的适应消息格式的变换。

生产者把定义消息格式的.proto文件和消息作为一个完整的消息序列化保存，完整保存的消息我称之为Wrapper message，原来的消息称之为payload message。

消费者把wrapper message反序列化，先得到payload message的消息类型，然后根据类型信息得到payload message，最后通过反射机制来使用该消息。

	message SelfDescribingMessage {
  	// Set of .proto files which define the type.
  	required FileDescriptorSet proto_files = 1;

  	// Name of the message type.  Must be defined by one of the files in
  	// proto_files.
  	required string type_name = 2;

  	// The message data.
  	required bytes message_data = 3;
	}

@Todo

### Reflection
- [一种自动反射消息类型的 Google Protobuf 网络传输方案](http://blog.csdn.net/solstice/article/details/6300108)

### 动态自描述消息

### 参考资料
- [Protobuf](https://github.com/google/protobuf)
- [玩转Protobuf](http://www.searchtb.com/2012/09/protocol-buffers.html)
- [Self-describing Messages](https://developers.google.com/protocol-buffers/docs/techniques?hl=zh-CN#self-description)

## RPC

- [使用google protobuf RPC实现echo service](http://www.codedump.info/?p=169)
- [基于protobuf的RPC实现](http://codemacro.com/2014/08/31/protobuf-rpc/)
- [RPC框架系列——Protocol Buffers](http://jeoygin.org/2011/09/rpc-framework-protocol-buffers.html)

- [Poppy]()

### GDT RPC代码解析

#### 公共代码

**echo_service**

echo_service.proto:

  service EchoService {
    option (gdt.qzone_protocol_version) = 1;
    rpc Echo(EchoRequest) returns (EchoResponse) {
      option (gdt.qzone_protocol_cmd) = 10;
    }
    rpc FormTest(FormTestMessage) returns(FormTestMessage);
  }


protoc编译后：

  class EchoService : public ::google::protobuf::Service {
   protected:
    // This class should be treated as an abstract interface.
    inline EchoService() {};
   public:
    virtual ~EchoService();

    typedef EchoService_Stub Stub;

    static const ::google::protobuf::ServiceDescriptor* descriptor();
    // 下面两个是虚函数,需要在子类实现
    virtual void Echo(::google::protobuf::RpcController* controller,
                         const ::gdt::rpc_examples::EchoRequest* request,
                         ::gdt::rpc_examples::EchoResponse* response,
                         ::google::protobuf::Closure* done);
    virtual void FormTest(::google::protobuf::RpcController* controller,
                         const ::gdt::rpc_examples::FormTestMessage* request,
                         ::gdt::rpc_examples::FormTestMessage* response,
                         ::google::protobuf::Closure* done);

    // implements Service ----------------------------------------------

    const ::google::protobuf::ServiceDescriptor* GetDescriptor();
    void CallMethod(const ::google::protobuf::MethodDescriptor* method,
                    ::google::protobuf::RpcController* controller,
                    const ::google::protobuf::Message* request,
                    ::google::protobuf::Message* response,
                    ::google::protobuf::Closure* done);
    const ::google::protobuf::Message& GetRequestPrototype(
      const ::google::protobuf::MethodDescriptor* method) const;
    const ::google::protobuf::Message& GetResponsePrototype(
      const ::google::protobuf::MethodDescriptor* method) const;

   private:
    GOOGLE_DISALLOW_EVIL_CONSTRUCTORS(EchoService);
  };

  class EchoService_Stub : public EchoService {
   public:
    EchoService_Stub(::google::protobuf::RpcChannel* channel);
    EchoService_Stub(::google::protobuf::RpcChannel* channel,
                     ::google::protobuf::Service::ChannelOwnership ownership);
    ~EchoService_Stub();

    inline ::google::protobuf::RpcChannel* channel() { return channel_; }

    // implements EchoService ------------------------------------------

    void Echo(::google::protobuf::RpcController* controller,
                         const ::gdt::rpc_examples::EchoRequest* request,
                         ::gdt::rpc_examples::EchoResponse* response,
                         ::google::protobuf::Closure* done);
    void FormTest(::google::protobuf::RpcController* controller,
                         const ::gdt::rpc_examples::FormTestMessage* request,
                         ::gdt::rpc_examples::FormTestMessage* response,
                         ::google::protobuf::Closure* done);
   private:
    ::google::protobuf::RpcChannel* channel_;
    bool owns_channel_;
    GOOGLE_DISALLOW_EVIL_CONSTRUCTORS(EchoService_Stub);
  };

  // 客户端实际调用的是RpcChannel的CallMethod
  void EchoService_Stub::Echo(::google::protobuf::RpcController* controller,
                                const ::gdt::rpc_examples::EchoRequest* request,
                                ::gdt::rpc_examples::EchoResponse* response,
                                ::google::protobuf::Closure* done) {
    channel_->CallMethod(descriptor()->method(0),
                         controller, request, response, done);
  }
  void EchoService_Stub::FormTest(::google::protobuf::RpcController* controller,
                                const ::gdt::rpc_examples::FormTestMessage* request,
                                ::gdt::rpc_examples::FormTestMessage* response,
                                ::google::protobuf::Closure* done) {
    channel_->CallMethod(descriptor()->method(1),
                         controller, request, response, done);
  }


**system/io_frame/net_options.h**

  setsockopt(fd, SOL_SOCKET, SO_RCVBUF, &buffer_size, static_cast<socklen_t>(sizeof(buffer_size)))
  setsockopt(fd, SOL_SOCKET, SO_SNDBUF, &buffer_size, static_cast<socklen_t>(sizeof(buffer_size)))

[What are SO_SNDBUF and SO_RECVBUF](http://stackoverflow.com/questions/4257410/what-are-so-sndbuf-and-so-recvbuf)


#### 客户端代码

**rpc/client_connection**

// Client side rpc connection base class.
class ClientConnection : public BaseConnection

用来在客户端建立连接，读取数据，发送数据等。

**rpc/rpc_channel**

RpcChannelInterface : public ::google::protobuf::RpcChannel

void CallMethod(
      const google::protobuf::MethodDescriptor* method,
      google::protobuf::RpcController* controller,
      const google::protobuf::Message* request,
      google::protobuf::Message* response,
      google::protobuf::Closure* done);

发送请求的背后,最后调用的其实是RpcChannel的CallMethod函数.所以,要实现RpcChannel类,最关键的就是要实现这个函数,在这个函数中完成发送请求的事务.

明天尝试一下，自己写好proto后，用protoc编译后，输出是什么样子的？

rpc_channel.h 里面是空的，没啥东西

**rpc/rpc_controller**

rpc_controller是一个rpc请求过程中的信息。

class RpcController : public google::protobuf::RpcController

主要保存下面这些信息：

  int error_code_;
  std::string reason_;
  int timeout_;
  SocketAddressStorage remote_address_;
  int64_t timestamp_;
  bool in_use_;
  kDefaultTimeout = 2000ms;

**rpc/load_balance**

./rpc/load_balancer，LoadBalancer是一个单例。

下面实现了4种load_balancer：

rpc/domain_load_balancer
rpc/l5_load_balancer
rpc/list_load_balancer
rpc/single_load_balancer

**rpc/RpcClient**

RpcClient是客户端的主类。一般情况下，一个客户端只需要有一个RpcClient。在初始化的时候，也可以设置线程个数，此个数等于PollThread的个数(多路器的个数)。

利用RpcClient::OpenChannel创建RpcChannel。先根据scheme(目前有qzone,ckv,http三种)创建对应的Factory：
RpcChannelFactory* factory = GDT_RPC_GET_CHANNEL_FACTORY(scheme)。再利用factory创建channel:  shared_ptr<RpcChannelInterface> channel_impl(factory->CreateChannel(multiplexers_, server, NetOptions()))。

创建channel时，先调用RpcChannel::Open。

这里注册了三个Channel以及Factory
./rpc/ckv_client_channel.cc:22:GDT_RPC_REGISTER_CHANNEL("ckv", CkvClientChannel);
./rpc/http_rpc_channel.cc:209:GDT_RPC_REGISTER_CHANNEL("http", HttpRpcChannel);
./rpc/qzone_client_channel.cc:269:GDT_RPC_REGISTER_CHANNEL("qzone", QzoneClientChannel);

**rpc/ClientConnection**

class  : public BaseConnection
class QzoneClientConnection : public ClientConnection
class CkvClientConnection : public ClientConnection

**代码流程**

rpc client里发起请求，内部调用的都是RpcChannel。

  Closure* done = ::NewCallback(this, &TestClient::AsyncCallDone, i,
                                controller, request, response);
  EchoService::Stub stub(channels_[i % channels_.size()].get());

class HttpRpcChannel : public RpcChannelImpl

如果是http请求，则Stub调用的是 HttpRpcChannel::CallMethod。根据是否有done回调函数，分为同步和异步。

CallMethod实际调用的是RpcChannelImpl::Call(context). Call函数里，先获取到connection。

shared_ptr<ClientConnection> connection = GetRoute(call_context);

GetRoute里首先判断是否已有connected_，如果没有新需要新建立连接。
result.reset(NewConnection())
(!result->Open((*multiplexers_)[index].get(), address, options_))

http_rpc_channel里，实现的是class HttpRpcConnection : public ClientConnection。

在ClientConnection::Open函数里，调用了NonblockingConnect。

OK，这下请求就算发送过去了。明天再把同步和异步搞清楚就OK了。

---

![](images/client_async_call.png)

channel_number 这个设置主要是为什么？多一点有什么好处？channel是socket connect的个数。

在客户端和服务端设置的threads number是PollThread的个数。channel number可以大于thread number。根据epoll机制，一个thread都可以支撑多个channel。


一个channel里持有很多连接的map，可以共享连接。

只要channel没有新建，则连接一直保留，所以这时是长连接。


创建连接：

![](images/client_build_connection.png)

#### 服务端代码

**system/io_frame/Multiplexer**

常见的多路复用有：PPC(Process Per Connection)，TPC(Thread PerConnection)，这些模型的缺陷是： resource usage and context-switching time influence the ability to handle many clients at a time。

select的缺点：

- 最大并发数限制。一个进程所打开的FD（文件描述符）是有限制的，由FD_SETSIZE设置。
- 效率问题，select每次调用都会线性扫描全部的FD集合。O(n)复杂度。
- 内核/用户空间的内存拷贝问题。通过内存拷贝让内核把FD消息通知给用户空间。

poll解决了第一个缺点，但第二，三个缺点依然存在。

epoll是一个相对完美的解决方案。(1)最大FD个数很大(由/proc/sys/fs/file-max给出)；(2)epoll不仅会告诉应用程序有I/0事件到来，还会告诉应用程序相关的信息，不用遍历；(3)内核与用户态传递消息使用共享内存；

epoll里还有一个level triggered和edge triggered的区分，具体请参考[the purpose of epoll's edge triggered option](http://stackoverflow.com/questions/9162712/what-is-the-purpose-of-epolls-edge-triggered-option)

- [Linux Epoll介绍和程序实例](http://blog.csdn.net/sparkliang/article/details/4770655)
- [How to use epoll? A complete example in C](https://banu.com/blog/2/how-to-use-epoll-a-complete-example-in-c/)

分发器/多路器Multiplexer其中主要是通过epoll实现分发。

主要函数有：Create，Poll，AddDescriptor，RemoveDescriptor，ModifyEvent，RegisterTimer等。RegisterTimer可以用来注册一个定时任务，这在某些场景还是蛮有用的。

调用过AddDescriptor的文件有：

	./net/http/client/connection.cc:214:  multiplexer()->AddDescriptor(this, Multiplexer::kIoEventReadWrite);
	./net/http/server/http_server.cc:246:  if (!multiplexer->AddDescriptor(connection.get())) {
	./net/http/server/listener.cc:123:  multiplexer->AddDescriptor(listener.get());
	./rpc/client_connection.cc:208:    multiplexer()->AddDescriptor(this, events);

Descriptor是FD描述类，其中持有成员变量fd以及close_callback_list，以及两个重要方法：OnWritable，OnReadable。这两个方法在连接时会被回调。

// MultiplexerNotifier is used to wake up epoll_wait
class MultiplexerNotifier : public Descriptor

Multiplexer持有成员变量MultiplexerNotifier。

更多Multiplexer的用法请参考：multiplexer_test。还解释了一个疑问：Poll函数的参数，是epoll_wait的timeout时间。

即便每个multiplexer不监听socket，但都会create一个fd来用notify。

**system/io_frame/poll_thread**

PollThread类是结合Multiplexer一起使用的，即Thread + Multiplexer。也就是每个PollThread，都在loop multiplexer，如果有事件，就处理。

**system/io_frame/base_connection**

base_connection继承自Descriptor。是connection基类：负责单次网络io。

**net/http_server**

uri: 是用来解析url的一个工具类。

譬如对于url:http://www.baidu.com/s?tn=monline_dg&bs=DVLOG&f=8&wd=glog+DVLOG#fragment，利用Uri类，可以解析出Host,port,Scheme,Fragment,Query等参数。

对于Query参数：tn=monline_dg&bs=DVLOG&f=8&wd=glog+DVLOG，可以利用QueryParams类快速解析出每个name所对应的value。

HttpServer常用的方法有：RegisterHttpHandler，RegisterPrefixHandler，Forward，RegisterStaticResource，AddLinkOnIndexPage等函数。

看一下http_server_test，里面有一些不错的例子，我可以写一个test_server，实际试一下。

**rpc/rpc_server**

class RpcServer : public HttpServer

在HttpServer的基础上，又新增了几个方法：RegisterService，RegisterJceProtoService(注册jce服务)。这些注册服务无非就是把service插入到成员变量vector中。

**rpc/rpc_service_register**

其中主要有三个方法：

// 用于qzone协议，qzone+pb
const QzoneMethodContext* FindQzoneMethodContext(int qzone_version, int qzone_cmd) const;
// 用于qzone+jce协议
const JceMethodContext* FindJceMethodContext(int qzone_version, int qzone_cmd) const;
// 用于protobuf
bool FindMethodByName(const std::string& full_name,
      google::protobuf::Service** service,
      const google::protobuf::MethodDescriptor** method_descriptor,
      RpcErrorCode* error_code) const;

**net/http/http_handler**

HttpHandlerRegistry里持有两个成员变量：handler_map_，prefix_map_。常用的方法有：两个Register，一个Find。Find访问的是HttpHandler。

HttpHandler。它最重要的函数是：HandleRequest。具体实现为：

  if (callback_) {
    HttpRequest* request_copy = new HttpRequest();
    request_copy->Swap(request);
    HttpResponse* response = new HttpResponse();
    shared_ptr<HttpServerConnection> shared_connection = connection->shared();
    Closure* done = NewCallback(OnResponseDone, shared_connection,
                                request_copy, response, response_modifier);
    callback_->Run(connection, request_copy, response, done);
  } else {
    HttpResponse response;
    simple_callback_->Run(request, &response);
    if (response_modifier)
      response_modifier->Run(request, &response);
    PreprocessResponse(request, &response);
    connection->SendResponse(response);
  }

HttpServerConnection继承自BaseConnection(Descriptor)。初始化的时候必须传入的参数有：fd, multiplexer, handler_registry。

HttpServerConnection什么时候被初始化呢？在HttpServer的OnAccept函数里，每从listen_socket获取一个fd，则初始化一个connection。handler_registry来自于HttpServer持有的成员变量HttpHandlerRegistry，multiplexer则是根据fd从multiplexers_按规则取出一个。

  int index = fd % num_threads_;
  Multiplexer* multiplexer = multiplexers_[index].get();

**执行流程**

1. 生成RpcServer::Options参数，这里可以指定服务端的poll_thread(即multiplexer)的线程数。
2. new RpcServer，因为RpcServer继承自HttpServer，HttpServer根据指定的threadnum(n)创建了n个multiplexer，并保存在multiplexers_。
   再接着RegisterDefaultPaths，背后调用的是handler_registry_->Register。
3. 给RpcServer注册服务。Register Proto/jce service是RpcServer的独有方法，RegisterHttpHander是HttpServer的方法。这里的注册只是将service存储在类成员变量vector里。

  server.RegisterService(echo.get());
  server.RegisterHttpHandler("/test", NewPermanentCallback(TestPage));

protobuf的相关处理由rpc_http_handler.cc RpcHttpHandler完成。

    shared_ptr<HttpHandler> handler(
        new RpcHttpHandler(rpc_service_registry_.get(),
                           rpc_service_stats_.get()));
    RegisterPrefixHandler(kHttpRpcPrefix, handler);

其他http处理由http_hander完成。


4. 执行server.Listen(FLAGS_ip, FLAGS_port)。在此之前，先执行RpcServer.BeforeListen()。这里会创建一个RpcServiceRegistry。把builtin服务和上一步注册的服务都写到RpcServiceRegistry。

接着再调用HttpServerListener::Listen，将HttpServerListener添加到multiplexer[0]，即multiplexer->AddDescriptor(listener.get())。并且把HttpServer::OnAccept注册为Listen的回调函数accept_callback_。

HttpServerListener继承于Descriptor，当multiplexer[0]发现有listen_fs有IoEvent时，会调用HttpServerListener::OnReadable()。

5. server.Start()，启动多个PollThread。每个PollThread的Loop函数，multiplexer都执行Poll()。

6. 最后是while (!server.IsStopped()){}。如果有client请求接入，最先会调用HttpServerListener::OnReadable()函数，这里会先调用accept函数，得到新连接的fd，再调用accept_callback_->Run(fd)。

void HttpServer::OnAccept(int fd) {
  int index = fd % num_threads_;
  Multiplexer* multiplexer = multiplexers_[index].get();
  shared_ptr<Descriptor> connection = MakeConnection(fd, multiplexer);
  if (!connection) {
    close(fd);
    return;
  }
  if (!multiplexer->AddDescriptor(connection.get())) {
    PLOG(WARNING) << "Add socket to poll failed, fd=" << fd;
    return;
  }
  connection_manager_->Add(connection);
  connection->PushCloseCallback(
      NewCallback(connection_manager_.get(), &ServerConnectionManager::Remove,
                  connection));
}

上面代码把新连接的fd也加入到multiplexer的监听中。

7. 如果有数据可以读入，则将调用RpcServerConnection::OnReadable函数，首先判断是qzone协议，还是http协议。如果是qzone协议(protobuf or jce)，先注册RpcServerConnection::OnRequestDone()为done的回调函数，并创建QzoneServiceHandler。

再调用HttpServerConnection::OnReadable函数，即BaseConnection::OnReadable()函数。

在OnReadable()中，再调用BaseConnection::ReadPackets。

GetNextSendingBuffer,GetPacketSize,OnPacketReceived,OnEofReceived是BaseConnection的纯虚函数。RpcServerConnection对这些纯虚函数进行了重写。

int RpcServerConnection::GetPacketSize(const StringPiece& buffer) {
  return handler_ ? GetQzonePacketSize(buffer) :
                    HttpServerConnection::GetPacketSize(buffer);
}

譬如上面，如果发现是qzone协议的话，则调用的是GetQzonePacketSize。

![](images/rpc_method_backtrace.png)

![](images/rpc_method_write.png)

![](images/rpc_method_requestdone.png)

8.

./system/io_frame/base_connection.cc:147:  int n = read(fd(), buffer, buffer_size);
./system/io_frame/base_connection.cc:108:  int n = send(fd(), buffer, buffer_size, flags);

![](images/read_packet.png)

![](images/write_packet.png)

客户端和服务端都是这个脉路。

**TODO**
记录几个点：jce是怎么处理的，qzone是怎么处理的。

1. 首先有jce格式，再定义一个proto格式。
2. 实现下面三个的代码

bool JceStructToPbMessage(const QZAP_NEW::wup_qzap_search_display_req* jce,
                          MixerRequest* request);
bool PbMessageToJceStruct(const MixerResponse* response,
                          QZAP_NEW::wup_qzap_search_display_rsp* jce);

GDT_RPC_DEFINE_JCE_PROTO_SERVICE(0, JceMixerService, MixerService,
  GDT_RPC_JCE_METHOD(0, SearchAd, QZAP_NEW::wup_qzap_search_display_req,
                     QZAP_NEW::wup_qzap_search_display_rsp)

3. 也就是说，如果是jce服务，rpc client会先将jce转成proto，再发起rpc调用，最后回报的时候再从proto解析为jce。
4. 如果是qzone服务，其实理论上它也是header+proto，那么在接受包的时候，判断是qzone协议的话，先把包头去掉。

feeds的rpc服务可以多学习一下。

adx的action, state与feeds的action, state以后也可以多学习学习！

### 参考资料

[What does the explicit keyword in C++ mean?](http://stackoverflow.com/questions/121162/what-does-the-explicit-keyword-in-c-mean)

[final specifier - C++ Reference](http://en.cppreference.com/w/cpp/language/final)

### PPT分享大纲

GDT RPC编程实践

- protobuf
- protobuf的反射机制
- socket编程，参数设置 [socket option](http://man7.org/linux/man-pages/man7/socket.7.html) socketoptions.h/cc里面的实现也看看
- 基于protobuf的rpc最简单实现 两个优点：简化client-server交互，就像在调用一个本地方法；通过Protobuf实现多种编程语言之间的交互。
get all the advantages of working with protocol buffers, including efficient serialization, a simple IDL, and easy interface updating.
- gdt_rpc实现
- 怎么处理jce,qzone,ckv等
- 怎么利用gdt rpc实现state和action
- google rpc [grpc](http://www.grpc.io)
- 最后编程进行时

下周的ppt，就讲这些内容~

还找到一个bug，明天提交一下。


- ProtoMessageToJsonValue。把代码截图一下。reflection。
- rpc server 回调done函数
- Protobuf memory内存的使用。 [Protobuf使用不当导致的程序内存上涨问题](http://qa.baidu.com/blog/?p=1179)
protobuf的clear操作适合于清理那些数据量变化不大的数据，对于大小变化较大的数据是不适合的，需要定期（或每次）进行delete操作。建议swap或者delete。
- [protobuf addons](https://github.com/google/protobuf/wiki/Third-Party-Add-ons)
- [rpc definition](https://developers.google.com/protocol-buffers/docs/reference/cpp-generated)
- [protobuf中会严重影响时间和空间损耗的地方 ](http://blog.chinaunix.net/uid-26922071-id-3723751.html)
repeated的性能问题。对于普通数据类型，在2^n+1时重新分配内存空间，而对于message数据，在2^n+1是分配对象地址空间，但每次都是new一个对象，这样就很损耗性能了。
- 把自描述的代码写完。然后就可以测试了。
- GDT的rpc:除了二进制协议外，Poppy支持还以普通的HTTP协议，传输以JSON/protobuf文本格式定义的消息。很方便用各种脚本语言调用，甚至用 bash，调 wget/curl 都能发起 RPC 调用。
Poppy的二进制协议与一般的设计不一样的是，它是以HTTP协议头为基础建立起来的，只是建立连接后的最初的采用HTTP协议，后续的消息往来直接用二进制协议，所以效率还是比较高的。
[poppy的官方资料](http://djt.qq.com/article/view/327)


handler列表：

./net/http/server/forward_handler.h
./net/http/server/gflags_handler.h
./net/http/server/http_handler.h
./net/http/server/static_resource_handler.h

下面一个处理qzone协议，一个处理http协议。主要是在server角度，接受数据，解包，调用实际service，封包等操作。

./rpc/qzone_service_handler.h
./rpc/rpc_http_handler.h

connection列表：

./net/http/client/connection.h
./rpc/client_connection.h
./system/io_frame/base_connection.h 这个是基类

两个connection类，主要是用来处理服务端的socket连接。OnReadable, OnWritable。
RpcServerConnection::OnReadable主要做了对http和qzone协议的区分，然后如果是http协议，则主要调用HttpServerConnection，如果是qzone协议，则主要调用QzoneServiceHandler里的方法。

class RpcServerConnection : public HttpServerConnection
./net/http/server/connection.h
./rpc/rpc_server_connection.h


service列表：
这个里面用途不大
./rpc/ckv_service.h
./rpc/jce_proto_service.h
./rpc/jce_service.h
./rpc/rpc_builtin_service.h
./rpc/udp_rpc_service.h

protocol列表：

./rpc/http_rpc_protocol.h
./rpc/rpc_qzone_protocol.h

客户端connection:

./client_connection.h:95:class ClientConnection : public BaseConnection
./ckv_client_channel.h:23:class CkvClientConnection : public ClientConnection
./http_rpc_channel.h:24:class HttpRpcConnection : public ClientConnection
./qzone_client_channel.h:22:class QzoneClientConnection : public ClientConnection
./rpc_channel_impl.h:42:  virtual ClientConnection* NewConnection() = 0;

客户端channel:

channel这边主要还是基于 BaseConnection这个在做。还是那两个入口函数，read和write。
ClientConnection里面会调用RpcClientCallContext。

./rpc_channel_impl.h:26:class RpcChannelImpl : public RpcChannelInterface {
./qzone_client_channel.h:35:class QzoneClientChannel : public RpcChannelImpl {
./http_rpc_channel.h:42:class HttpRpcChannel : public RpcChannelImpl {
./ckv_client_channel.h:33:class CkvClientChannel: public RpcChannelImpl {

客户端balancer列表，主要来做负载均衡。

./rpc/domain_load_balancer.h
./rpc/l5_load_balancer.h
./rpc/list_load_balancer.h
./rpc/load_balancer.h
./rpc/single_load_balancer.h

注意CKV这里的搞法是：

复用了客户端的multiplexer，继承自RpcClientCallContext。
客户端主要负责写两个函数：EncodeRequest，OnPacketReceived。
EncodeRequest这个函数是对消息做打包。OnPacketReceived是对消息做解包。

还要注意，同步调用和异步调用的区别。
异步调用的话，纯粹就是复用了multiplexer的功能，做回调。
而同步的话，其实就是调用后，做一个响应的等待，利用的是AutoResetEvent, "./system/concurrency/event.h"。

持有的是长连接通路。看这个函数就知道了：shared_ptr<ClientConnection> RpcChannelImpl::GetRoute

刚才又看了一遍 jeff的rpc，再看起来，感觉非常清晰了。
还是有帮助的。至少这下搞了，这些rpc的实现我基本都理解了。再也不会觉得玄乎了。

还有支持json格式。支持默认页面请求。这个到时候分享的时候再讲一下。

明天再问一下陈老师，看看现在channel是不是线程安全的。现在这个样子感觉不是呢？
而且默认的echo client，是4个线程，但是1个连接。也就是感觉4个epoll都在监听一个fd。这种感觉有点奇怪啊。

[epoll manual](http://ssdr.github.io/2015/01/epoll-manual/)
=======
- 目前就支持两种协议：qzone，http，ckv。
- GDT如果用http协议的话，这里的搞法是：先发一个http包头，再发多个body。如果是http协议的话，则它的uri.Path是""/rpc/gdt.rpc_examples.EchoService.Echo""
- qzone协议的代码在：base_class_old/include/qzone_protocol.h




clinet 选取multiplexer的时候，所用的策略。实现在rpc_channel_impl.cc里。
如果只有一个connection的话，其实一直就用了一个channel。
int index = connect_count_ % multiplexers_->size();  // Round-robbin
