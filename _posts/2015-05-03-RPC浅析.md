---
layout: post
title: "RPC浅析"
---

# RPC浅析

## Protobuf简介

### 简单介绍
[protobuf](https://github.com/google/protobuf)

优点：

- 用来序列化结构化数据，类似于xml，但是smaller, faster, and simpler，适合网络传输
- 支持跨平台多语言(e.g. Python, Java, Go, C++, Ruby, JavaNano)
- 消息格式升级，有较好的兼容性(想想以前用struct定义网络传输协议,解除version的痛楚)

缺点：

- 可读性差(not human-readable or human-editable)
- 不具有自描述性(self-describing)

### Reflection

Reflection: 常用于pb与xml,json等其他格式的转换。

![](https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/reflection_protobuf.png)

更多请参考：
[一种自动反射消息类型的 Google Protobuf 网络传输方案](http://blog.csdn.net/solstice/article/details/6300108)

### 自描述消息

生产者：产生消息，填充内容，并序列化保存

消费者：读取数据，反序列化得到消息，使用消息

目的：解除这种耦合，让消费者能动态的适应消息格式的变换。

生产者把定义消息格式的.proto文件和消息作为一个完整的消息序列化保存，完整保存的消息我称之为Wrapper message，原来的消息称之为payload message。

消费者把wrapper message反序列化，先得到payload message的消息类型，然后根据类型信息得到payload message，最后通过反射机制来使用该消息。

	message SelfDescribingMessage {
  	// Set of .proto files which define the type.
  	required FileDescriptorSet proto_files = 1;

  	// Name of the message type.  Must be defined by one of the files in
  	// proto_files.
  	required string type_name = 2;

  	// The message data.
  	required bytes message_data = 3;
	}

**Self-describing Messages 生产者**

- 使用 protoc生成代码时加上参数–descriptor_set_out，输出类型信息(即SelfDescribingMessage的第一个字段内容)到一个文件，这里假设文件名为desc.set，protoc –cpp_out=. –descriptor_set_out=desc.set addressbook.proto
- payload message使用方式不需要修改tutorial::AddressBook address_book;PromptForAddress(address_book.add_person());
- 在保存时使用文件desc.set内容填充SelfDescribingMessage的第一个字段，使用AddressBookAddressBook的full name填充SelfDescribingMessage的第二个字段，AddressBook序列化后的数据填充第三个字段。最后序列化SelfDescribingMessage保存到文件中。

**Self-describing Messages 消费者**

消费者编译时需要知道SelfDescribingMessage，不需要知道AddressBook，运行时可以正常操作AddressBook消息。

![](https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/self-describing_message_consume.png)

### 动态自描述消息
@TODO

### 工程实践

- 一般对日志数据只加不删不改, 所以其字段设计要极慎重。
- 千万不要随便修改tag number。
- 不要随便添加或者删除required field。
- Clear并不会清除message memory（clear操作适合于清理那些数据量变化不大的数据，对于大小变化较大的数据是不适合的，需要定期（或每次）进行delete操作。建议swap或者delete）
- repeated message域，size不要太大。
- 如果一个数据太大，不要使用protobuf。



### 参考资料
- [Protobuf](https://github.com/google/protobuf)
- [玩转Protobuf](http://www.searchtb.com/2012/09/protocol-buffers.html)
- [Self-describing Messages](https://developers.google.com/protocol-buffers/docs/techniques?hl=zh-CN#self-description)
- Protobuf memory内存的使用。 [Protobuf使用不当导致的程序内存上涨问题](http://qa.baidu.com/blog/?p=1179)
protobuf的clear操作适合于清理那些数据量变化不大的数据，对于大小变化较大的数据是不适合的，需要定期（或每次）进行delete操作。建议swap或者delete。
- [protobuf中会严重影响时间和空间损耗的地方 ](http://blog.chinaunix.net/uid-26922071-id-3723751.html)
repeated的性能问题。对于普通数据类型，在2^n+1时重新分配内存空间，而对于message数据，在2^n+1是分配对象地址空间，但每次都是new一个对象，这样就很损耗性能了。

## RPC

### 业界的RPC

基于protobuf的rpc最简单实现 两个优点：简化client-server交互，就像在调用一个本地方法；通过Protobuf实现多种编程语言之间的交互。
get all the advantages of working with protocol buffers, including efficient serialization, a simple IDL, and easy interface updating.

- [使用google protobuf RPC实现echo service](http://www.codedump.info/?p=169)
- [基于protobuf的RPC实现](http://codemacro.com/2014/08/31/protobuf-rpc/)
- [RPC框架系列——Protocol Buffers](http://jeoygin.org/2011/09/rpc-framework-protocol-buffers.html)
- [Poppy](http://djt.qq.com/article/view/327)

### GDT RPC代码解析

#### 公共代码

**echo_service**

echo_service.proto:

	service EchoService {
		option (gdt.qzone_protocol_version) = 1;
		rpc Echo(EchoRequest) returns (EchoResponse) {
			option (gdt.qzone_protocol_cmd) = 10;
		}
		rpc FormTest(FormTestMessage) returns(FormTestMessage);
	}


protoc编译后：

	class EchoService : public ::google::protobuf::Service {
		protected:
		// This class should be treated as an abstract interface.
		inline EchoService() {};
		public:
		virtual ~EchoService();

		typedef EchoService_Stub Stub;

		static const ::google::protobuf::ServiceDescriptor* descriptor();
		// 下面两个是虚函数,需要在子类实现
		virtual void Echo(::google::protobuf::RpcController* controller,
							const ::gdt::rpc_examples::EchoRequest* request,
							::gdt::rpc_examples::EchoResponse* response,
							::google::protobuf::Closure* done);
		virtual void FormTest(::google::protobuf::RpcController* controller,
							const ::gdt::rpc_examples::FormTestMessage* request,
							::gdt::rpc_examples::FormTestMessage* response,
							::google::protobuf::Closure* done);

		// implements Service ----------------------------------------------

		const ::google::protobuf::ServiceDescriptor* GetDescriptor();
		void CallMethod(const ::google::protobuf::MethodDescriptor* method,
						::google::protobuf::RpcController* controller,
						const ::google::protobuf::Message* request,
						::google::protobuf::Message* response,
						::google::protobuf::Closure* done);
		const ::google::protobuf::Message& GetRequestPrototype(
			const ::google::protobuf::MethodDescriptor* method) const;
		const ::google::protobuf::Message& GetResponsePrototype(
			const ::google::protobuf::MethodDescriptor* method) const;

		private:
		GOOGLE_DISALLOW_EVIL_CONSTRUCTORS(EchoService);
	};

	class EchoService_Stub : public EchoService {
		public:
		EchoService_Stub(::google::protobuf::RpcChannel* channel);
		EchoService_Stub(::google::protobuf::RpcChannel* channel,
						::google::protobuf::Service::ChannelOwnership ownership);
		~EchoService_Stub();

		inline ::google::protobuf::RpcChannel* channel() { return channel_; }

		// implements EchoService ------------------------------------------

		void Echo(::google::protobuf::RpcController* controller,
							const ::gdt::rpc_examples::EchoRequest* request,
							::gdt::rpc_examples::EchoResponse* response,
							::google::protobuf::Closure* done);
		void FormTest(::google::protobuf::RpcController* controller,
							const ::gdt::rpc_examples::FormTestMessage* request,
							::gdt::rpc_examples::FormTestMessage* response,
							::google::protobuf::Closure* done);
		private:
		::google::protobuf::RpcChannel* channel_;
		bool owns_channel_;
		GOOGLE_DISALLOW_EVIL_CONSTRUCTORS(EchoService_Stub);
	};

	// 客户端实际调用的是RpcChannel的CallMethod
	void EchoService_Stub::Echo(::google::protobuf::RpcController* controller,
								const ::gdt::rpc_examples::EchoRequest* request,
								::gdt::rpc_examples::EchoResponse* response,
								::google::protobuf::Closure* done) {
		channel_->CallMethod(descriptor()->method(0),
							controller, request, response, done);
	}
	void EchoService_Stub::FormTest(::google::protobuf::RpcController* controller,
								const ::gdt::rpc_examples::FormTestMessage* request,
								::gdt::rpc_examples::FormTestMessage* response,
								::google::protobuf::Closure* done) {
		channel_->CallMethod(descriptor()->method(1),
							controller, request, response, done);
	}


**system/io\_frame/net\_options.h**

- 非阻塞IO: O_NONBLOCK
- CloseOnExec: FD_CLOEXEC (该句柄在fork子进程后执行exec时就关闭)
- SO_SNDBUF
- SO_RCVBUF
- SO_LINGER 设置套接口关闭后的行为
- TCP_NODELAY：禁用Nagle‘s Algorithm(积累数据量到TCP Segment Size后发送)
- SO_REUSEADDR：让端口释放后立即可以被再次使用

更多参考资料：

- [What are SO\_SNDBUF and SO\_RECVBUF](http://stackoverflow.com/questions/4257410/what-are-so-sndbuf-and-so-recvbuf)
- [非阻塞IO](http://blog.csdn.net/houlaizhe221/article/details/6580775)
- [FD_CLOEXEC解析](http://blog.csdn.net/chrisniu1984/article/details/7050663)
- [SO_RCVBUF and SO_SNDBUF](http://blog.chinaunix.net/uid-29075379-id-3905006.html)。
接收缓冲区被TCP和UDP用来缓存网络上来的数据，一直保存到应用进程读走为止。一个发送缓冲区和一个接收缓冲区，TCP的全双工的工作模式以及TCP的滑动窗口便是依赖于这两个独立的buffer以及此buffer的填充状态。
- [setsockopt ：SO_LINGER 选项设置](http://blog.csdn.net/factor2000/article/details/3929816)
- [神秘的40毫秒延迟与 TCP_NODELAY](http://jerrypeng.me/2013/08/mythical-40ms-delay-and-tcp-nodelay/)
- [SO_REUSEADDR的意义](http://www.cnblogs.com/mydomain/archive/2011/08/23/2150567.html)。一个端口释放后会等待两分钟之后才能再被使用，SO_REUSEADDR是让端口释放后立即就可以被再次使用。
- [socket option](http://man7.org/linux/man-pages/man7/socket.7.html) socketoptions.h/cc里面的实现也看看

#### 客户端代码

- RpcClient:
负责所有RpcChannel对象的管理和对服务器端应答的处理
- RpcChannel:
代表通讯通道，每个服务器地址对应于一个RpcChannel对象，客户端通过它向服务器端发送方法调用请求并接收结果。
- RpcController:
存储一次rpc方法调用的上下文，包括对应的连接标识，方法执行结果等。
- RpcServer:
服务器端的具体业务服务对象的容器，负责监听和接收客户端的请求，分发并调用实际的服务对象方法。


**rpc/client_connection**

	// Client side rpc connection base class.
	class ClientConnection : public BaseConnection
	class QzoneClientConnection : public ClientConnection
	class CkvClientConnection : public ClientConnection

用来在客户端建立连接，读取数据，发送数据等。

**rpc/rpc_channel**

	RpcChannelInterface : public ::google::protobuf::RpcChannel

	void CallMethod(
		const google::protobuf::MethodDescriptor* method,
		google::protobuf::RpcController* controller,
		const google::protobuf::Message* request,
		google::protobuf::Message* response,
		google::protobuf::Closure* done);

发送请求的背后,最后调用的其实是RpcChannel的CallMethod函数.所以,要实现RpcChannel类,最关键的就是要实现这个函数,在这个函数中完成发送请求的事务。


**rpc/rpc_controller**

rpc_controller是一个rpc请求过程中的信息。

	class RpcController : public google::protobuf::RpcController

主要保存下面这些信息：

	int error_code_;
	std::string reason_;
	int timeout_;
	SocketAddressStorage remote_address_;
	int64_t timestamp_;
	bool in_use_;
	kDefaultTimeout = 2000ms;

**rpc/load_balance**

LoadBalancer是一个单例。实现了4种load_balancer：

	rpc/domain_load_balancer
	rpc/l5_load_balancer
	rpc/list_load_balancer
	rpc/single_load_balancer

**rpc/RpcClient**

RpcClient是客户端的主类。一般情况下，一个客户端只需要有一个RpcClient。在初始化的时候，也可以设置线程个数，此个数等于PollThread的个数(多路器的个数)。

利用RpcClient::OpenChannel创建RpcChannel。先根据scheme(目前有qzone,ckv,http三种)创建对应的Factory：

	RpcChannelFactory* factory = GDT_RPC_GET_CHANNEL_FACTORY(scheme)。

再利用factory创建channel:

	shared_ptr<RpcChannelInterface> channel_impl(factory->CreateChannel(multiplexers_, server, NetOptions()))。

创建channel时，先调用RpcChannel::Open。

这里注册了三个Channel以及Factory

	./rpc/ckv_client_channel.cc:22:GDT_RPC_REGISTER_CHANNEL("ckv", CkvClientChannel);
	./rpc/http_rpc_channel.cc:209:GDT_RPC_REGISTER_CHANNEL("http", HttpRpcChannel);
	./rpc/qzone_client_channel.cc:269:GDT_RPC_REGISTER_CHANNEL("qzone", QzoneClientChannel);


**Client代码流程**

rpc client里发起请求，内部调用的都是RpcChannel。

	Closure* done = ::NewCallback(this, &TestClient::AsyncCallDone, i,
								  controller, request, response);
	EchoService::Stub stub(channels_[i % channels_.size()].get());

如果是http请求，则Stub调用的是 HttpRpcChannel::CallMethod。根据是否有done回调函数，分为同步和异步。

	class HttpRpcChannel : public RpcChannelImpl

CallMethod实际调用的是RpcChannelImpl::Call(context)。Call函数里，先获取到connection。

	shared_ptr<ClientConnection> connection = GetRoute(call_context);

GetRoute里首先判断是否已有connected_，如果没有新需要新建立连接。

	result.reset(NewConnection())
	(!result->Open((*multiplexers_)[index].get(), address, options_))

http_rpc_channel里，实现的是

	class HttpRpcConnection : public ClientConnection。

在ClientConnection::Open函数里，调用了NonblockingConnect。

OK，这下请求就算发送过去了。

channel_number 这个设置主要是为什么？多一点有什么好处？channel是socket connect的个数。

clinet 选取multiplexer的时候，所用的策略。实现在rpc_channel_impl.cc里。
如果只有一个connection的话，其实一直就用了一个channel。
int index = connect_count_ % multiplexers_->size();  // Round-robbin

在客户端和服务端设置的threads number是PollThread的个数。channel number可以大于thread number。根据epoll机制，一个thread都可以支撑多个channel。

一个channel里持有很多连接的map，可以共享连接。持有的是长连接通路。看这个函数就知道了：shared_ptr<ClientConnection> RpcChannelImpl::GetRoute。

只要channel没有新建，则连接一直保留，所以这时是长连接。

创建连接的backtrace：

![](https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/client_build_connection.png)

client异步调用的backtrace：

![](https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/client_async_call.png)


#### 服务端代码

**system/io_frame/Multiplexer**

常见的多路复用有：PPC(Process Per Connection)，TPC(Thread PerConnection)，这些模型的缺陷是： resource usage and context-switching time influence the ability to handle many clients at a time。

select的缺点：

- 最大并发数限制。一个进程所打开的FD（文件描述符）是有限制的，由FD_SETSIZE设置。
- 效率问题，select每次调用都会线性扫描全部的FD集合。O(n)复杂度。
- 内核/用户空间的内存拷贝问题。通过内存拷贝让内核把FD消息通知给用户空间。

poll解决了第一个缺点，但第二，三个缺点依然存在。

epoll是一个相对完美的解决方案。(1)最大FD个数很大(由/proc/sys/fs/file-max给出)；(2)epoll不仅会告诉应用程序有I/0事件到来，还会告诉应用程序相关的信息，不用遍历；(3)内核与用户态传递消息使用共享内存；

epoll里还有一个level triggered和edge triggered的区分，level triggered vs edge triggered：edge-trigger模式中，epoll\_wait仅当状态发生变化的时候才获得通知(即便缓冲区中还有未处理的数据)；而level-triggered模式下，epoll\_wait只要有数据，将不断被触发。具体请参考[the purpose of epoll's edge triggered option](http://stackoverflow.com/questions/9162712/what-is-the-purpose-of-epolls-edge-triggered-option)

分发器/多路器Multiplexer其中主要是通过epoll实现分发。[epoll manual](http://ssdr.github.io/2015/01/epoll-manual/)

- [Linux Epoll介绍和程序实例](http://blog.csdn.net/sparkliang/article/details/4770655)
- [How to use epoll? A complete example in C](https://banu.com/blog/2/how-to-use-epoll-a-complete-example-in-c/)

Multiplexer类的主要函数有：Create，Poll，AddDescriptor，RemoveDescriptor，ModifyEvent，RegisterTimer等。RegisterTimer可以用来注册一个定时任务，这在某些场景还是蛮有用的。

调用过AddDescriptor的文件有：

	./net/http/client/connection.cc:214:  multiplexer()->AddDescriptor(this, Multiplexer::kIoEventReadWrite);
	./net/http/server/http_server.cc:246:  if (!multiplexer->AddDescriptor(connection.get())) {
	./net/http/server/listener.cc:123:  multiplexer->AddDescriptor(listener.get());
	./rpc/client_connection.cc:208:    multiplexer()->AddDescriptor(this, events);

Descriptor是FD描述类，其中持有成员变量fd以及close_callback_list，以及两个重要方法：OnWritable，OnReadable。这两个方法在连接时会被回调。

	// MultiplexerNotifier is used to wake up epoll_wait
	class MultiplexerNotifier : public Descriptor

Multiplexer持有成员变量MultiplexerNotifier。即便每个multiplexer不监听socket，但都会create一个fd来用notify。

更多Multiplexer的用法请参考：multiplexer_test。还解释了一个疑问：Poll函数的参数，是epoll\_wait的timeout时间，也就是最多等待多久epoll\_wait就返回。

**system/io_frame/poll_thread**

PollThread类是结合Multiplexer一起使用的，即Thread + Multiplexer。也就是每个PollThread，都在loop multiplexer，如果有事件，就处理。

**system/io_frame/base_connection**

base_connection继承自Descriptor。是connection基类：负责单次网络io。

**net/http_server**

uri: 是用来解析url的一个工具类。

譬如对于url:http://www.baidu.com/s?tn=monline_dg&bs=DVLOG&f=8&wd=glog+DVLOG#fragment，利用Uri类，可以解析出Host,port,Scheme,Fragment,Query等参数。

对于Query参数：tn=monline_dg&bs=DVLOG&f=8&wd=glog+DVLOG，可以利用QueryParams类快速解析出每个name所对应的value。

HttpServer常用的方法有：RegisterHttpHandler，RegisterPrefixHandler，Forward，RegisterStaticResource，AddLinkOnIndexPage等函数。

看一下http_server_test，里面有一些不错的例子，我可以写一个test_server，实际试一下。

**rpc/rpc_server**

class RpcServer : public HttpServer

在HttpServer的基础上，又新增了几个方法：RegisterService，RegisterJceProtoService(注册jce服务)。这些注册服务无非就是把service插入到成员变量vector中。

**rpc/rpc_service_register**

其中主要有三个方法，这个会被server端查找对应service的时候被用到。

	// 用于qzone协议，qzone+pb
	const QzoneMethodContext* FindQzoneMethodContext(int qzone_version, int qzone_cmd) const;
	// 用于qzone+jce协议
	const JceMethodContext* FindJceMethodContext(int qzone_version, int qzone_cmd) const;
	// 用于protobuf
	bool FindMethodByName(const std::string& full_name,
		google::protobuf::Service** service,
		const google::protobuf::MethodDescriptor** method_descriptor,
		RpcErrorCode* error_code) const;

**net/http/http_handler**

HttpHandlerRegistry里持有两个成员变量：handler_map_，prefix_map_。它的功能与rpc_service_register类似。
常用的方法有：两个Register，一个Find。Find访问的是HttpHandler。

HttpHandler。它最重要的函数是：HandleRequest。具体实现为：

	if (callback_) {
		HttpRequest* request_copy = new HttpRequest();
		request_copy->Swap(request);
		HttpResponse* response = new HttpResponse();
		shared_ptr<HttpServerConnection> shared_connection = connection->shared();
		Closure* done = NewCallback(OnResponseDone, shared_connection,
									request_copy, response, response_modifier);
		callback_->Run(connection, request_copy, response, done);
	} else {
		HttpResponse response;
		simple_callback_->Run(request, &response);
		if (response_modifier)
			response_modifier->Run(request, &response);
		PreprocessResponse(request, &response);
		connection->SendResponse(response);
	}

HttpServerConnection继承自BaseConnection(Descriptor)。初始化的时候必须传入的参数有：fd, multiplexer, handler_registry。

HttpServerConnection什么时候被初始化呢？在HttpServer的OnAccept函数里，每从listen_socket获取一个fd，则初始化一个connection。handler_registry来自于HttpServer持有的成员变量HttpHandlerRegistry，multiplexer则是根据fd从multiplexers_按规则取出一个。

	int index = fd % num_threads_;
	Multiplexer* multiplexer = multiplexers_[index].get();

**执行流程**

1. 生成RpcServer::Options参数，这里可以指定服务端的poll_thread(即multiplexer)的线程数。
2. new RpcServer，因为RpcServer继承自HttpServer，HttpServer根据指定的threadnum(n)创建了n个multiplexer，并保存在multiplexers\_。再接着RegisterDefaultPaths，背后调用的是handler\_registry_->Register。
3. 给RpcServer注册服务。Register Proto/jce service是RpcServer的独有方法，RegisterHttpHander是HttpServer的方法。这里的注册只是将service存储在类成员变量vector里。

		server.RegisterService(echo.get());
		server.RegisterHttpHandler("/test", NewPermanentCallback(TestPage));

	protobuf的相关处理由rpc\_http\_handler.cc RpcHttpHandler完成。

		shared_ptr<HttpHandler> handler(
			new RpcHttpHandler(rpc_service_registry_.get(),
							rpc_service_stats_.get()));
		RegisterPrefixHandler(kHttpRpcPrefix, handler);

	其他http处理由http_hander完成。

4. 执行server.Listen(FLAGS_ip, FLAGS_port)。在此之前，先执行RpcServer.BeforeListen()。这里会创建一个RpcServiceRegistry。把builtin服务和上一步注册的服务都写到RpcServiceRegistry。
接着再调用HttpServerListener::Listen，将HttpServerListener添加到multiplexer[0]，即multiplexer->AddDescriptor(listener.get())。并且把HttpServer::OnAccept注册为Listen的回调函数accept_callback_。

	HttpServerListener继承于Descriptor，当multiplexer[0]发现有listen_fs有IoEvent时，会调用HttpServerListener::OnReadable()。

5. server.Start()，启动多个PollThread。每个PollThread的Loop函数，multiplexer都执行Poll()。

6. 最后是while (!server.IsStopped()){}。如果有client请求接入，最先会调用HttpServerListener::OnReadable()函数，这里会先调用accept函数，得到新连接的fd，再调用accept_callback_->Run(fd)。

		void HttpServer::OnAccept(int fd) {
			int index = fd % num_threads_;
			Multiplexer* multiplexer = multiplexers_[index].get();
			shared_ptr<Descriptor> connection = MakeConnection(fd, multiplexer);
			if (!connection) {
				close(fd);
				return;
			}
			if (!multiplexer->AddDescriptor(connection.get())) {
				PLOG(WARNING) << "Add socket to poll failed, fd=" << fd;
				return;
			}
			connection_manager_->Add(connection);
			connection->PushCloseCallback(
				NewCallback(connection_manager_.get(), &ServerConnectionManager::Remove,
							connection));
		}

	上面代码把新连接的fd也加入到multiplexer的监听中。

7. 如果有数据可以读入，则将调用RpcServerConnection::OnReadable函数，首先判断是qzone协议，还是http协议。如果是qzone协议(protobuf or jce)，先注册RpcServerConnection::OnRequestDone()为done的回调函数，并创建QzoneServiceHandler。
再调用HttpServerConnection::OnReadable函数，即BaseConnection::OnReadable()函数。
在OnReadable()中，再调用BaseConnection::ReadPackets。
GetNextSendingBuffer,GetPacketSize,OnPacketReceived,OnEofReceived是BaseConnection的纯虚函数。RpcServerConnection对这些纯虚函数进行了重写。

		int RpcServerConnection::GetPacketSize(const StringPiece& buffer) {
		return handler_ ? GetQzonePacketSize(buffer) :
							HttpServerConnection::GetPacketSize(buffer);
		}

	譬如上面，如果发现是qzone协议的话，则调用的是GetQzonePacketSize。

	![](https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/rpc_method_backtrace.png)

	![](https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/rpc_method_write.png)

	![](https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/rpc_method_requestdone.png)

8. 实际read packet和write packet的逻辑如下：

		./system/io_frame/base_connection.cc:147:  int n = read(fd(), buffer, buffer_size);
		./system/io_frame/base_connection.cc:108:  int n = send(fd(), buffer, buffer_size, flags);

	![](https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/read_packet.png)

	![](https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/write_packet.png)

	客户端和服务端都是这个脉路。

9. 把这两个图熟悉一下。一个是http协议请求时，server端的调用逻辑：

	![](https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/http_echo_bt.png)

	另一个是qzone协议请求时，server端的调用逻辑：

	![](https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/qzone_echo_bt.png)

10. Content-Type设置kContentTypeProtobuf，kContentTypeJson，kContentTypeProtobufText时不同的处理方式。

		static const char* const kContentTypeJson = "application/json";
		static const char* const kContentTypeProtobuf = "application/x-protobuf";
		static const char* const kContentTypeProtobufText = "text/x-protobuf";

	需要在客户端那里设置相应的值，就可以得到处理逻辑。


**jce是怎么处理的**

1. 首先有jce格式，再定义一个proto格式。
2. 实现下面三个的代码

		bool JceStructToPbMessage(const QZAP_NEW::wup_qzap_search_display_req* jce,
								MixerRequest* request);
		bool PbMessageToJceStruct(const MixerResponse* response,
								QZAP_NEW::wup_qzap_search_display_rsp* jce);

		GDT_RPC_DEFINE_JCE_PROTO_SERVICE(0, JceMixerService, MixerService,
		GDT_RPC_JCE_METHOD(0, SearchAd, QZAP_NEW::wup_qzap_search_display_req,
							QZAP_NEW::wup_qzap_search_display_rsp)

3. 也就是说，如果是jce服务，rpc client会先将jce转成proto，再发起rpc调用，最后回报的时候再从proto解析为jce。
4. 如果是qzone服务，其实理论上它也是header+proto，那么在接受包的时候，判断是qzone协议的话，先把包头去掉。

**CKV这里的搞法**

复用了客户端的multiplexer，继承自RpcClientCallContext。
客户端主要负责写两个函数：EncodeRequest，OnPacketReceived。
EncodeRequest这个函数是对消息做打包。OnPacketReceived是对消息做解包。

还要注意，同步调用和异步调用的区别。
异步调用的话，纯粹就是复用了multiplexer的功能，做回调。
而同步的话，其实就是调用后，做一个响应的等待，利用的是AutoResetEvent, "./system/concurrency/event.h"。

**more**

@TODO: feeds的rpc服务可以多学习一下。adx的action, state与feeds的action, state以后也可以多学习学习！

### 参考资料

- [What does the explicit keyword in C++ mean?](http://stackoverflow.com/questions/121162/what-does-the-explicit-keyword-in-c-mean)
- [final specifier - C++ Reference](http://en.cppreference.com/w/cpp/language/final)
- google rpc [grpc](http://www.grpc.io)
- [protobuf addons](https://github.com/google/protobuf/wiki/Third-Party-Add-ons)
- [rpc definition](https://developers.google.com/protocol-buffers/docs/reference/cpp-generated)
- [poppy的官方资料](http://djt.qq.com/article/view/327)

### 补余

GDT的rpc:除了二进制协议外，Poppy支持还以普通的HTTP协议，传输以JSON/protobuf文本格式定义的消息。很方便用各种脚本语言调用，甚至用 bash，调 wget/curl 都能发起 RPC 调用。
Poppy的二进制协议与一般的设计不一样的是，它是以HTTP协议头为基础建立起来的，只是建立连接后的最初的采用HTTP协议，后续的消息往来直接用二进制协议，所以效率还是比较高的。

handler列表：

	./net/http/server/forward_handler.h
	./net/http/server/gflags_handler.h
	./net/http/server/http_handler.h
	./net/http/server/static_resource_handler.h

下面一个处理qzone协议，一个处理http协议。主要是在server角度，接受数据，解包，调用实际service，封包等操作。

	./rpc/qzone_service_handler.h
	./rpc/rpc_http_handler.h

connection列表：

	./net/http/client/connection.h
	./rpc/client_connection.h
	./system/io_frame/base_connection.h 这个是基类

两个connection类，主要是用来处理服务端的socket连接。OnReadable, OnWritable。
RpcServerConnection::OnReadable主要做了对http和qzone协议的区分，然后如果是http协议，则主要调用HttpServerConnection，如果是qzone协议，则主要调用QzoneServiceHandler里的方法。

	class RpcServerConnection : public HttpServerConnection
	./net/http/server/connection.h
	./rpc/rpc_server_connection.h


service列表：
这个里面用途不大

	./rpc/ckv_service.h
	./rpc/jce_proto_service.h
	./rpc/jce_service.h
	./rpc/rpc_builtin_service.h
	./rpc/udp_rpc_service.h

protocol列表：

	./rpc/http_rpc_protocol.h
	./rpc/rpc_qzone_protocol.h

客户端connection:

	./client_connection.h:95:class ClientConnection : public BaseConnection
	./ckv_client_channel.h:23:class CkvClientConnection : public ClientConnection
	./http_rpc_channel.h:24:class HttpRpcConnection : public ClientConnection
	./qzone_client_channel.h:22:class QzoneClientConnection : public ClientConnection
	./rpc_channel_impl.h:42:  virtual ClientConnection* NewConnection() = 0;

客户端channel:

channel这边主要还是基于 BaseConnection这个在做。还是那两个入口函数，read和write。
ClientConnection里面会调用RpcClientCallContext。

	./rpc_channel_impl.h:26:class RpcChannelImpl : public RpcChannelInterface {
	./qzone_client_channel.h:35:class QzoneClientChannel : public RpcChannelImpl {
	./http_rpc_channel.h:42:class HttpRpcChannel : public RpcChannelImpl {
	./ckv_client_channel.h:33:class CkvClientChannel: public RpcChannelImpl {

客户端balancer列表，主要来做负载均衡。

	./rpc/domain_load_balancer.h
	./rpc/l5_load_balancer.h
	./rpc/list_load_balancer.h
	./rpc/load_balancer.h
	./rpc/single_load_balancer.h


刚才又看了一遍 jeff的rpc，再看起来，感觉非常清晰了。
还是有帮助的。至少这下搞了，这些rpc的实现我基本都理解了。再也不会觉得玄乎了。

还有支持json格式。支持默认页面请求。这个到时候分享的时候再讲一下。

明天再问一下陈老师，看看现在channel是不是线程安全的。现在这个样子感觉不是呢？
而且默认的echo client，是4个线程，但是1个连接。也就是感觉4个epoll都在监听一个fd。这种感觉有点奇怪啊。

- 目前就支持两种协议：qzone，http。
- GDT如果用http协议的话，这里的搞法是：先发一个http包头，再发多个body(未实现)。如果是http协议的话，则它的uri.Path是""/rpc/gdt.rpc_examples.EchoService.Echo""
- qzone协议的代码在：base_class_old/include/qzone_protocol.h

做一下性能对比：在数据量比较小的时候，用qzone协议比http协议性能要好一倍。主要是包的大小影响比较大。