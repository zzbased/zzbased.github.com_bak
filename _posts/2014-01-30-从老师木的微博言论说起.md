---
layout: post
title: "从\"老师木微博言论\"说机器学习"
---

12年年底的时候，[老师木](http://weibo.com/p/1005051991303247)在微博上发表了很多关于机器学习领域的言论，有些言论很惊人，譬如对李国杰院士文章的不同意见，譬如机器学习无用论。当时读来，对我的学习帮助很大(虽有些点理解并不深)。

现在我们已经知道，老师木的真名是：袁锦辉，我们也知道了老师木的最新作品：[lightLDA](http://arxiv.org/abs/1412.1576)，而老师木前段时间给我们部门做过一次技术分享(题目是:LightLDA: Extremely Fast and Scalable Topic Model)，那时看来，倒很有学术范，找不到微博上那种"肆意妄为"的调调。现在2年多过去了，我在腾讯又蹉跎了两年，两年里机器学习领域又出现了好些新玩意，cnn，word2vec，rnn，lstm，一时之间你方唱罢我登场，不亦乐乎。最近趁春节，偶然间又回顾了一遍"老师木的微博言论"，仍有较多启发，我们不妨从这里说起，聊聊老师木的言论，以及工业界某一线小卒眼里的机器学习。

下文摘抄了一部分老师木的言论，对其中某些细节，我做了一些展开以及阐述。完整言论请参考老师木的微博原文，也可以参考[52cs整理的老师木言论集合](http://www.52cs.org/?cat=26)。

## 概率与统计

### 概率与统计
Probability和Statistics是不同的概念，前者是给定模型计算数据的可能性，后者是给定数据的可能性推模型。概率很美，统计很丑，再漂亮的统计也是基于假设的，而人为的假设缺乏客观。

因此在学习分类器时，面对一组特征，我们当然喜欢和目标y互信息最大的特征。尽管道理上互信息为特征选择提供了方向，但并没有带来多少可操作性，因为要计算互信息，必须先知道x和y的概率分布。然而一旦有它们的概率分布，我们可以直接做贝叶斯决策就能达到贝叶斯错误率，而不必在求助于互信息。

任给一批观测数据，背后最简洁的rule是什么？统计学家一看这个问题不可计算，那还搞个什么？于是耍了个花招，说我们不寻求宇宙内最短程序，我们只在一个受限的假设空间寻找最简洁的rule，于是统计学家们就在假设空间上做文章。很不幸，限制假设空间后，搜索出最优rule时常复杂度很高，npc是家常便饭。于是进一步限制rule的结构，只变化参数才好操作。

对于输入的数据，当你知晓其规律时才能根据规律写一个很短的程序生成同样的数据。当你不知道规律时，只能按原样printf。所以根本是要挖掘规律，统计由数据推模型就是在搞一件这么不可能的事。

更多请参考[52cs link](http://www.52cs.org/?p=336)

### 概率图
概率图是图论和概率理论联姻的产物，借助图结构表达多元随机变量之间复杂的概率依赖关系。Judea pearl在该领域做了最早最重要的贡献。

Graphical model火的一个例子是CRF广泛应用，做出CNN的Lecun Yann的团队曾在一篇文章中说他们比lafferty更早做出CRF，也就是98年 IEEE proceddings上那篇很长文章中描述的graph transformer networks，去读这篇会发现算法的本质确实是一样的。

LDA中，数据集合上的topic有model distribution，而pLSA中，数据集合上topic只有empirical distribution；前者是bayesian，后者是mle，数据足够多时，二者趋同。

更多请参考[52cs link](http://www.52cs.org/?p=339)

### 信息处理不等式

“信息逐层丢失“是有严格理论依据的，信息论里有一个著名的公式叫信息处理不等式，设a,b,c分别代表信息处理的结果，即b是a的处理结果，c是b的处理结果，那么可以证明:a和c之间的互信息不会超过a与b之间的互信息。这表明信息处理不会增加信息，大部分处理都会丢失信息。既然处理加工不会增加信息，为什么还要各种花样对信息进行处理呢？

因为好的系统是有策略的丢无意义信息，这意味着distill有意义信息。考察所有信号处理和机器学习的工作，无非就是设计滤波器，提取一些信息，扔掉一些信息。

更多请参考[weibo link](http://weibo.com/1991303247/z8F8dzt3B)

### 高斯分布

1809年，高斯发表了关于天体运行论的著作，在第二卷第三节中，他导出正态曲线适合于表示误差规律，同时承认拉普拉斯较早的推导。正态分布在十九世纪前叶因高斯的工作而加以推广，所以通常称作高斯分布。
要领略高斯分布的美妙，至少有几个方面不可缺少：

1. 与中心极限定理的联系；

2. 与最小二乘拟合的关系；

3. 与最大熵原理的联系，高斯分布在给定一阶和二阶统计量约束下熵最大的分布。

4. 高斯分布与机器学习2范数正则化的联系，特别是SVM中常用最大间隔概念的联系；

5. 高斯分布与大家耳熟能详的主成分分析PCA的理论联系；

6. 高斯的共轭分布还是高斯，这是不是唯一具有这个性质的分布；

7. 高斯分布与独立成分分析ICA的理论联系，主要用到高斯分布不相关与独立的等价性（严格地说有个小陷阱，我被网友指正过）。

更多请参考[52cs link](http://www.52cs.org/?p=67)

### 随机数学
随机数学一般包括三个方面：概率论，统计学，随机过程。
概率论重点研究模型产生数据的机制，统计学重点研究由数据推理模型的机制，随机过程重点研究多元随机变量的规律。

IT民工除了懂计算机学科的算法数据结构复杂性理论，还需要掌握的数学：数学分析，泛函，矩阵计算，概率统计，最优化理论。

推荐几本书：Athanasios Papoulis的《概率、随机变量与随机过程》。Larry Wasserman的《All of Statistics：A concise Course of Statistical inference》

更多请参考[weibo link](http://weibo.com/1991303247/z8Q24optq)

## 常用机器学习

### 有监督学习

代表性的三种算法必看：svm,lr,adaboost。三个方面必须弄懂：损失函数，正则化，线性与非线性。

svm,lr,adaboost的损失函数分别对应hinge loss，cross entropy loss，exponential loss。
不同的loss function对应着不同的机器学习算法。

- 0-1 loss: count(y != y')  -> Perceptron
- squared loss: (y-y')^2  -> Linear regression
- exponential loss: exp(-y*w^T*x)  -> Adaboost
- hinge loss: max(0, 1-y*w^T*x)  -> SVM
- cross entropy loss: log2(1 + exp(-y*w^T*x))  -> Logistic regression

hinge loss，exp loss，cross entropy loss三者之间并无本质区别，都是0-1 loss的凸上届。使用凸上届取代0-1损失就是所谓的凸松弛技术，把组合优化问题转化为好求解的凸优化问题。

![](https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/VC理论/hinge-exp-logistic-bounds.png)

正则化，需要理解为什么1范数可以得到稀疏解，为什么2范数能得到最大间隔解。

这其中，还涉及到三种损失：期望风险，经验风险，结构风险。期望风险是针对真实模型的，一般没法计算。经验风险是针对样本数据的，一般用经验风险最小化来估计期望风险。为了尽可能避免过拟合现象的出现,就要对模型的复杂度进行惩罚,这就是正则化, 一般正则化,就是对模型的参数进行惩罚，此时是结构风险。

**最好还要弄懂正则化与贝叶斯参数估计的联系**，从而进一步知道最小描述长度准则，以及奥卡姆剃刀原则。

naive bayes怎么转化成线性分类器。弄懂这个，就可以明白线性分类器中学到的法向量中各维权重的物理意义。

duda 模式识别2.9

[如何理解naive Bayes](http://blog.csdn.net/pennyliang/article/details/6651897) 通俗易懂，这篇建立了naive bayes和线性分类器的联系，线性分类器中权重w 和截距b 在概率模型中的解释。最好强调下，naive 之所以 naive 的 条件独立假设。

线性与非线性，要弄懂线性可分，线性无关，VC维之间的关系。

无结构预测能力的模型->structured output prediction。lr变成crf，svm变成m3n(max-margin markov nets)。

更多请参考[有监督学习原地址](http://1.guzili.sinaapp.com/?p=20)

### 为什么L1正则项容易得到稀疏解?

加入L2正则化，则是ridge regression。加入L1，则是lasso(least absolute shrinkage and selection operator)。

![](https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/l1_l2_regularization.png)

可以看到，l1-ball 与 l2-ball 的不同就在于他在和每个坐标轴相交的地方都有“角”出现，而目标函数的测地线除非位置摆得非常好，大部分时候都会在角的地方相交。注意到在角的位置为产生稀疏性，例如图中的相交点就有w1=0，而更高维的时候（想象一下三维的l1-ball 是什么样的？）除了角点以外，还有很多边的轮廓也是有很大的概率成为第一次相交的地方，又会产生稀疏性。相比之下，L2-ball就没有这样的性质，因为没有角，所以第一次相交的地方出现在具有稀疏性的位置的概率就变得非常小了。

![](https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/l1_l2_regularization2.png)

ridge regression相比于原始的least suqare解只是做了一个全局缩放，而LASSO 则是做了一个soft thresholding：将绝对值小于  的那些系数直接变成零了，这也就更加令人信服地解释了LASSO 为何能够产生稀疏解了。

总结为：L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。Lasso在特征选择时候非常有用，而Ridge就只是一种规则化而已。

上面从直观上来解释了为什么l1-regularization 能产生稀疏性，而l2-regularization 不行的原因了。而至于从理论上如何解释，请参考[Sparsity and Some Basics of L1 Regularization](http://freemind.pluskid.org/machine-learning/sparsity-and-some-basics-of-l1-regularization/)。

### 为什么2范数能得到最大间隔解?

[统计学习那些事](http://cos.name/2011/12/stories-about-statistical-learning/) 除了这篇文章，统计之都的很多文章再看看。

### 线性分类器
logistic regression本质是线性分类器，只不过在线性变化后通过sigmoid函数做了非线性变换，将值域映射到[0,1]。svm和lr都可以视作单层的neural network，也就是感知器的小变形。线性neural netword的能力是有限的，譬如xor问题。

说起神经网络，通常是指多层非线性的神经网络。对于lr，如果要有非线性，一般是我们人工生成非线性的特征；而对于nn，它可以自己生成非线性的表述(即特征)，也就是我们常说的learning representation。

更多请参考[weibo link](http://weibo.com/1991303247/z8Q42rhD1)

### sigmoid
为什么sigmoid函数比别的非线性变换更有吸引力吗？

做sigmoid变换的目的是把（-inf，+inf）取值范围的信号（用x表示）映射到（0，1）范围内（用y表示）：y=h(x)。

寻找一个映射h能在有观测误差e的情况下最优的保持输入信号的信息，用信息论的语言描述就是x与y之间的互信息最大。当y服从均匀分布时熵最大(互信息最大)，因此能把x映射成一个服从均匀分布的变量y的映射h是最优的。

答案是：知道x的概率密度函数f(x)，那么对f(x)的积分可得累积分布函数F(x)，F就是最优的h，即y=F(x)最优。

Sigmoid变换对于概率密度函数呈倒置钟形的信号来说是最优变换，而现实中倒置钟形分布的信号又比比皆是，很多自然的信号都是这样，譬如高斯分布就很普遍。换句话说：对于概率密度不是倒置钟形的信号会有自己最优的变换，而不一定是sigmoid函数。

![](https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/sigmoid图.jpg)

@ICT山世光：我的歪解，它能压缩无用长尾巴，扩展核心分辨率

对y的等距量化，对应到输入信号x时是非等距量化，同样长的y区间，对应到x时区间是不等的。同样长度的y，在x高密度区域只能编码一小段，而在x低密度区域却编码一长段。

注释1：KL散度( Kullback–Leibler divergence)，又称相对熵（relative entropy)，是描述两个概率分布P和Q差异的一种方法。

设两个随机变量(X,Y)的联合分布为p(x,y)，边际分布分别为p(x),p(y)，互信息I(X,Y)是联合分布p(x,y)与乘积分布p(x)p(y)的相对熵。
H(X,Y)=H(X)+H(Y|X)=H(Y)+H(X|Y)，I(X;Y)=H(X)-H(X|Y)。
![](https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/kl_divergence.png)

注释2：如果用c的库函数rand()生成服从高斯分布或贝塔分布，or其他分布的随机数？应该有不少人知道答案吧。C函数rand可生成[0,1）均匀分布随机数，用一种叫inverse cumulative distribution function的变换方法可以生成任意分布的随机数。

更多请参考[为什么我们喜欢用sigmoid这类S型非线性变换](http://www.52cs.org/?p=363)

### 无监督学习

无监督学习，在操作上比较直接可做的两件事有：1）聚类，把每个d维向量作为整体来研究向量空间上的结构，共N个instance；2）特征分析（降维等），把每个N维向量作为整体来研究，共d个instance。**两者是对偶关系，存在等价联系**，譬如PCA和K-Means之间的等价关系，当然理想情况是两者的结合。所谓两者的结合，就是同时在特征和样例两个维度上分析，分析哪些样例在哪些维度上是共同的因素主导的。**方法上PCA和K-Means向Topic Model的发展反映了这个趋势**。最根本上，无监督学习的目标是：给定观察数据，推理出数据产生的机制。

更多请参考[weibo link](http://weibo.com/1991303247/z8MOLdSoE)

### 无监督分词

这里说的是这篇文章 [互联网时代的社会语言学：基于SNS的文本数据挖掘](http://www.matrix67.com/blog/archives/5044)

更多请参考[weibo link](http://weibo.com/1991303247/z8MMLq5zX)

### SVM神话

@余凯_西二旗民工 很多有关SVM的教科书都有misleading: (1)KKT条件，support vectors，quadratic programing都是浮云；（2）kernel本身对理解学习问题有帮助，但实际工程上用处为0；（3）hinge loss只是众多可选项之一，logistic效果一点不差。

有理论结果表明support vector数目随训练样本数目线性增长，所以，svm并不是真正稀疏。

看损失函数曲线，真看不出hinge loss比log loss好，只是hinge loss能得到稀疏解，看上去很美。

这里讨论了 loss function和nonlinear两个问题，只有从优化难度考虑时对hinge loss和 log loss的比较才有意义，后者可导，解无约束优化的各种梯度方法都可以上，前者要这么做只有用sub gradient。是否用kernel应主要考虑非线性是否有必要，复杂度是否可接受。

一般情况，维度大且稀疏，核函数基本无用。

更多请参考[原地址](http://weibo.com/1991303247/z8MLB2fqw) [52cs link](http://www.52cs.org/?p=359)

## 计算机视觉

### 弱谈Marr视觉计算理论

Marr最大的贡献是提出应从理论、算法和实现三个层次去研究视觉，这种指导思想在计算机视觉领域只有少数人还在遵循，但在心理学、理论神经科学还是基本的研究范式。

更多请参考[原地址](http://weibo.com/1991303247/z8LUXjrpe) [52cs link](http://www.52cs.org/?p=61)

## 实际应用

### 博弈论与广告

计算广告系统中关键词竞价就是一个典型的例子，什么ctr预估只是这个系统中做好做，最容易理解的一个模块，能用经济模型来统一研究这个系统才有挑战，譬如从广告主角度买什么关键词，出多少钱等。

Ctr预估是整个环节中最客观，最静态的部分，仅仅是match game，匹配query与ad而已，可期望有最优解。而其他环节是动态的，多角色参与的，短期利益和长期利益权衡的，好多因素，不知道现有系统中所采取的策略是否最优。

更多请参考 [weibo link](http://weibo.com/1991303247/z8Pjm7ekZ) [52cs link](http://www.52cs.org/?p=356)

### 对李国杰院士大数据一文的不同意见

其实大数据对模型、理论的需求更加迫切。我们说的学习都是归纳法，自动或半自动发现数据内在的统计规律，一旦发现规律就可以得到对数据更简短的描述，又称compact representation，所有的学习算法都是在做这件事情，所有的自然科学研究也都在做这个。

Learning representation当然好，机械模仿生物神经网络的结构和行为当然也有一定价值，但这还不够。好的representation一定是捕捉了数据regularity从而使表示又简洁又有效。但regularity的挖掘又何尝容易，搞机器学习的也无非是在一个假设空间找个还凑合的了事。Learning as compression、最小描述长度、奥卡姆剃刀、科尔莫格罗夫复杂性、描述复杂性等等都是这件事相关的学术思想。

从压缩的观点看，描述数据所需要的bit数分两部分构成，一部分是模型（规则）本身所需要的bit，一种是用模型解释数据时得到的残差所需要的bit数。（注意：有了模型，我们只需要描述残差和模型就可重构数据）。当模型（规则）和残差解释数据不会带来压缩效果时，就不要用模型了。

对于有的问题，大数据意味着，任何一个有益的模式机器都看到过，而且可能看到不止一次，当然不再需要模型。机器学习归根到底是记忆，只不过是稍微聪明的记忆而已，有一定的泛化能力，和人类的记忆还不能比。有的问题看上去高维、稀疏，那是因为对条件独立挖掘的不够。

更多请参考[52cs link](http://www.52cs.org/?p=342)

## 学习机器学习

### 理想的机器学习书
吴军在讨论“规则与统计”，以及对阿米特-辛格之简单哲学的吹捧，显得很没境界。哪个流派称为显学，哪个没落，十年河东，十年河西而已，历史上这样的故事发生了很多次了。而且他在讨论“规则与统计”时捧统计抑规则，而在讨论艾米特辛格简单哲学时，捧规则抑统计，不知他信仰到底是什么？

李航的书对LR与MaxEnt之间的关系介绍得也不够理想。应引入指数族分布，给定一些约束求最大熵分布，数学结果是指数族分布；且参数结果与已知分布为指数族时的最大似然解等价。具有广泛实例的指数族竟是最大熵原理的自然结果，让人感叹自然界的奇迹，也内心欣赏这样一个连定理都称不上的原则的神奇。

物理意义。一定要给每一个数学结论都赋予物理意义。来龙去脉，给出每一个重要发展的来龙去脉、历史渊源或者八卦背景，学术点叫motivation。只有了解这些才能融会贯通，历史地理解一个方法或理论的地位、局限性等。

现在有一本Kevin Murphy出了一个 Machine Leaning：a probabilistic perspective，这本书不错，我写过一句话评论：complete but not comprehensive enough。 有些独特的东西，以后再谈。

更多请参考[weibo link](http://weibo.com/1991303247/z8MqXeo3F) [52cs link](http://www.52cs.org/?p=70)

### Domingos 《机器学习那点事儿》解读

把不好吃的皮剥掉，论文精彩之处来了，有一些例子：1）基于10w词汇的垃圾邮件过滤；2）从100w样例学习含有100个布尔变量的表达式；3）学习析取范式；4）pac的阐述；5）高维空间反直觉举例；6）xor学习；7）n位校验函数学习。

更多请参考 [52cs link](http://www.52cs.org/?p=79)

### 关于如何学习

信号处理对信息学科也很重要。

博士最应得到训练的是发现问题的能力，而不是解决问题的能力，前者比后者更难，如果只要后者，硕士阶段的训练就足够了。研究的开始源于心中的疑惑。开始阶段，疑惑通常借助教材就能解决，中间一些疑惑需要大量阅读相关较新的文章就能解决。到后来，你发现还有一些疑惑即使是最著名的论文和专家也不能给你满意答案，这时你差不多就找到问题了。有些学生很“幸运”，遇到了事必躬亲的导师，他会告诉你去做什么问题有前途，此时你通常就越过了寻找问题的阶段，你比别人进入状态快，不过很可能永远缺失发现问题、提出问题的能力，总是做一些人云亦云的问题。

遇到疑惑，你会根据自己掌握的知识数据经验观测，形成一些猜测，所谓的hypothesis，你还不确认，可能需要推理或设计一些实验去论证或推翻。当然，这些hypothesis可能不重要，不关键，甚至出发点事错误的，这可能源于知识面狭窄，见多识广的导师通常此时可帮上忙。最近有不少ml背景的学生关注我，我特别想分享的一个建议是：要“科学”的作研究。有两个含义，第一是“正确”的方式，而不是错误的方式，第二个含义是要像先贤们做science一样，学生时代是有这个条件的。

更多请参考[52cs link](http://www.52cs.org/?p=85)

### 机器学习有没有用

给机器学习的泼点冷水，如果不是写论文，只为解决实际问题，学那么多统计、优化、矩阵之类的得不偿失，还不如多花点时间提高编程能力实惠。我花了那么多时间搞熟练哪些数学，摸清楚各种复杂机器学习算法的内涵以及互相之间的关系，多了在给别人讲解时有孔乙己懂回字有几种写法这样虚无的优越感，现在还没发现实际中有什么大用途。实际问题中，喜欢使用的都是直接有力的解法。当然，我比别人多懂一点这种简单粗暴的做法背后也有深刻的道理。

机器学习强大也弱小，计算机专业学生要看到机器学习的局限性是不需要概率统计背景的，只需要看俩经典的“简单”问题。机器学习本质是从观测数据序列中自动挖掘出能解释数据的最通用（最简单）规律。给一批布尔表达式，有求最小析取范式的好方法吗？给一批字符串，有求最短正则表达的好办法吗？.*不算。

未来成功的互联网服务一定是基于机器学习的，但不会用太复杂的工具，因为海量数据使太复杂的方法既不可行也无必要，主要瓶颈还是在infrastructure和数据质量上。

更多请参考[52cs link](http://www.52cs.org/?p=76)，[weibo link](http://weibo.com/1991303247/z8F4f9cBt)


## 深度学习

### Deep Learning

更多请参考[weibo link](http://weibo.com/1991303247/z8QNjkV5l)

## 其他

### 规则与统计
更多请参考[52cs link](http://www.52cs.org/?p=82)

### 跨界的机器学习
更多请参考[52cs link](http://www.52cs.org/?p=73)

### 关于基
更多请参考[52cs link](http://www.52cs.org/?p=333)

### 一些关于产品的想法
更多请参考[weibo link](http://weibo.com/1991303247/z8PlL3B0J)

### 人机对话
更多请参考[52cs link](http://www.52cs.org/?p=64)

### 优质数据
更多请参考[weibo link](http://1.guzili.sinaapp.com/?p=27)

### 学术界
更多请参考[原地址](http://1.guzili.sinaapp.com/?p=30)

