---
layout: post
title: "从\"老师木微博言论\"说机器学习"
---

12年年底的时候，[老师木](http://weibo.com/p/1005051991303247)在微博上发表了很多关于机器学习领域的言论，有些言论很惊人，譬如对李国杰院士文章的不同意见，譬如机器学习无用论。那时在我个人读来，好些观念都很具有学习价值。

现在我们已经知道，老师木的真名是：，我们也知道了老师木的最新作品：lightLDA，而老师木前段时间给我们部门做过一次技术分享，那时看来，倒很有学术范，找不到微博上那种"肆意妄为"的调调。现在2年过去了，我在腾讯又蹉跎了两年，两年里机器学习领域又出现了好些新玩意，cnn，word2vec，rnn，lstm，一时之间你方唱罢我登场，不亦乐乎。最近趁春节，偶然间又回顾了一遍"老师木的微博言论"，仍有启发，我们不妨从这里说起，聊聊老师木的言论，以及工业界某一线小卒眼里的机器学习。

下文摘抄了一部分老师木的言论，对其中某些细节，我做了一些展开以及阐述。完整言论请参考老师木的微博原文，也可以参考[52cs整理的老师木言论集合](http://www.52cs.org/?cat=26)。

## 概率与统计

### 概率与统计

[52cs link](http://www.52cs.org/?p=336)

### 概率图

[52cs link](http://www.52cs.org/?p=339)

### 信息处理不等式

“信息逐层丢失“是有严格理论依据的，信息论里有一个著名的公式叫信息处理不等式，设a,b,c分别代表信息处理的结果，即b是a的处理结果，c是b的处理结果，那么可以证明:a和c之间的互信息不会超过a与b之间的互信息。这表明信息处理不会增加信息，大部分处理都会丢失信息。既然处理加工不会增加信息，为什么还要各种花样对信息进行处理呢？

[原地址](http://weibo.com/1991303247/z8F8dzt3B)

### 高斯分布
[52cs link](http://www.52cs.org/?p=67)

### 随机数学
[weibo link](http://weibo.com/1991303247/z8Q24optq)

## 常用机器学习

### 有监督学习

代表性的三种算法必看：svm,lr,adaboost。三个方面必须弄懂：损失函数，正则化，线性与非线性。

svm,lr,adaboost的损失函数分别对应hinge loss，log loss，exponential loss。三者之间并无本质区别，都是0-1 loss的凸上届罢了。使用凸上届取代0-1损失就是所谓的凸松弛技术，把组合优化问题转化为好求解的凸优化问题。

正则化，需要理解为什么1范数可以得到稀疏解，为什么2范数能得到最大间隔解。

这其中，还涉及到三种损失：期望风险，经验风险，结构风险。期望风险是针对真实模型的，一般没法计算。经验风险是针对样本数据的，一般用经验风险最小化来估计期望风险。为了尽可能避免过拟合现象的出现,就要对模型的复杂度进行惩罚,这就是正则化, 一般正则化,就是对模型的参数进行惩罚，此时是结构风险。

**最好还要弄懂正则化与贝叶斯参数估计的联系**，从而进一步知道最小描述长度准则，以及奥卡姆剃刀原则。

naive bayes怎么转化成线性分类器。弄懂这个，就可以明白线性分类器中学到的法向量中各维权重的物理意义。

duda 模式识别2.9

[如何理解naive Bayes](http://blog.csdn.net/pennyliang/article/details/6651897) 通俗易懂，这篇建立了naive bayes和线性分类器的联系，线性分类器中权重w 和截距b 在概率模型中的解释。最好强调下，naive 之所以 naive 的 条件独立假设。

线性与非线性，要弄懂线性可分，线性无关，VC维之间的关系。

无结构预测能力的模型->structured output prediction。lr变成crf，svm变成m3n(max-margin markov nets)。

[有监督学习原地址](http://1.guzili.sinaapp.com/?p=20)

### 为什么L1正则项容易得到稀疏解?

加入L2正则化，则是ridge regression。加入L1，则是lasso(least absolute shrinkage and selection operator)。

![](https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/l1_l2_regularization.png)

可以看到，l1-ball 与 l2-ball 的不同就在于他在和每个坐标轴相交的地方都有“角”出现，而目标函数的测地线除非位置摆得非常好，大部分时候都会在角的地方相交。注意到在角的位置为产生稀疏性，例如图中的相交点就有w1=0，而更高维的时候（想象一下三维的l1-ball 是什么样的？）除了角点以外，还有很多边的轮廓也是有很大的概率成为第一次相交的地方，又会产生稀疏性。相比之下，L2-ball就没有这样的性质，因为没有角，所以第一次相交的地方出现在具有稀疏性的位置的概率就变得非常小了。

![](https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/l1_l2_regularization2.png)

ridge regression相比于原始的least suqare解只是做了一个全局缩放，而LASSO 则是做了一个soft thresholding：将绝对值小于  的那些系数直接变成零了，这也就更加令人信服地解释了LASSO 为何能够产生稀疏解了。

总结为：L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。Lasso在特征选择时候非常有用，而Ridge就只是一种规则化而已。

上面从直观上来解释了为什么l1-regularization 能产生稀疏性，而l2-regularization 不行的原因了。而至于从理论上如何解释，请参考[Sparsity and Some Basics of L1 Regularization](http://freemind.pluskid.org/machine-learning/sparsity-and-some-basics-of-l1-regularization/)。

### 为什么2范数能得到最大间隔解?

[统计学习那些事](http://cos.name/2011/12/stories-about-statistical-learning/) 除了这篇文章，统计之都的很多文章再看看。

### 线性分类器
logistic regression本质是线性分类器，只不过在线性变化后通过sigmoid函数做了非线性变换，将值域映射到[0,1]。svm和lr都可以视作单层的neural network，也就是感知器的小变形。线性neural netword的能力是有限的，譬如xor问题。

说起神经网络，通常是指多层非线性的神经网络。对于lr，如果要有非线性，一般是我们人工生成非线性的特征；而对于nn，它可以自己生成非线性的表述(即特征)，也就是我们常说的learning representation。

[weibo link](http://weibo.com/1991303247/z8Q42rhD1)

### sigmoid
为什么sigmoid函数比别的非线性变换更有吸引力吗？

做sigmoid变换的目的是把（-inf，+inf）取值范围的信号（用x表示）映射到（0，1）范围内（用y表示）：y=h(x)。

寻找一个映射h能在有观测误差e的情况下最优的保持输入信号的信息，用信息论的语言描述就是x与y之间的互信息最大。当y服从均匀分布时熵最大(互信息最大)，因此能把x映射成一个服从均匀分布的变量y的映射h是最优的。

答案是：知道x的概率密度函数f(x)，那么对f(x)的积分可得累积分布函数F(x)，F就是最优的h，即y=F(x)最优。

Sigmoid变换对于概率密度函数呈倒置钟形的信号来说是最优变换，而现实中倒置钟形分布的信号又比比皆是，很多自然的信号都是这样，譬如高斯分布就很普遍。换句话说：对于概率密度不是倒置钟形的信号会有自己最优的变换，而不一定是sigmoid函数。

![](https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/sigmoid图.jpg)

@ICT山世光：我的歪解，它能压缩无用长尾巴，扩展核心分辨率

对y的等距量化，对应到输入信号x时是非等距量化，同样长的y区间，对应到x时区间是不等的。同样长度的y，在x高密度区域只能编码一小段，而在x低密度区域却编码一长段。

注释1：KL散度( Kullback–Leibler divergence)，又称相对熵（relative entropy)，是描述两个概率分布P和Q差异的一种方法。

设两个随机变量(X,Y)的联合分布为p(x,y)，边际分布分别为p(x),p(y)，互信息I(X,Y)是联合分布p(x,y)与乘积分布p(x)p(y)的相对熵。
H(X,Y)=H(X)+H(Y|X)=H(Y)+H(X|Y)，I(X;Y)=H(X)-H(X|Y)。
![](https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/kl_divergence.png)

注释2：如果用c的库函数rand()生成服从高斯分布或贝塔分布，or其他分布的随机数？应该有不少人知道答案吧。C函数rand可生成[0,1）均匀分布随机数，用一种叫inverse cumulative distribution function的变换方法可以生成任意分布的随机数。

[为什么我们喜欢用sigmoid这类S型非线性变换](http://www.52cs.org/?p=363)

### 无监督学习

无监督学习，在操作上比较直接可做的两件事有：1）聚类，把每个d维向量作为整体来研究向量空间上的结构，共N个instance；2）特征分析（降维等），把每个N维向量作为整体来研究，共d个instance。**两者是对偶关系，存在等价联系**，譬如PCA和K-Means之间的等价关系，当然理想情况是两者的结合。所谓两者的结合，就是同时在特征和样例两个维度上分析，分析哪些样例在哪些维度上是共同的因素主导的。**方法上PCA和K-Means向Topic Model的发展反映了这个趋势**。最根本上，无监督学习的目标是：给定观察数据，推理出数据产生的机制。

### 无监督分词

这里说的是这篇文章 [互联网时代的社会语言学：基于SNS的文本数据挖掘](http://www.matrix67.com/blog/archives/5044)

[原地址](http://weibo.com/1991303247/z8MMLq5zX)

### SVM神话

@余凯_西二旗民工 很多有关SVM的教科书都有misleading: (1)KKT条件，support vectors，quadratic programing都是浮云；（2）kernel本身对理解学习问题有帮助，但实际工程上用处为0；（3）hinge loss只是众多可选项之一，logistic效果一点不差。

有理论结果表明support vector数目随训练样本数目线性增长，所以，svm并不是真正稀疏。

看损失函数曲线，真看不出hinge loss比log loss好，只是hinge loss能得到稀疏解，看上去很美。

这里讨论了 loss function和nonlinear两个问题，只有从优化难度考虑时对hinge loss和 log loss的比较才有意义，后者可导，解无约束优化的各种梯度方法都可以上，前者要这么做只有用sub gradient。是否用kernel应主要考虑非线性是否有必要，复杂度是否可接受。

一般情况，维度大且稀疏，核函数基本无用。

[原地址](http://weibo.com/1991303247/z8MLB2fqw) [52cs link](http://www.52cs.org/?p=359)

## 计算机视觉

### 弱谈Marr视觉计算理论
Marr最大的贡献是提出应从理论、算法和实现三个层次去研究视觉，这种指导思想在计算机视觉领域只有少数人还在遵循，但在心理学、理论神经科学还是基本的研究范式。

[原地址](http://weibo.com/1991303247/z8LUXjrpe) [52cs link](http://www.52cs.org/?p=61)

## 实际应用

### 博弈论与广告

计算广告系统中关键词竞价就是一个典型的例子，什么ctr预估只是这个系统中做好做，最容易理解的一个模块，能用经济模型来统一研究这个系统才有挑战，譬如从广告主角度买什么关键词，出多少钱等。

Ctr预估是整个环节中最客观，最静态的部分，仅仅是match game，匹配query与ad而已，可期望有最优解。而其他环节是动态的，多角色参与的，短期利益和长期利益权衡的，好多因素，不知道现有系统中所采取的策略是否最优。

[weibo link](http://weibo.com/1991303247/z8Pjm7ekZ) [52cs link](http://www.52cs.org/?p=356)

### 对李国杰院士大数据一文的不同意见
其实大数据对模型、理论的需求更加迫切。我们说的学习都是归纳法，自动或半自动发现数据内在的统计规律，一旦发现规律就可以得到对数据更简短的描述，又称compact representation，所有的学习算法都是在做这件事情，所有的自然科学研究也都在做这个。

Learning representation当然好，机械模仿生物神经网络的结构和行为当然也有一定价值，但这还不够。好的representation一定是捕捉了数据regularity从而使表示又简洁又有效。但regularity的挖掘又何尝容易，搞机器学习的也无非是在一个假设空间找个还凑合的了事。Learning as compression、最小描述长度、奥卡姆剃刀、科尔莫格罗夫复杂性、描述复杂性等等都是这件事相关的学术思想。

从压缩的观点看，描述数据所需要的bit数分两部分构成，一部分是模型（规则）本身所需要的bit，一种是用模型解释数据时得到的残差所需要的bit数。（注意：有了模型，我们只需要描述残差和模型就可重构数据）。当模型（规则）和残差解释数据不会带来压缩效果时，就不要用模型了。

对于有的问题，大数据意味着，任何一个有益的模式机器都看到过，而且可能看到不止一次，当然不再需要模型。机器学习归根到底是记忆，只不过是稍微聪明的记忆而已，有一定的泛化能力，和人类的记忆还不能比。有的问题看上去高维、稀疏，那是因为对条件独立挖掘的不够。

[52cs link](http://www.52cs.org/?p=342)

## 学习机器学习

### 理想的机器学习书
吴军在讨论“规则与统计”，以及对阿米特-辛格之简单哲学的吹捧，显得很没境界。哪个流派称为显学，哪个没落，十年河东，十年河西而已，历史上这样的故事发生了很多次了。而且他在讨论“规则与统计”时捧统计抑规则，而在讨论艾米特辛格简单哲学时，捧规则抑统计，不知他信仰到底是什么？

李航的书对LR与MaxEnt之间的关系介绍得也不够理想。应引入指数族分布，给定一些约束求最大熵分布，数学结果是指数族分布；且参数结果与已知分布为指数族时的最大似然解等价。具有广泛实例的指数族竟是最大熵原理的自然结果，让人感叹自然界的奇迹，也内心欣赏这样一个连定理都称不上的原则的神奇。

物理意义。一定要给每一个数学结论都赋予物理意义。来龙去脉，给出每一个重要发展的来龙去脉、历史渊源或者八卦背景，学术点叫motivation。只有了解这些才能融会贯通，历史地理解一个方法或理论的地位、局限性等。

现在有一本Kevin Murphy出了一个 Machine Leaning：a probabilistic perspective，这本书不错，我写过一句话评论：complete but not comprehensive enough。 有些独特的东西，以后再谈。

[weibo link](http://weibo.com/1991303247/z8MqXeo3F) [52cs link](http://www.52cs.org/?p=70)


### Domingos 《机器学习那点事儿》解读

把不好吃的皮剥掉，论文精彩之处来了，有一些例子：1）基于10w词汇的垃圾邮件过滤；2）从100w样例学习含有100个布尔变量的表达式；3）学习析取范式；4）pac的阐述；5）高维空间反直觉举例；6）xor学习；7）n位校验函数学习。

[52cs link](http://www.52cs.org/?p=79)

### 也谈对机器学习的几点认识

[52cs link](http://www.52cs.org/?p=88)

### 关于如何学习

[52cs link](http://www.52cs.org/?p=85)

### 机器学习有没有用

[52cs link](http://www.52cs.org/?p=76)

### 机器学习无用论
[weibo link](http://weibo.com/1991303247/z8F4f9cBt)

## 深度学习

### Deep Learning
[weibo link](http://weibo.com/1991303247/z8QNjkV5l)

## 其他

### 规则与统计
[52cs link](http://www.52cs.org/?p=82)

### 跨界的机器学习
[52cs link](http://www.52cs.org/?p=73)

### 关于基
[52cs link](http://www.52cs.org/?p=333)

### 一些关于产品的想法
[weibo link](http://weibo.com/1991303247/z8PlL3B0J)

### 人机对话
[52cs link](http://www.52cs.org/?p=64)

### 优质数据
[原地址](http://1.guzili.sinaapp.com/?p=27)

### 学术界
[原地址](http://1.guzili.sinaapp.com/?p=30)

