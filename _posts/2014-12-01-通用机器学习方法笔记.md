---
layout: post
title: "通用机器学习方法笔记"
---

### 机器学习通用问题

- [Multi-task learning（多任务学习）简介](http://blog.csdn.net/u013854886/article/details/38425499)

	目前多任务学习方法大致可以总结为两类，一是不同任务之间共享相同的参数（common parameter），二是挖掘不同任务之间隐藏的共有数据特征（latent feature）

- 迁移学习

	- [迁移学习与自我学习](http://blog.csdn.net/jwh_bupt/article/details/8901261)
	- [迁移学习的相关概念](http://blog.csdn.net/jwh_bupt/article/details/9276165)
	- [阅读笔记：Boosting for Transfer learning](http://www.zhizhihu.com/html/y2011/2902.html)

	- 当前只有少量新的标记的数据，但是有大量旧的已标记的数据（甚至是其他类别的有效数据），这时通过挑选这些旧数据中的有效的数据，加入到当前的训练数据中，训练新的模型。

	- 迁移学习的目标是将从一个环境中学到的知识用来帮助新环境中的学习任务。

	- 例子：当教会了电脑学习区分大象和犀牛的能力后，电脑利用这一本领更快或更准确地来学习如何区分飞机和鸟。

	- 参考文献：
		- Boosting for transfer learning
		- Self-taught learning: transfer learning from unlabeled data
		- A survey on transfer learning. IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 22, NO. 10, OCTOBER 2010

- 强化学习(reinforcement learning)
	- [开源] reinforce —— Python下“即插即用”型强化学习(reinforcement learning)库 GitHub:http://t.cn/Rw0G3pI 其实现基于Andrew Ng的noteshttp://t.cn/Rw0qVLu 以及另一篇关于强化学习实现的文章《Reinforcement Learning》http://t.cn/Rw0GFdc

- 机器学习的那些事
	- [A few useful things to know about machine learning](http://www.valleytalk.org/wp-content/uploads/2012/11/机器学习那些事.pdf)
	- [机器学习的12堂课](https://breezedeus.github.io/2012/10/30/breezedeus-things-about-ml.html)
	- [机器学习常见的错误](http://www.douban.com/note/413022836/)
	- 学习=表示(representation)+评价(evaluation)+优化(optimization)；表示即为学习器的假设空间。所选的learner应该具有某种表达形式，通过learner最终获得的模型应该具有给定的这种特征(如分割面为超平面)。选定了这种特性，就相当于为learner选定了一组候选classifier。

	![](https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/machine_learning.png)

	- 泛化很重要。将泛化作为目标给机器学习带来一个有趣的结果。与其他大部分优化问题不同,机器学习无法获得希望优化的那个函数! 我们不得不用训练误差来代替测试误差。
	- 过拟合。解决过拟合的思路：交叉验证，正则项，在决定是否增加新的结构时进行诸如卡方测试等统计显著性检验 (statistical significance test), 用来决定类别分布是否会因为增 加这个结构而不同。
	![](https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/bias_and_variance.png)

	- 直觉不适用于高维空间。
	- 要学习很多模型, 而不仅仅是一个。model emsemble。
	- 简单并不意味着准确。应当先选择简单假设,这是因为简单本身就是一个优点,而不是因为所假设的与准确率有什么联系。
	- 相关并不意味着因果。
	- 可表示并不代表可学习。
	- 更多的数据打败更聪明的算法。
	- 特征工程是关键。
	- 理论上的保证不像你所想的那样。

- [机器学习综述——机器学习理论基础与发展脉络](http://xilinx.eetrend.com/article/8319)
	- 如果一个系统能够通过执行某个过程改进它的性能，这就是学习。从大量无序的信息到简洁有序的知识
		- 从有限观察概括特定问题世界模型的机器学习
		- 发现观测数据中暗含的各种关系的数据分析
		- 从观测数据挖掘有用知识的数据挖掘
	-  模型假设+模型选择+学习算法；常用的损失函数包括0-1损失、平方误差损失、绝对损失、对数损失等等。

		统计机器学习方法的三个问题都是非常值得研究的，对于模型假设这个问题，如果模型都选择错误，无论后面如何选择模型，也都难以反映数据集的正确分布。因此，首先需要选择对模型做出正确假设，如何选择模型的假设空间是一个学问，除掉交叉验证的方法之外还有不少其他方法。模型选择的关键在于如何设计损失函数，而损失函数通常包括损失项和正则化项，不同的模型选择策略通常选出的模型也非常不同，从而导致模型的预测效果也大大不同。学习算法比较定式，不同的学习算法不仅学习的效率不同，而且学习出来的效果也不一样。

	- SVM方法是通过一个非线性映射p，把样本空间映射到一个高维乃至无穷维的特征空间中（Hilber空间），使得在原来的样本空间中非线性可分的问题转化为在特征空间中的线性可分的问题。分类，回归等问题，很可能在低维样本空间无法线性处理的样本集，在高维特征空间中却可以通过一个线性超平面实现线性划分（或回归）。一般的升维都会带来计算的复杂化，SVM方法巧妙地解决了这个难题：应用核函数的展开定理，就不需要知道非线性映射的显式表达式；由于是在高维特征空间中建立线性学习机，所以与线性模型相比，不但几乎不增加计算的复杂性，而且在某种程度上避免了“维数灾难”．这一切要归功于核函数的展开和计算理论。

		SVM 的最终决策函数只由少数的支持向量所确定,计算的复杂性取决于支持向量的数目,而不是样本空间的维数,这在某种意义上避免了“维数灾难”

	-  增强机器学习，无监督学习，有监督学习，半监督学习，自我学习，迁移学习，多任务学习等等。这些学习方法的基本原理都要理解。

- [An Introduction to Supervised Machine Learning and Pattern Classification: The Big Picture](http://t.cn/Rw4Wz73) 很不错的监督机器学习(重点是模式分类)介绍资料 云:http://t.cn/Rw4WIrh

- [邹博讲EM、GMM](http://ask.julyedu.com/question/64) 被称赞为：“最清楚的一次GMM”，“老师讲的太好，太多人想听”等等。

- [关系提取 relation_extraction](https://github.com/mrlyk423/relation_extraction)。ransE通过h + r = t的目标学习知识图谱表示，效果引人关注。最近我组林衍凯同学针对TransE对1-N、N-1类型关系效果不佳的问题，提出TransR将实体映射到不同关系空间中构建优化目标，效果最高比TransE提升近20%。论文 http://t.cn/RZSpha8 ，实现TransE、TransH和TransR全部开源：http://t.cn/RZe9of4

- [文章]《Robust Machine Learning》http://t.cn/RZkX55C 针对机器学习算法如何在面对错误样本时更健壮的问题，作者讨论了几种角度的方案：用(相对)阈值过滤；针对性的特征选择；对每个特征建立查处模型，训练、验证、纠偏。联系这两天热议的深度学习对抗样本问题的解决，差不多也是这些角度。

- [IPN]《Advice for applying Machine Learning》http://t.cn/RZreqPu 为机器学习的使用提供了一些建议，包括可视化分析方法、机器学习算法的选取、过拟合及欠拟合的处理、大数据集的处理、各种损失函数的比较等，很实用。Andrew Ng的《Advice for applying Machine Learning》http://t.cn/RZdaera

- [文章]《All Models of Learning have Flaws》http://t.cn/zlvnrVW 机器学习模型的缺陷(抱怨)，讨论实际使用时各种模型的限制条件，很实用，推荐阅读

- [文章]《MACHINE LEARNING WORK-FLOW》机器学习工作流系列文章：1.综述:http://t.cn/R7ZP8C5 2.数据预处理:http://t.cn/RZrS1GN 3.特征抽取:http://t.cn/R7aQPbR 4.完整性检查和数据分割:http://t.cn/R7eAXgy 5.特征预处理:http://t.cn/RzLBuRk 目前就写到Part5，感兴趣可以持续关注该博主

- [课程]《Machine Learning - A introductory course on machine learning》http://t.cn/RZsmDSu USC的Fei Sha和Yan Liu的机器学习课程，讲义组织得不错，知识覆盖也比较全面，可看作机器学习的速成参考，推荐学习 云:http://t.cn/RZsuLXo

- 【机器学习入门教材】Max Welling教授在UCI讲授《机器学习》多年，他认为许多教材堆砌数学公式，缺乏对这些公式的解释。他2011年写了一本入门书，公式不多，但是对许多概念提供了符合直觉的解释。Max Welling的机器学习入门书免费下载：http://t.cn/RZiZ7BH 工业界人士、本科生、非CS专业学生均适合。

- [文章]《Machine Learning Overview》机器学习概述系列文章：Part1:http://t.cn/RZ1yEYg Part2.Logistic Regression http://t.cn/RZ1yetX Part3.Decision Trees and Random Forests http://t.cn/RZ1U5Zg Part4还没写，感兴趣可继续关注

- [幻灯]《Tutorial Slides by Andrew W. Moore》http://t.cn/RZmOG7c 由Google计算机科学家、曾任CMU教授的Andrew W. Moore整理的系列教程，内容覆盖统计数据挖掘的各个方面，包括概率基础、统计数据分析基础、经典机器学习算法和数据挖掘算法，是很好的学习材料。简短版目录列表:http://t.cn/zHiLcXa

- 一个非常好的电子书在线阅读及下载网站： http://t.cn/RP2JcJ3 ，已经收集书籍超过200多万，很多专业书籍均能下载到电子版，例如Bishop的PRML，Duda的PR经典教科书，Vapnik的统计学习理论的2本经典著作...

- [Machine Learning Done Wrong](http://www.52ml.net/15845.html) 作者总结了机器学习七种易犯的错误：1.想当然用缺省Loss；2.非线性情况下用线性模型；3.忘记Outlier；4.样本少时用High Viriance模型；5.不做标准化就用L1/L2等正则；6.不考虑线性相关直接用线性模型；7.LR模型中用参数绝对值判断feature重要性。

- Andrew Ng的《机器学习应用建议》：http://t.cn/h4b8MK 以及不莱梅博士Jan Hendrik Metzen与其意见相对应的一些Python交互式范例：http://t.cn/RZreqPu

- [文章]《New to Machine Learning? Avoid these three mistakes》http://t.cn/RwCyVD5 James Faghmous提醒机器学习初学者要避免的三方面错误，很棒的文章，推荐阅读

- [机器学习知识索引](http://metacademy.org/browse)

- [机器学习的11个开源项目](http://www.infoq.com/cn/news/2014/12/11-machine-learning-project) Scikit-learn，Shogun，Mlib，CUDA-Convnet，ConvNetJs等。

- [文章]《ML Pitfalls: Measuring Performance (Part 1)》http://t.cn/Rw8BTiX 机器学习那些坑之性能评价，浅显易懂的实操型文章，跟着过一遍Scikit-Learn也能熟悉熟悉。机器学习的门槛真是越来越低了，赶快学起来！

- Alice Zheng的最新教学博客http://t.cn/RA7rdBV介绍了如何做机器学习的模型评估。第一部分介绍了指标选择（RMSE， AUC, Precision-Recall...), 训练测试集生成（hold-out, cross-validation)，以及参数搜索的方法(random， grid)。机器学习入门值得一读！

- 【经典:十张图看机器学习】《Machine learning in 10 pictures》Deniz Yuret http://t.cn/8Fg56AN 很多朋友很早就推荐过，这里还是不厌其烦再推一下，每次阅读都有新体会，确实经典 @伯乐在线官方微博 的译文版《十张图解释机器学习的基本概念》http://t.cn/8s5j2h3 云(网页pdf):http://t.cn/RA5KkLW

### 机器学习历史

- 【译文：机器学习ML简史】http://t.cn/RwnEx45 【专稿：大数据简史】http://t.cn/RAZlmG3 【机器学习简史】http://t.cn/RZ9BpU5 【神经网络简史】http://t.cn/RZXn0iX

### 机器学习基本算法
- 正则化
	- 【机器学习中使用「正则化来防止过拟合」到底是一个什么原理？为什么正则化项就可以防止过拟合？】@刑无刀 : 数学原理我不细说，google能得到一大坨解释，以及其他各位牛人也都回答了，奥坎姆剃刀这类概念我也不说，相信题主早就知道。我想你就差一个… http://t.cn/RwRJx8J（分享自 @知乎）

- Classifier
	- 数据挖掘中分类算法小结：(1)决策树；(2) KNN法(K-Nearest Neighbor)；(3) SVM法；(4) VSM法；(5) Bayes法；(6)神经网络。http://t.cn/Rwve07S

	- [文章]《Machine Learning classifier gallery》http://t.cn/RwIGRn4 机器学习分类器示例图集，对几种典型分类器的决策区域进行横向和纵向可视化比较，很赞

	- [文章-Comparing supervised learning algorithms](http://www.dataschool.io/comparing-supervised-learning-algorithms/) 监督学习算法横向比较(表格)，包括KNN、Linear regression、Logistic regression、Naive Bayes、Decision trees、Random Forests、AdaBoost、Neural networks，很不错，推荐

	- [文章-An interview with "Box Plots for Education" winner Quoc Le](http://blog.drivendata.org/2015/02/26/box-plots-winner-interview-quoc-le/) “Box Plots for Education”竞赛优胜者Quoc Le访谈，关于个人、竞赛和数据科学

	- 推荐VW作者John Langford一篇有意思的博客 All Models of Learning have Flaws http://t.cn/zlvnrVW 作者以表格的方式，介绍了各种机器学习方法的概念，讨论了主要优点与不足之处。


- Sparse linear models
	- [视频]《Sparse Linear Models》http://t.cn/Rwjek2Q Stanford的Trevor Hastie教授在H2O.ai Meet-Up上的报告，讲稀疏线性模型——面向“宽数据”(特征维数超过样本数)的线性模型 云:http://t.cn/RwjDD7V 13年同主题报告:http://t.cn/RwjDD7I 讲义:http://t.cn/RwjDD7f


- LDA
	- 【LDA入门与Java实现】 这是一篇面向工程师的LDA入门笔记，并且提供一份开箱即用Java实现。本文只记录基本概念与原理，并不涉及公式推导。文中的LDA实现核心部分采用了arbylon的LdaGibbsSampler并力所能及地注解了，在搜狗分类语料库上测试良好，开源在GitHub上。什么… http://t.cn/RZBIEYh
	- [@Copper_PKU 推荐的35篇Topic Model论文](http://www.7300days.com/index.php/stds/topic/list/id/27/name/Topic%20modeling)
	- [Care and Feeding of Topic Models: Problems, Diagnostics, and Improvements](http://www.cs.colorado.edu/~jbg/docs/2014_book_chapter_care_and_feeding.pdf)  Dave Blei得意门生Jordan Boyd-Graber（科罗拉多大学助理教授）和博士后David Mimno（康奈尔助理教授）是公认的主题模型(Topic Model)专家。近日他们写了一个简短的主题模型入门介绍，并且讨论了主题模型的问题，评价手段，以及部分改进方法。
	- Labeled LDA：[话题模型（topic model）的提出及发展历史](http://blog.csdn.net/xianlingmao/article/details/7076165) Labeled LDA与LDA最大的不同是： LDA是在所有topics上为某一个词进行选择某个topic，而labeled LDA则是只从文档相关的label对应的topic中去选择，其余和LDA都是一样的。
	- [Topic Modeling for the Uninitiated](http://bugra.github.io/work/notes/2015-02-21/topic-modeling-for-the-uninitiated/)
	- [音频]《Topic models: Past, present, and future》http://t.cn/RwOQXK4 对话David M. Blei，讨论主题模型的过去、现在和未来 云:http://t.cn/RwOQ08t
	- [文章]《Automatic topic-modelling with Latent Dirichlet Allocation》http://t.cn/Rwr8JtB 很好的LDA介绍文章，内容包括LDA主题模型的原理、限制和相关开源工具 云(视频):http://t.cn/RwgPEI2
	- [FSTM(Fully sparse topic models)](https://www.jaist.ac.jp/~s1060203/codes/fstm/)


- CRF
	- CRF训练，但标注数据很少。感兴趣的朋友可以参考下Semi-supervised Sequence Labeling for Named Entity Extraction based on Tri-Training:Case Study on Chinese Person Name Extraction
	- [视频]《Log-linear Models and Conditional Random Fields》http://t.cn/SUGYtC Charles Elkan讲的对数线性模型和条件随机场，非常棒的教程 讲义:http://t.cn/RZ1kQ6A
	- http://t.cn/zO7uh30 推荐这个项目，虽然现在都流行 Deep Learning了， CRF 类方法还是很容易达到一个比较高的 Score， 这个项目 f-score 低了 0.7 % 但是速度 提升了 10倍，隐含的，可以处理更大量的样本数据。
	- PPT 来了！机器学习班第15次课，邹博讲条件随机场CRF的PPT 下载地址：http://t.cn/RzE4Oy8，第16次课，邹博讲PCA&SVD的PPT 下载地址：http://t.cn/RzE4OyQ，@sumnous_t 讲社区发现算法的PPT 下载地址：http://t.cn/RzE4OyR。顺便说句，sumnous还曾是算法班周六班的学员，一年下来，进步很大。分享！

- SVM
	- 【分类战车SVM】第五话：核函数（哦，这实在太神奇了！）→ http://t.cn/RZ0JICY
	- 【SVM之菜鸟实现】之前帖子有bug, 担心有朋友受其害，这里给出正确版，用matlab做伪代码: yp=x*w; idx=find(yp.*y<1) e="yp(idx)-y(idx);"   f="e'*e+c*w*w."  df="2(x(idx,:)'*e+c*w);" fdf="" lbfgslbfgs="" svmsgd="" em="">
	- [文章]《The Trouble with SVMs》http://t.cn/RwvkOx4 非常棒的强调特征选择对分类器重要性的文章。情感分类中，根据互信息对复杂高维特征降维再使用朴素贝叶斯分类器，取得了比SVM更理想的效果，训练和分类时间也大大降低——更重要的是，不必花大量时间在学习和优化SVM上——特征也一样no free lunch
	- [文章]《Linear SVM Classifier on Twitter User Recognition》http://t.cn/RwuEugE Python下用线性SVM分类器做Twitter作者预测(识别)
	- 【SVM 的简要推导过程】SVM 是一块很大的内容，网上有写得非常精彩的博客。这篇博客目的不是详细阐述每一个理论和细节，而在于在不丢失重要推导步骤的条件下从宏观上把握 SVM 的思路。via.daniel-D http://t.cn/RwBWENB

- Boost
	- 陈天奇的xgboost。[有监督代码心得](http://weibo.com/p/1001603795687165852957)，[xgboost](https://github.com/tqchen/xgboost) ，[分布式机器学习](http://weibo.com/p/1001603801281637563132)，[模板和张量库](http://weibo.com/p/1001603795728785912771)，[迭代器和流水处理](http://weibo.com/p/1001603795714256832384)，[tutorial](http://courses.cs.washington.edu/courses/cse546/14au/slides/oct22_recitation_boosted_trees.pdf)

- 聚类
	- [Science上发表的超赞聚类算法](http://www.52ml.net/16296.html)  一种很简洁优美的聚类算法, 可以识别各种形状的类簇, 并且其超参数很容易确定.

- Ensemble
	- [文章]《Ask a Data Scientist: Ensemble Methods》http://t.cn/RwoVO5O “Ask a Data Scientist.”系列文章之Ensemble Methods，通俗程度可以和昨天介绍的Quora随机森林解释相媲美，但更为详尽，对常用Ensemble框架及其特点也进行了介绍，很好

	- [IPN] Linear Regression http://t.cn/RwaDQb2、Decision Trees http://t.cn/RwakZSM和Ensembling http://t.cn/RwakwH6 三个不错的介绍性notebook，配有Python例子程序，内容改编自《An Introduction to Statistical Learning with Applications in R》http://t.cn/RwaklfK 这本书

	- [集成学习：机器学习刀光剑影 之 屠龙刀](http://www.52cs.org/?p=383)
		- Bagging和boosting也是当今两大杀器RF（Random Forests）和GBDT（Gradient Boosting Decision Tree）之所以成功的主要秘诀。
		- Bagging主要减小了variance，而Boosting主要减小了bias，而这种差异直接推动结合Bagging和Boosting的MultiBoosting的诞生。参考:Geoffrey I. Webb (2000). MultiBoosting: A Technique for Combining Boosting and Wagging. Machine Learning. Vol.40(No.2)
		- LMT(Logistic Model Tree ) 应运而生，它把LR和DT嫁接在一起，实现了两者的优势互补。对比GBDT和DT会发现GBDT较DT有两点好处：1）GBDT本身是集成学习的一种算法，效果可能较DT好；2）GBDT中的DT一般是Regression Tree，所以预测出来的绝对值本身就有比较意义，而LR能很好利用这个值。这是个非常大的优势，尤其是用到广告竞价排序的场景上。
		- 关于Facebook的GBDT+LR方法，它出发点简单直接，效果也好。但这个朴素的做法之后，有很多可以从多个角度来分析的亮点：可以是简单的stacking，也可以认为LR实际上对GBDT的所有树做了选择集成，还可以GBDT学习了基，甚至可以认为最后的LR实际对树做了稀疏求解，做了平滑。

- Factorization Machine
	- [Factorization Machines](http://www.ismll.uni-hildesheim.de/pub/pdfs/Rendle2010FM.pdf)
	- [Factorization Machines with libFM](http://www.ics.uci.edu/~smyth/courses/cs277/papers/factorization_machines_with_libFM.pdf)
	- [FM介绍](http://luowei828.blog.163.com/blog/static/310312042013101462926555)
	[开源] libFM —— 开源Factorization Machines(FM)工具 [GitHub](http://t.cn/Rh0QKfr) FM可对任意实值向量进行预测，可看作自动的特征选择/组合方法。参考文献：[中文简介](http://luowei828.blog.163.com/blog/static/310312042013101462926555) [开山之作](http://pan.baidu.com/s/1hqmZqE4) [KDD2012的Toturial](http://pan.baidu.com/s/1gdCJQdP)  [最新例子文章](https://thierrysilbermann.wordpress.com/2015/02/11/simple-libfm-example-part1/)

- Sigmoid
	- [为什么我们喜欢用sigmoid这类S型非线性变换？](http://www.52cs.org/?p=363)

- 数据分析
	- [数据分析 陷阱](http://1.guzili.sinaapp.com/?p=241)

- 时间序列
	- 这个方法omg做流量预测时用到过. [文章]《Auto-regression and Moving-average time series - Simplified》http://t.cn/RwRfXdX 自回归滑动平均(ARMA)时间序列的简单介绍，ARMA是研究时间序列的重要方法，由自回归模型（AR模型）与滑动平均模型（MA模型）为基础“混合”构成

