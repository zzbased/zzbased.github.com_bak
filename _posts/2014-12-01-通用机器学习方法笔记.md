---
layout: post
title: "通用机器学习方法笔记"
---

### 机器学习通用问题

- [Multi-task learning（多任务学习）简介](http://blog.csdn.net/u013854886/article/details/38425499)

	目前多任务学习方法大致可以总结为两类，一是不同任务之间共享相同的参数（common parameter），二是挖掘不同任务之间隐藏的共有数据特征（latent feature）

- 迁移学习

	- [迁移学习与自我学习](http://blog.csdn.net/jwh_bupt/article/details/8901261)
	- [迁移学习的相关概念](http://blog.csdn.net/jwh_bupt/article/details/9276165)
	- [阅读笔记：Boosting for Transfer learning](http://www.zhizhihu.com/html/y2011/2902.html)

	- 当前只有少量新的标记的数据，但是有大量旧的已标记的数据（甚至是其他类别的有效数据），这时通过挑选这些旧数据中的有效的数据，加入到当前的训练数据中，训练新的模型。

	- 迁移学习的目标是将从一个环境中学到的知识用来帮助新环境中的学习任务。

	- 例子：当教会了电脑学习区分大象和犀牛的能力后，电脑利用这一本领更快或更准确地来学习如何区分飞机和鸟。

	- 参考文献：
		- Boosting for transfer learning
		- Self-taught learning: transfer learning from unlabeled data
		- A survey on transfer learning. IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 22, NO. 10, OCTOBER 2010


- 机器学习的那些事
	- [A few useful things to know about machine learning](http://www.valleytalk.org/wp-content/uploads/2012/11/机器学习那些事.pdf)
	- [机器学习的12堂课](https://breezedeus.github.io/2012/10/30/breezedeus-things-about-ml.html)
	- [机器学习常见的错误](http://www.douban.com/note/413022836/)
	- 学习=表示(representation)+评价(evaluation)+优化(optimization)；表示即为学习器的假设空间。所选的learner应该具有某种表达形式，通过learner最终获得的模型应该具有给定的这种特征(如分割面为超平面)。选定了这种特性，就相当于为learner选定了一组候选classifier。

	![](https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/machine_learning.png)

	- 泛化很重要。将泛化作为目标给机器学习带来一个有趣的结果。与其他大部分优化问题不同,机器学习无法获得希望优化的那个函数! 我们不得不用训练误差来代替测试误差。
	- 过拟合。解决过拟合的思路：交叉验证，正则项，在决定是否增加新的结构时进行诸如卡方测试等统计显著性检验 (statistical significance test), 用来决定类别分布是否会因为增 加这个结构而不同。
	![](https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/bias_and_variance.png)

	- 直觉不适用于高维空间。
	- 要学习很多模型, 而不仅仅是一个。model emsemble。
	- 简单并不意味着准确。应当先选择简单假设,这是因为简单本身就是一个优点,而不是因为所假设的与准确率有什么联系。
	- 相关并不意味着因果。
	- 可表示并不代表可学习。
	- 更多的数据打败更聪明的算法。
	- 特征工程是关键。
	- 理论上的保证不像你所想的那样。

- [机器学习综述——机器学习理论基础与发展脉络](http://xilinx.eetrend.com/article/8319)
	- 如果一个系统能够通过执行某个过程改进它的性能，这就是学习。从大量无序的信息到简洁有序的知识
		- 从有限观察概括特定问题世界模型的机器学习
		- 发现观测数据中暗含的各种关系的数据分析
		- 从观测数据挖掘有用知识的数据挖掘
	-  模型假设+模型选择+学习算法；常用的损失函数包括0-1损失、平方误差损失、绝对损失、对数损失等等。

		统计机器学习方法的三个问题都是非常值得研究的，对于模型假设这个问题，如果模型都选择错误，无论后面如何选择模型，也都难以反映数据集的正确分布。因此，首先需要选择对模型做出正确假设，如何选择模型的假设空间是一个学问，除掉交叉验证的方法之外还有不少其他方法。模型选择的关键在于如何设计损失函数，而损失函数通常包括损失项和正则化项，不同的模型选择策略通常选出的模型也非常不同，从而导致模型的预测效果也大大不同。学习算法比较定式，不同的学习算法不仅学习的效率不同，而且学习出来的效果也不一样。

	- SVM方法是通过一个非线性映射p，把样本空间映射到一个高维乃至无穷维的特征空间中（Hilber空间），使得在原来的样本空间中非线性可分的问题转化为在特征空间中的线性可分的问题。分类，回归等问题，很可能在低维样本空间无法线性处理的样本集，在高维特征空间中却可以通过一个线性超平面实现线性划分（或回归）。一般的升维都会带来计算的复杂化，SVM方法巧妙地解决了这个难题：应用核函数的展开定理，就不需要知道非线性映射的显式表达式；由于是在高维特征空间中建立线性学习机，所以与线性模型相比，不但几乎不增加计算的复杂性，而且在某种程度上避免了“维数灾难”．这一切要归功于核函数的展开和计算理论。

		SVM 的最终决策函数只由少数的支持向量所确定,计算的复杂性取决于支持向量的数目,而不是样本空间的维数,这在某种意义上避免了“维数灾难”

	-  增强机器学习，无监督学习，有监督学习，半监督学习，自我学习，迁移学习，多任务学习等等。这些学习方法的基本原理都要理解。

- [An Introduction to Supervised Machine Learning and Pattern Classification: The Big Picture](http://t.cn/Rw4Wz73) 很不错的监督机器学习(重点是模式分类)介绍资料 云:http://t.cn/Rw4WIrh

- [邹博讲EM、GMM](http://ask.julyedu.com/question/64) 被称赞为：“最清楚的一次GMM”，“老师讲的太好，太多人想听”等等。

- [关系提取 relation_extraction](https://github.com/mrlyk423/relation_extraction)。ransE通过h + r = t的目标学习知识图谱表示，效果引人关注。最近我组林衍凯同学针对TransE对1-N、N-1类型关系效果不佳的问题，提出TransR将实体映射到不同关系空间中构建优化目标，效果最高比TransE提升近20%。论文 http://t.cn/RZSpha8 ，实现TransE、TransH和TransR全部开源：http://t.cn/RZe9of4

- [文章]《Robust Machine Learning》http://t.cn/RZkX55C 针对机器学习算法如何在面对错误样本时更健壮的问题，作者讨论了几种角度的方案：用(相对)阈值过滤；针对性的特征选择；对每个特征建立查处模型，训练、验证、纠偏。联系这两天热议的深度学习对抗样本问题的解决，差不多也是这些角度。

- [IPN]《Advice for applying Machine Learning》http://t.cn/RZreqPu 为机器学习的使用提供了一些建议，包括可视化分析方法、机器学习算法的选取、过拟合及欠拟合的处理、大数据集的处理、各种损失函数的比较等，很实用。Andrew Ng的《Advice for applying Machine Learning》http://t.cn/RZdaera

- [文章]《All Models of Learning have Flaws》http://t.cn/zlvnrVW 机器学习模型的缺陷(抱怨)，讨论实际使用时各种模型的限制条件，很实用，推荐阅读

- [文章]《MACHINE LEARNING WORK-FLOW》机器学习工作流系列文章：1.综述:http://t.cn/R7ZP8C5 2.数据预处理:http://t.cn/RZrS1GN 3.特征抽取:http://t.cn/R7aQPbR 4.完整性检查和数据分割:http://t.cn/R7eAXgy 5.特征预处理:http://t.cn/RzLBuRk 目前就写到Part5，感兴趣可以持续关注该博主

- [课程]《Machine Learning - A introductory course on machine learning》http://t.cn/RZsmDSu USC的Fei Sha和Yan Liu的机器学习课程，讲义组织得不错，知识覆盖也比较全面，可看作机器学习的速成参考，推荐学习 云:http://t.cn/RZsuLXo

- 【机器学习入门教材】Max Welling教授在UCI讲授《机器学习》多年，他认为许多教材堆砌数学公式，缺乏对这些公式的解释。他2011年写了一本入门书，公式不多，但是对许多概念提供了符合直觉的解释。Max Welling的机器学习入门书免费下载：http://t.cn/RZiZ7BH 工业界人士、本科生、非CS专业学生均适合。

- [文章]《Machine Learning Overview》机器学习概述系列文章：Part1:http://t.cn/RZ1yEYg Part2.Logistic Regression http://t.cn/RZ1yetX Part3.Decision Trees and Random Forests http://t.cn/RZ1U5Zg Part4还没写，感兴趣可继续关注

- [幻灯]《Tutorial Slides by Andrew W. Moore》http://t.cn/RZmOG7c 由Google计算机科学家、曾任CMU教授的Andrew W. Moore整理的系列教程，内容覆盖统计数据挖掘的各个方面，包括概率基础、统计数据分析基础、经典机器学习算法和数据挖掘算法，是很好的学习材料。简短版目录列表:http://t.cn/zHiLcXa

- 一个非常好的电子书在线阅读及下载网站： http://t.cn/RP2JcJ3 ，已经收集书籍超过200多万，很多专业书籍均能下载到电子版，例如Bishop的PRML，Duda的PR经典教科书，Vapnik的统计学习理论的2本经典著作...

- [Machine Learning Done Wrong](http://www.52ml.net/15845.html) 作者总结了机器学习七种易犯的错误：1.想当然用缺省Loss；2.非线性情况下用线性模型；3.忘记Outlier；4.样本少时用High Viriance模型；5.不做标准化就用L1/L2等正则；6.不考虑线性相关直接用线性模型；7.LR模型中用参数绝对值判断feature重要性。

- Andrew Ng的《机器学习应用建议》：http://t.cn/h4b8MK 以及不莱梅博士Jan Hendrik Metzen与其意见相对应的一些Python交互式范例：http://t.cn/RZreqPu

### 机器学习基本算法

- Classifier
	- 数据挖掘中分类算法小结：(1)决策树；(2) KNN法(K-Nearest Neighbor)；(3) SVM法；(4) VSM法；(5) Bayes法；(6)神经网络。http://t.cn/Rwve07S

- LDA
	- 【LDA入门与Java实现】 这是一篇面向工程师的LDA入门笔记，并且提供一份开箱即用Java实现。本文只记录基本概念与原理，并不涉及公式推导。文中的LDA实现核心部分采用了arbylon的LdaGibbsSampler并力所能及地注解了，在搜狗分类语料库上测试良好，开源在GitHub上。什么… http://t.cn/RZBIEYh
	- [@Copper_PKU 推荐的35篇Topic Model论文](http://www.7300days.com/index.php/stds/topic/list/id/27/name/Topic%20modeling)
	- [Care and Feeding of Topic Models: Problems, Diagnostics, and Improvements](http://www.cs.colorado.edu/~jbg/docs/2014_book_chapter_care_and_feeding.pdf)  Dave Blei得意门生Jordan Boyd-Graber（科罗拉多大学助理教授）和博士后David Mimno（康奈尔助理教授）是公认的主题模型(Topic Model)专家。近日他们写了一个简短的主题模型入门介绍，并且讨论了主题模型的问题，评价手段，以及部分改进方法。


- CRF
	- CRF训练，但标注数据很少。感兴趣的朋友可以参考下Semi-supervised Sequence Labeling for Named Entity Extraction based on Tri-Training:Case Study on Chinese Person Name Extraction
	- [视频]《Log-linear Models and Conditional Random Fields》http://t.cn/SUGYtC Charles Elkan讲的对数线性模型和条件随机场，非常棒的教程 讲义:http://t.cn/RZ1kQ6A
	- http://t.cn/zO7uh30 推荐这个项目，虽然现在都流行 Deep Learning了， CRF 类方法还是很容易达到一个比较高的 Score， 这个项目 f-score 低了 0.7 % 但是速度 提升了 10倍，隐含的，可以处理更大量的样本数据。
	- PPT 来了！机器学习班第15次课，邹博讲条件随机场CRF的PPT 下载地址：http://t.cn/RzE4Oy8，第16次课，邹博讲PCA&SVD的PPT 下载地址：http://t.cn/RzE4OyQ，@sumnous_t 讲社区发现算法的PPT 下载地址：http://t.cn/RzE4OyR。顺便说句，sumnous还曾是算法班周六班的学员，一年下来，进步很大。分享！


- SVM
	- 【分类战车SVM】第五话：核函数（哦，这实在太神奇了！）→ http://t.cn/RZ0JICY
	- 【SVM之菜鸟实现】之前帖子有bug, 担心有朋友受其害，这里给出正确版，用matlab做伪代码: yp=x*w; idx=find(yp.*y<1) e="yp(idx)-y(idx);"   f="e'*e+c*w*w."  df="2(x(idx,:)'*e+c*w);" fdf="" lbfgslbfgs="" svmsgd="" em="">
	- [文章]《The Trouble with SVMs》http://t.cn/RwvkOx4 非常棒的强调特征选择对分类器重要性的文章。情感分类中，根据互信息对复杂高维特征降维再使用朴素贝叶斯分类器，取得了比SVM更理想的效果，训练和分类时间也大大降低——更重要的是，不必花大量时间在学习和优化SVM上——特征也一样no free lunch

- Boost
	- 陈天奇的xgboost。[有监督代码心得](http://weibo.com/p/1001603795687165852957)，[xgboost](https://github.com/tqchen/xgboost) ，[分布式机器学习](http://weibo.com/p/1001603801281637563132)，[模板和张量库](http://weibo.com/p/1001603795728785912771)，[迭代器和流水处理](http://weibo.com/p/1001603795714256832384)，[tutorial](http://courses.cs.washington.edu/courses/cse546/14au/slides/oct22_recitation_boosted_trees.pdf)

- 聚类
	- [Science上发表的超赞聚类算法](http://www.52ml.net/16296.html)  一种很简洁优美的聚类算法, 可以识别各种形状的类簇, 并且其超参数很容易确定.

