<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>100的情怀 - 技术博客</title>
    <description>机器学习，自然语言处理，计算广告学，工作与生活，总结与温习
</description>
    <link>http://yourdomain.com/</link>
    <atom:link href="http://yourdomain.com/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sat, 21 Feb 2015 20:10:40 +0800</pubDate>
    <lastBuildDate>Sat, 21 Feb 2015 20:10:40 +0800</lastBuildDate>
    <generator>Jekyll v2.5.3</generator>
    
      <item>
        <title>Welcome to Jekyll!</title>
        <description>&lt;p&gt;You’ll find this post in your &lt;code&gt;_posts&lt;/code&gt; directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run &lt;code&gt;jekyll serve&lt;/code&gt;, which launches a web server and auto-regenerates your site when a file is updated.&lt;/p&gt;

&lt;p&gt;To add new posts, simply add a file in the &lt;code&gt;_posts&lt;/code&gt; directory that follows the convention &lt;code&gt;YYYY-MM-DD-name-of-post.ext&lt;/code&gt; and includes the necessary front matter. Take a look at the source for this post to get an idea about how it works.&lt;/p&gt;

&lt;p&gt;Jekyll also offers powerful support for code snippets:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;Hi, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;#{&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Tom&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#=&amp;gt; prints &amp;#39;Hi, Tom&amp;#39; to STDOUT.&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Check out the &lt;a href=&quot;http://jekyllrb.com&quot;&gt;Jekyll docs&lt;/a&gt; for more info on how to get the most out of Jekyll. File all bugs/feature requests at &lt;a href=&quot;https://github.com/jekyll/jekyll&quot;&gt;Jekyll’s GitHub repo&lt;/a&gt;. If you have questions, you can ask them on &lt;a href=&quot;https://github.com/jekyll/jekyll-help&quot;&gt;Jekyll’s dedicated Help repository&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Sun, 22 Feb 2015 02:53:25 +0800</pubDate>
        <link>http://yourdomain.com/jekyll/update/2015/02/22/welcome-to-jekyll.html</link>
        <guid isPermaLink="true">http://yourdomain.com/jekyll/update/2015/02/22/welcome-to-jekyll.html</guid>
        
        
        <category>jekyll</category>
        
        <category>update</category>
        
      </item>
    
      <item>
        <title>sampling</title>
        <description>&lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;h2&gt;参考资料&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://www.cnblogs.com/xbinworld/p/4266146.html&quot;&gt;随机采样方法整理与讲解&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.cnblogs.com/daniel-D/p/3388724.html&quot;&gt;从随机过程到马尔科夫链蒙特卡洛方法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.cs.ubc.ca/%7Earnaud/andrieu_defreitas_doucet_jordan_intromontecarlomachinelearning.pdf&quot;&gt;An Introduction to MCMC for Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://vcla.stat.ucla.edu/old/MCMC/MCMC_tutorial/Lect2_Basic_MCMC.pdf&quot;&gt;Markov chain Monte Carlo Basics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://vcla.stat.ucla.edu/old/MCMC/MCMC_tutorial.htm&quot;&gt;Markov Chain Monte Carlo for Computer Vision &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www-scf.usc.edu/%7Emohammab/sampling.pdf&quot;&gt;Sampling Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.bb.ustc.edu.cn/jpkc/xiaoji/jswl/skja/chapter2-3a.pdf&quot;&gt;任意分布的伪随机变量的抽样&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;蒙特卡洛数值积分&lt;/h2&gt;

&lt;p&gt;求f(x)的积分，如\(\int&lt;em&gt;a^b{f(x)dx}\)。如果f(x)形式比较复杂，则可以通过数值解法来求近似的结果。常用的方法是：蒙特卡洛积分。
$$\int&lt;/em&gt;a^b{\frac{f(x)}{q(x)}q(x)dx}$$
这样把q(x)看做是x在区间[a,b]内的概率分布，而把前面的分数部分看做是一个函数，在q(x)下随机抽取n个样本，当n足够大时，可以用均值来近似：\(\frac{1}{n}\sum&lt;em&gt;{i=1}^n{\frac{f(x&lt;/em&gt;i)}{q(x_i)}}\)。只要q(x)比较容易采样就可以了。&lt;/p&gt;

&lt;p&gt;随机模拟方法的核心就是如何对一个概率分布得到样本，即抽样(sampling)。&lt;/p&gt;

&lt;h2&gt;均匀分布&lt;/h2&gt;

&lt;p&gt;$$x&lt;em&gt;{n+1}=(ax&lt;/em&gt;n+c)\mod m$$&lt;/p&gt;

&lt;h2&gt;Box-Muller 变换&lt;/h2&gt;

&lt;p&gt;如果随机变量\(U&lt;em&gt;1,U&lt;/em&gt;2\)独立，且U&lt;em&gt;1,U&lt;/em&gt;2 ~ Uniform[0,1]
$$Z&lt;em&gt;0=\sqrt{-2lnU&lt;/em&gt;1}\cos{(2\pi U&lt;em&gt;2)}$$
$$Z&lt;/em&gt;1=\sqrt{-2lnU&lt;em&gt;1}\sin{(2\pi U&lt;/em&gt;2)}$$
则\(Z&lt;em&gt;0，Z&lt;/em&gt;1\)独立且服从标准正态分布。&lt;/p&gt;

&lt;h2&gt;接受-拒绝抽样(Acceptance-Rejection sampling)&lt;/h2&gt;

&lt;h2&gt;重要性抽样(Importance sampling)&lt;/h2&gt;

&lt;h2&gt;马尔科夫链，马尔科夫稳态&lt;/h2&gt;

&lt;h2&gt;MCMC-Gibbs sampling算法&lt;/h2&gt;
</description>
        <pubDate>Fri, 06 Feb 2015 00:00:00 +0800</pubDate>
        <link>http://yourdomain.com/2015/02/06/sampling.html</link>
        <guid isPermaLink="true">http://yourdomain.com/2015/02/06/sampling.html</guid>
        
        
      </item>
    
      <item>
        <title>语义分析的一些方法</title>
        <description>&lt;p&gt;author: vincentyao@tencent.com&lt;/p&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;p&gt;语义分析，本文指运用各种机器学习方法，挖掘与学习文本、图片等的深层次概念。wikipedia上的解释：In machine learning, semantic analysis of a corpus is the task of building structures that approximate concepts from a large set of documents(or images)。&lt;/p&gt;

&lt;p&gt;工作这几年，陆陆续续实践过一些项目，有搜索广告，社交广告，微博广告，品牌广告，内容广告等。要使我们广告平台效益最大化，首先需要理解用户，Context(将展示广告的上下文)和广告，才能将最合适的广告展示给用户。而这其中，就离不开对用户，对上下文，对广告的语义分析，由此催生了一些子项目，例如文本语义分析，图片语义理解，语义索引，短串语义关联，用户广告语义匹配等。&lt;/p&gt;

&lt;p&gt;接下来我将写一写我所认识的语义分析的一些方法，虽说我们在做的时候，效果导向居多，方法理论理解也许并不深入，不过权当个人知识点总结，有任何不当之处请指正，谢谢。&lt;/p&gt;

&lt;p&gt;本文主要由以下四部分组成：文本基本处理，文本语义分析，图片语义分析，语义分析小结。先讲述文本处理的基本方法，这构成了语义分析的基础。接着分文本和图片两节讲述各自语义分析的一些方法，值得注意的是，虽说分为两节，但文本和图片在语义分析方法上有很多共通与关联。最后我们简单介绍下语义分析在广点通&amp;quot;用户广告匹配&amp;quot;上的应用，并展望一下未来的语义分析方法。&lt;/p&gt;

&lt;h3&gt;1 文本基本处理&lt;/h3&gt;

&lt;p&gt;在讲文本语义分析之前，我们先说下文本基本处理，因为它构成了语义分析的基础。而文本处理有很多方面，考虑到本文主题，这里只介绍中文分词以及Term Weighting。&lt;/p&gt;

&lt;h4&gt;1.1 中文分词&lt;/h4&gt;

&lt;p&gt;拿到一段文本后，通常情况下，首先要做分词。分词的方法一般有如下几种：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;基于字符串匹配的分词方法。此方法按照不同的扫描方式，逐个查找词库进行分词。根据扫描方式可细分为：正向最大匹配，反向最大匹配，双向最大匹配，最小切分(即最短路径)；总之就是各种不同的启发规则。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;全切分方法。它首先切分出与词库匹配的所有可能的词，再运用统计语言模型决定最优的切分结果。它的优点在于可以解决分词中的歧义问题。下图是一个示例，对于文本串&amp;quot;南京市长江大桥&amp;quot;，首先进行词条检索(一般用Trie存储)，找到匹配的所有词条（南京，市，长江，大桥，南京市，长江大桥，市长，江大桥，江大，桥），以词网格(word lattices)形式表示，接着做路径搜索，基于统计语言模型(例如n-gram)[18]找到最优路径，最后可能还需要命名实体识别。下图中&amp;quot;南京市 长江 大桥&amp;quot;的语言模型得分，即P(南京市，长江，大桥)最高，则为最优切分。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/rnnlm1.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;图1. &amp;quot;南京市长江大桥&amp;quot;语言模型得分&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;由字构词的分词方法。可以理解为字的分类问题，也就是自然语言处理中的sequence labeling问题，通常做法里利用HMM，MAXENT，MEMM，CRF等预测文本串每个字的tag[62]，譬如B，E，I，S，这四个tag分别表示：beginning, ending, inside, single，也就是一个词的开始，结束，中间，以及单个字的词。 例如&amp;quot;南京市长江大桥&amp;quot;的标注结果可能为：&amp;quot;南(B)京(I)市(E)长(B)江(E)大(B)桥(E)&amp;quot;&lt;/p&gt;

&lt;p&gt;由于CRF既可以像最大熵模型一样加各种领域feature，又避免了HMM的齐次马尔科夫假设，所以基于CRF的分词目前是效果最好的，具体请参考文献[61,62,63]。&lt;/p&gt;

&lt;p&gt;除了HMM，CRF等模型，分词也可以基于深度学习方法来做，如文献[9][10]所介绍，也取得了state-of-the-art的结果。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/word_segmentation.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;图2. 基于深度学习的中文分词&lt;/p&gt;

&lt;p&gt;上图是一个基于深度学习的分词示例图。我们从上往下看，首先对每一个字进行Lookup Table，映射到一个固定长度的特征向量(这里可以利用词向量，boundary entropy，accessor variety等)；接着经过一个标准的神经网络，分别是linear，sigmoid，linear层，对于每个字，预测该字属于B,E,I,S的概率；最后输出是一个矩阵，矩阵的行是B,E,I,S 4个tag，利用viterbi算法就可以完成标注推断，从而得到分词结果。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;一个文本串除了分词，还需要做词性标注，命名实体识别，新词发现等。通常有两种方案，一种是pipeline approaches，就是先分词，再做词性标注；另一种是joint approaches，就是把这些任务用一个模型来完成。有兴趣可以参考文献[9][62]等。&lt;/p&gt;

&lt;p&gt;一般而言，方法一和方法二在工业界用得比较多，方法三因为采用复杂的模型，虽准确率相对高，但耗时较大。在腾讯内部采用的是方法二。&lt;/p&gt;

&lt;h4&gt;1.2 语言模型&lt;/h4&gt;

&lt;p&gt;前面在讲&amp;quot;全切分分词&amp;quot;方法时，提到了语言模型，并且通过语言模型，还可以引出词向量，所以这里把语言模型简单阐述一下。&lt;/p&gt;

&lt;p&gt;语言模型是用来计算一个句子产生概率的概率模型，即\(P(w&lt;em&gt;1,w&lt;/em&gt;2,w&lt;em&gt;3...w&lt;/em&gt;m) \)，m表示词的总个数。根据贝叶斯公式，\(P(w&lt;em&gt;1,w&lt;/em&gt;2,w&lt;em&gt;3 ... w&lt;/em&gt;m) = P(w&lt;em&gt;1)P(w&lt;/em&gt;2|w&lt;em&gt;1)P(w&lt;/em&gt;3|w&lt;em&gt;1,w&lt;/em&gt;2) ... P(w&lt;em&gt;m|w&lt;/em&gt;1,w&lt;em&gt;2 ... w&lt;/em&gt;{m-1})\)。&lt;/p&gt;

&lt;p&gt;最简单的语言模型是N-Gram，它利用马尔科夫假设，认为句子中每个单词只与其前n-1个单词有关，即假设产生\(w&lt;em&gt;m\)这个词的条件概率只依赖于前n-1个词，则有\(P(w&lt;/em&gt;m|w&lt;em&gt;1,w&lt;/em&gt;2...w&lt;em&gt;{m-1}) = P(w&lt;/em&gt;m|w&lt;em&gt;{m-n+1},w&lt;/em&gt;{m-n+2} ... w_{m-1})\)。其中n越大，模型可区别性越强，n越小，模型可靠性越高。&lt;/p&gt;

&lt;p&gt;N-Gram语言模型简单有效，但是它只考虑了词的位置关系，没有考虑词之间的相似度，词法和词语义，并且还存在数据稀疏的问题，所以后来，又逐渐提出更多的语言模型，例如Class-based ngram model，topic-based ngram model，cache-based ngram model，skipping ngram model，指数语言模型（最大熵模型，条件随机域模型）等。若想了解更多请参考文章[18]。&lt;/p&gt;

&lt;p&gt;最近，随着深度学习的兴起，神经网络语言模型也变得火热[4]。用神经网络训练语言模型的经典之作，要数Bengio等人发表的《A Neural Probabilistic Language Model》[3]，它也是基于n-gram的，首先将每个单词\(w&lt;em&gt;{m-n+1},w&lt;/em&gt;{m-n+2} ... w&lt;em&gt;{m-1}\)映射到词向量空间，再把各个单词的词向量组合成一个更大的向量作为神经网络输入，输出是\(P(w&lt;/em&gt;m)\)。本文将此模型简称为ffnnlm（Feed-forward Neural Net Language Model）。ffnnlm解决了传统n-gram的两个缺陷：(1)词语之间的相似性可以通过词向量来体现；(2)自带平滑功能。文献[3]不仅提出神经网络语言模型，还顺带引出了词向量，关于词向量，后文将再细述。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/ffnnlm.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;图3. 基于神经网络的语言模型&lt;/p&gt;

&lt;p&gt;从最新文献看，目前state-of-the-art语言模型应该是基于循环神经网络(recurrent neural network)的语言模型，简称rnnlm[5][6]。循环神经网络相比于传统前馈神经网络，其特点是：可以存在有向环，将上一次的输出作为本次的输入。而rnnlm和ffnnlm的最大区别是：ffnnmm要求输入的上下文是固定长度的，也就是说n-gram中的 n 要求是个固定值，而rnnlm不限制上下文的长度，可以真正充分地利用所有上文信息来预测下一个词，本次预测的中间隐层信息(例如下图中的context信息)可以在下一次预测里循环使用。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/simple_rnn.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;图4. 基于simple RNN(time-delay neural network)的语言模型&lt;/p&gt;

&lt;p&gt;如上图所示，这是一个最简单的rnnlm，神经网络分为三层，第一层是输入层，第二层是隐藏层(也叫context层)，第三层输出层。
假设当前是t时刻，则分三步来预测\(P(w_m)\)：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;单词\(w_{m-1}\)映射到词向量，记作input(t)&lt;/li&gt;
&lt;li&gt;连接上一次训练的隐藏层context(t-1)，经过sigmoid function，生成当前t时刻的context(t)&lt;/li&gt;
&lt;li&gt;利用softmax function，预测\(P(w_m)\)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;参考文献[7]中列出了一个rnnlm的library，其代码紧凑。利用它训练中文语言模型将很简单，上面&amp;quot;南京市 长江 大桥&amp;quot;就是rnnlm的预测结果。&lt;/p&gt;

&lt;p&gt;基于RNN的language model利用BPTT(BackPropagation through time)算法比较难于训练，原因就是深度神经网络里比较普遍的vanishing gradient问题[55]（在RNN里，梯度计算随时间成指数倍增长或衰减，称之为Exponential Error Decay）。所以后来又提出基于LSTM(Long short term memory)的language model，LSTM也是一种RNN网络，关于LSTM的详细介绍请参考文献[54,49,52]。LSTM通过网络结构的修改，从而避免vanishing gradient问题，具体分析请参考文献[83]。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/lstm_unit.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;图5. LSTM memory cell&lt;/p&gt;

&lt;p&gt;如上图所示，是一个LSTM unit。如果是传统的神经网络unit，output activation bi = activation_function(ai)，但LSTM unit的计算相对就复杂些了，它保存了该神经元上一次计算的结果，通过input gate，output gate，forget gate来计算输出，具体过程请参考文献[53，54]。&lt;/p&gt;

&lt;h4&gt;1.3 Term Weighting&lt;/h4&gt;

&lt;h5&gt;Term重要性&lt;/h5&gt;

&lt;p&gt;对文本分词后，接下来需要对分词后的每个term计算一个权重，重要的term应该给与更高的权重。举例来说，&amp;quot;什么产品对减肥帮助最大？&amp;quot;的term weighting结果可能是: &amp;quot;什么 0.1，产品 0.5，对 0.1，减肥 0.8，帮助 0.3，最大 0.2&amp;quot;。Term weighting在文本检索，文本相关性，核心词提取等任务中都有重要作用。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Term weighting的打分公式一般由三部分组成：local，global和normalization [1,2]。即
\(TermWeight=L&lt;em&gt;{i,j} G&lt;/em&gt;i N&lt;em&gt;j\)。\(L&lt;/em&gt;{i,j}\)是term i在document j中的local weight，\(G&lt;em&gt;i\)是term i的global weight，\(N&lt;/em&gt;j\)是document j的归一化因子。
常见的local，global，normalization weight公式[2]有：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/local_weight.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;图6. Local weight formulas&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/global_weight.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;图7. Global weight formulas&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/normlization_weight.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;图8. Normalization factors&lt;/p&gt;

&lt;p&gt;Tf-Idf是一种最常见的term weighting方法。在上面的公式体系里，Tf-Idf的local weight是FREQ，glocal weight是IDFB，normalization是None。tf是词频，表示这个词出现的次数。df是文档频率，表示这个词在多少个文档中出现。idf则是逆文档频率，idf=log(TD/df)，TD表示总文档数。tf-idf在很多场合都很有效，但缺点也比较明显，以&amp;quot;词频&amp;quot;度量重要性，不够全面，譬如在搜索广告的关键词匹配时就不够用。&lt;/p&gt;

&lt;p&gt;除了TF-IDF外，还有很多其他term weighting方法，例如Okapi，MI，LTU，ATC，TF-ICF[59]等。通过local，global，normalization各种公式的组合，可以生成不同的term weighting计算方法。不过上面这些方法都是无监督计算方法，有一定程度的通用性，但在一些特定场景里显得不够灵活，不够准确，所以可以基于有监督机器学习方法来拟合term weighting结果。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/okapi.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;图9. Okapi计算公式&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;利用有监督机器学习方法来预测weight。这里类似于机器学习的分类任务，对于文本串的每个term，预测一个[0,1]的得分，得分越大则term重要性越高。&lt;/p&gt;

&lt;p&gt;既然是有监督学习，那么就需要训练数据。如果采用人工标注的话，极大耗费人力，所以可以采用训练数据自提取的方法，利用程序从搜索日志里自动挖掘。从海量日志数据里提取隐含的用户对于term重要性的标注，得到的训练数据将综合亿级用户的&amp;quot;标注结果&amp;quot;，覆盖面更广，且来自于真实搜索数据，训练结果与标注的目标集分布接近，训练数据更精确。下面列举三种方法(除此外，还有更多可以利用的方法)：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;从搜索session数据里提取训练数据，用户在一个检索会话中的检索核心意图是不变的，提取出核心意图所对应的term，其重要性就高。&lt;/li&gt;
&lt;li&gt;从历史短串关系资源库里提取训练数据，短串扩展关系中，一个term出现的次数越多，则越重要。&lt;/li&gt;
&lt;li&gt;从搜索广告点击日志里提取训练数据，query与bidword共有term的点击率越高，它在query中的重要程度就越高。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;通过上面的方法，可以提取到大量质量不错的训练数据（数十亿级别的数据，这其中可能有部分样本不准确，但在如此大规模数据情况下，绝大部分样本都是准确的）。&lt;/p&gt;

&lt;p&gt;有了训练数据，接下来提取特征，基于逻辑回归模型来预测文本串中每个term的重要性。所提取的特征包括：
- term的自解释特征，例如term专名类型，term词性，term idf，位置特征，term的长度等；
- term与文本串的交叉特征，例如term与文本串中其他term的字面交叉特征，term转移到文本串中其他term的转移概率特征，term的文本分类、topic与文本串的文本分类、topic的交叉特征等。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h5&gt;核心词、关键词提取&lt;/h5&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;短文本串的核心词提取。对短文本串分词后，利用上面介绍的term weighting方法，获取term weight后，取一定的阈值，就可以提取出短文本串的核心词。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;长文本串(譬如web page)的关键词提取。这里简单介绍几种方法。想了解更多，请参考文献[69]。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;采用基于规则的方法。考虑到位置特征，网页特征等。&lt;/li&gt;
&lt;li&gt;基于广告主购买的bidword和高频query建立多模式匹配树，在长文本串中进行全字匹配找出候选关键词，再结合关键词weight，以及某些规则找出优质的关键词。&lt;/li&gt;
&lt;li&gt;类似于有监督的term weighting方法，也可以训练关键词weighting的模型。&lt;/li&gt;
&lt;li&gt;基于文档主题结构的关键词抽取，具体可以参考文献[71]。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;2 文本语义分析&lt;/h3&gt;

&lt;p&gt;前面讲到一些文本基本处理方法。一个文本串，对其进行分词和重要性打分后（当然还有更多的文本处理任务），就可以开始更高层的语义分析任务。&lt;/p&gt;

&lt;h4&gt;2.1 Topic Model&lt;/h4&gt;

&lt;p&gt;首先介绍主题模型。说到主题模型，第一时间会想到pLSA，NMF，LDA。关于这几个目前业界最常用的主题模型，已经有相当多的介绍了，譬如文献[60，64]。在这里，主要想聊一下主题模型的应用以及最新进展(考虑到LDA是pLSA的generalization，所以下面只介绍LDA)。&lt;/p&gt;

&lt;h5&gt;LDA训练算法简单介绍&lt;/h5&gt;

&lt;p&gt;LDA的推导这里略过不讲，具体请参考文献[64]。下面我们主要看一下怎么训练LDA。&lt;/p&gt;

&lt;p&gt;在Blei的原始论文中，使用variational inference和EM算法进行LDA推断(与pLSA的推断过程类似，E-step采用variational inference)，但EM算法可能推导出局部最优解，且相对复杂。目前常用的方法是基于gibbs sampling来做[57]。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Step1: 随机初始化每个词的topic，并统计两个频率计数矩阵：Doc-Topic 计数矩阵N(t,d)，描述每个文档中的主题频率分布；Word-Topic 计数矩阵N(w,t)，表示每个主题下词的频率分布。&lt;/li&gt;
&lt;li&gt;Step2: 遍历训练语料，按照概率公式(下图所示)重新采样每个词所对应的topic, 更新N(t,d)和N(w,t)的计数。&lt;/li&gt;
&lt;li&gt;Step3: 重复 step2，直到模型收敛。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对文档d中词w的主题z进行重新采样的公式有非常明确的物理意义，表示为P(w|z)P(z|d)，直观的表示为一个“路径选择”的过程。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/lda_sampling.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;图10. gibbs sampling过程图&lt;/p&gt;

&lt;p&gt;以上描述过程具体请参考文献[65]。&lt;/p&gt;

&lt;p&gt;对于LDA模型的更多理论介绍，譬如如何实现正确性验证，请参考文献[68]，而关于LDA模型改进，请参考Newman团队的最新文章《Care and Feeding of Topic Models》[12]。&lt;/p&gt;

&lt;h5&gt;主题模型的应用点&lt;/h5&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;在广点通内部，主题模型已经在很多方面都得到成功应用[65]，譬如文本分类特征，相关性计算，ctr预估，精确广告定向，矩阵分解等。具体来说，基于主题模型，可以计算出文本，用户的topic分布，将其当作pctr，relevance的特征，还可以将其当作一种矩阵分解的方法，用于降维，推荐等。不过在我们以往的成功运用中，topic模型比较适合用做某些机器学习任务的特征，而不适合作为一种独立的方法去解决某种特定的问题，例如触发，分类。Blei是这样评价lda的：it can easily be used as a module in more complicated models for more complicated goals。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;为什么topic model不适合作为一种独立的方法去解决某种特定的问题(例如分类，触发等)。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;个人总结，主要原因是lda模型可控性可解释性相对比较差：对于每个topic，不能用很明确的语义归纳出这个topic在讲什么；重新训练一遍lda模型，每个topic id所对应的语义可能发生了变化；有些topic的准确性比较好，有些比较差，而对于比较差的topic，没有特别好的针对性的方法去优化它；&lt;/li&gt;
&lt;li&gt;另外一个就是topic之间的重复，特别是在topic数目比较多的情况，重复几乎是不可避免的，当时益总在开发peacock的时候，deduplicate topic就是一个很重要的任务。如果多个topic描述的意思一致时，用topic id来做检索触发，效果大半是不好的，后来我们也尝试用topic word来做，但依旧不够理想。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h5&gt;主题模型最新进展&lt;/h5&gt;

&lt;p&gt;首先主题模型自PLSA, LDA后，又提出了很多变体，譬如HDP。LDA的topic number是预先设定的，而HDP的topic number是不固定，而是从训练数据中学习得到的，这在很多场景是有用的，具体参考&lt;a href=&quot;http://datascience.stackexchange.com/questions/128/latent-dirichlet-allocation-vs-hierarchical-dirichlet-process&quot;&gt;hdp vs lda&lt;/a&gt;。想了解更多LDA模型的升级，请参考文献[73,74]。&lt;/p&gt;

&lt;p&gt;深度学习方面，Geoff Hinton及其学生用Deep Boltzmann Machine研究出了类似LDA的隐变量文本模型[82]，文章称其抽取的特征在文本检索与文本分类上的结果比LDA好。&lt;a href=&quot;http://weibo.com/u/2387864597&quot;&gt;heavenfireray&lt;/a&gt;在其微博评论道：lda结构是word-hidden topic。类lda结构假设在topic下产生每个word是条件独立而且参数相同。这种假设导致参数更匹配长文而非短文。该文章提出word-hidden topic-hidden word，其实是(word,hidden word)-hidden topic，增加的hidden word平衡了参数对短文的适配，在分类文章数量的度量上更好很自然。&lt;/p&gt;

&lt;p&gt;其次，随着目前互联网的数据规模的逐渐增加，大规模并行PLSA，LDA训练将是主旋律。大规模主题模型训练，除了从系统架构上进行优化外，更关键的，还需要在算法本身上做升级。variational方法不太适合并行化，且速度相对也比较慢，这里我们着重看sampling-base inference。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;collapsed Gibbs sampler[57]：O(K)复杂度，K表示topic的总个数。&lt;/li&gt;
&lt;li&gt;SparseLDA[66]：算法复杂度为O(Kd + Kw)，Kd表示文档d所包含的topic个数，Kw表示词w所属的topic个数，考虑到一个文档所包含的topic和一个词所属的topic个数是有限的，肯定远小于K，所以相比于collapsed Gibbs，复杂度已有较大的下降。&lt;/li&gt;
&lt;li&gt;AliasLDA[56]：利用alias table和Metropolis-Hastings，将词这个维度的采样复杂度降至O(1)。所以算法总复杂度为O(Kd)。&lt;/li&gt;
&lt;li&gt;Metropolis-Hastings sampler[13]：复杂度降至O(1)。这里不做分析了，具体请参考文献[13]&lt;/li&gt;
&lt;/ul&gt;

&lt;h5&gt;主题模型并行化&lt;/h5&gt;

&lt;p&gt;在文献[67]中，Newman团队提出了LDA算法的并行化版本Approximate distributed-LDA，如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/ad_lda.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;图11. AD-LDA算法&lt;/p&gt;

&lt;p&gt;在原始gibbs sampling算法里，N(w,t)这个矩阵的更新是串行的，但是研究发现，考虑到N(w,t)矩阵在迭代过程中，相对变化较小，多个worker独立更新N(w,t)，在一轮迭代结束后再根据多个worker的本地更新合并到全局更新N(w,t)，算法依旧可以收敛[67]。&lt;/p&gt;

&lt;p&gt;那么，主题模型的并行化(不仅仅是主题模型，其实是绝大部分机器学习算法)，主要可以从两个角度来说明：数据并行和模型并行。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;数据并行。这个角度相对比较直观，譬如对于LDA模型，可以将训练数据按照worker数目切分为M片(M为worker数)，每个worker保存一份全局的N(w,t)矩阵，在一轮迭代里，各个worker独立计算，迭代结束后，合并各个worker的本地更新。这个思路可以借用目前通用的并行计算框架，譬如Spark，Hadoop，Graphlab等来实现。&lt;/li&gt;
&lt;li&gt;模型并行。考虑到矩阵N(w,t)在大规模主题模型中相当巨大，单机内存不可能存下。所以直观的想法，可以将N(w,t)也切分成多个分片。N(w,t)可以考虑使用全局的parameter server来存储，也可以考虑存储在不同worker上，利用MPI AllReduce来通信。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;数据与模型并行，可以形象的描述为一个棋盘。棋盘的行按照数据划分，棋盘的列按照模型划分。LDA的并行化，就是通过这样的切分，将原本巨大的，不可能在单机存储的矩阵切分到不同的机器，使每台机器都能够将参数存储在内存。再接着，各个worker相对独立计算，计算的过程中不时按照某些策略同步模型数据。&lt;/p&gt;

&lt;p&gt;最近几年里，关于LDA并行化已有相当多的开源实现，譬如：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;PLDA，&lt;a href=&quot;https://code.google.com/p/plda/&quot;&gt;PLDA+&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/sudar/Yahoo_LDA&quot;&gt;Yahoo LDA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/mli/parameter_server&quot;&gt;Parameter server&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;最近的并行LDA实现Peacock[70,65]和LigthLda[13]没有开源()，但我们可以从其论文一窥究竟，总体来说，并行化的大体思路是一致的。譬如LightLDA[13]，下图是实现架构框图，它将训练数据切分成多个Block，模型通过parameter server来同步，每个data block，类似于sliding windows，在计算完V1的采样后，才会去计算V2的采样(下图中V1,V2,V3表示word空间的划分，即模型的划分)。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/parallelism_lda.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;图12. LightLda并行结构图&lt;/p&gt;

&lt;h4&gt;2.2 词向量，句向量&lt;/h4&gt;

&lt;h5&gt;词向量是什么&lt;/h5&gt;

&lt;p&gt;在文本分析的vector space model中，是用向量来描述一个词的，譬如最常见的One-hot representation。One-hot representation方法的一个明显的缺点是，词与词之间没有建立关联。在深度学习中，一般用Distributed Representation来描述一个词，常被称为&amp;quot;Word Representation&amp;quot;或&amp;quot;Word Embedding&amp;quot;，也就是我们俗称的&amp;quot;词向量&amp;quot;。&lt;/p&gt;

&lt;p&gt;词向量起源于hinton在1986年的论文[11]，后来在Bengio的ffnnlm论文[3]中，被发扬光大，但它真正被我们所熟知，应该是word2vec[14]的开源。在ffnnlm中，词向量是训练语言模型的一个副产品，不过在word2vec里，是专门来训练词向量，所以word2vec相比于ffnnlm的区别主要体现在：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;模型更加简单，去掉了ffnnlm中的隐藏层，并去掉了输入层跳过隐藏层直接到输出层的连接。&lt;/li&gt;
&lt;li&gt;训练语言模型是利用第m个词的前n个词预测第m个词，而训练词向量是用其前后各n个词来预测第m个词，这样做真正利用了上下文来预测，如下图所示。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/word2vec.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;图13. word2vec的训练算法&lt;/p&gt;

&lt;p&gt;上图是word2vec的两种训练算法：CBOW(continuous bag-of-words)和Skip-gram。在cbow方法里，训练目标是给定一个word的context，预测word的概率；在skip-gram方法里，训练目标则是给定一个word，预测word的context的概率。&lt;/p&gt;

&lt;p&gt;关于word2vec，在算法上还有较多可以学习的地方，例如利用huffman编码做层次softmax，negative sampling，工程上也有很多trick，具体请参考文章[16][17]。&lt;/p&gt;

&lt;h5&gt;词向量的应用&lt;/h5&gt;

&lt;p&gt;词向量的应用点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;可以挖掘词之间的关系，譬如同义词。&lt;/li&gt;
&lt;li&gt;可以将词向量作为特征应用到其他机器学习任务中，例如作为文本分类的feature，Ronan collobert在Senna[37]中将词向量用于POS, CHK, NER等任务。&lt;/li&gt;
&lt;li&gt;用于机器翻译[28]。分别训练两种语言的词向量，再通过词向量空间中的矩阵变换，将一种语言转变成另一种语言。&lt;/li&gt;
&lt;li&gt;word analogy，即已知a之于b犹如c之于d，现在给出 a、b、c，C(a)-C(b)+C(c)约等于C(d)，C(*)表示词向量。可以利用这个特性，提取词语之间的层次关系。&lt;/li&gt;
&lt;li&gt;Connecting Images and Sentences，image understanding。例如文献，DeViSE: A deep visual-semantic em-bedding model。&lt;/li&gt;
&lt;li&gt;Entity completion in Incomplete Knowledge bases or ontologies，即relational extraction。Reasoning with neural tensor net- works for knowledge base completion。&lt;/li&gt;
&lt;li&gt;more word2vec applications，点击&lt;a href=&quot;http://www.quora.com/Do-you-know-any-interesting-applications-using-distributed-representations-of-words-obtained-from-NNLM-eg-word2vec&quot;&gt;link1&lt;/a&gt;，&lt;a href=&quot;https://www.quora.com/What-are-some-interesting-Word2Vec-results&quot;&gt;link2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;除了产生词向量，word2vec还有很多其他应用领域，对此我们需要把握两个概念：doc和word。在词向量训练中，doc指的是一篇篇文章，word就是文章中的词。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;假设我们将一簇簇相似的用户作为doc（譬如QQ群），将单个用户作为word，我们则可以训练user distributed representation，可以借此挖掘相似用户。&lt;/li&gt;
&lt;li&gt;假设我们将一个个query session作为doc，将query作为word，我们则可以训练query distributed representation，挖掘相似query。&lt;/li&gt;
&lt;/ul&gt;

&lt;h5&gt;句向量&lt;/h5&gt;

&lt;p&gt;分析完word distributed representation，我们也许会问，phrase，sentence是否也有其distributed representation。最直观的思路，对于phrase和sentence，我们将组成它们的所有word对应的词向量加起来，作为短语向量，句向量。在参考文献[34]中，验证了将词向量加起来的确是一个有效的方法，但事实上还有更好的做法。&lt;/p&gt;

&lt;p&gt;Le和Mikolov在文章《Distributed Representations of Sentences and Documents》[20]里介绍了sentence vector，这里我们也做下简要分析。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;先看c-bow方法，相比于word2vec的c-bow模型，区别点有：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;训练过程中新增了paragraph id，即训练语料中每个句子都有一个唯一的id。paragraph id和普通的word一样，也是先映射成一个向量，即paragraph vector。paragraph vector与word vector的维数虽一样，但是来自于两个不同的向量空间。在之后的计算里，paragraph vector和word vector累加或者连接起来，作为输出层softmax的输入。在一个句子或者文档的训练过程中，paragraph id保持不变，共享着同一个paragraph vector，相当于每次在预测单词的概率时，都利用了整个句子的语义。&lt;/li&gt;
&lt;li&gt;在预测阶段，给待预测的句子新分配一个paragraph id，词向量和输出层softmax的参数保持训练阶段得到的参数不变，重新利用梯度下降训练待预测的句子。待收敛后，即得到待预测句子的paragraph vector。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/sentence2vec0.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;图14. sentence2vec cbow算法&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;sentence2vec相比于word2vec的skip-gram模型，区别点为：在sentence2vec里，输入都是paragraph vector，输出是该paragraph中随机抽样的词。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/sentence2vec1.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;图15. sentence2vec skip-gram算法&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面是sentence2vec的结果示例。先利用中文sentence语料训练句向量，然后通过计算句向量之间的cosine值，得到最相似的句子。可以看到句向量在对句子的语义表征上还是相当惊叹的。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/sentence2vec4.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;图16. sentence2vec 结果示例&lt;/p&gt;

&lt;h5&gt;词向量的改进&lt;/h5&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;学习词向量的方法主要分为：Global matrix factorization和Shallow Window-Based。Global matrix factorization方法主要利用了全局词共现，例如LSA；Shallow Window-Based方法则主要基于local context window，即局部词共现，word2vec是其中的代表；Jeffrey Pennington在word2vec之后提出了&lt;a href=&quot;http://nlp.stanford.edu/projects/glove/&quot;&gt;GloVe&lt;/a&gt;，它声称结合了上述两种方法，提升了词向量的学习效果。它与word2vec的更多对比请点击&lt;a href=&quot;http://www.quora.com/How-is-GloVe-different-from-word2vec&quot;&gt;GloVe vs word2vec&lt;/a&gt;，&lt;a href=&quot;http://radimrehurek.com/2014/12/making-sense-of-word2vec/&quot;&gt;GloVe &amp;amp; word2vec评测&lt;/a&gt;。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;目前通过词向量可以充分发掘出&amp;quot;一义多词&amp;quot;的情况，譬如&amp;quot;快递&amp;quot;与&amp;quot;速递&amp;quot;；但对于&amp;quot;一词多义&amp;quot;，束手无策，譬如&amp;quot;苹果&amp;quot;(既可以表示苹果手机、电脑，又可以表示水果)，此时我们需要用多个词向量来表示多义词。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;2.3 卷积神经网络&lt;/h4&gt;

&lt;h5&gt;卷积&lt;/h5&gt;

&lt;p&gt;介绍卷积神经网络(convolutional neural network，简记cnn)之前，我们先看下卷积。&lt;/p&gt;

&lt;p&gt;在一维信号中，卷积的运算，请参考&lt;a href=&quot;http://zh.wikipedia.org/wiki/%E5%8D%B7%E7%A7%AF&quot;&gt;wiki&lt;/a&gt;，其中的图示很清楚。在图像处理中，对图像用一个卷积核进行卷积运算，实际上是一个滤波的过程。下面是卷积的数学表示：&lt;/p&gt;

&lt;p&gt;$$f(x,y)*w(x,y) = \sum&lt;em&gt;{s=-a}^{a} \sum&lt;/em&gt;{t=-b}^{b} w(s,t) f(x-s,y-t)$$&lt;/p&gt;

&lt;p&gt;f(x,y)是图像上点(x,y)的灰度值，w(x,y)则是卷积核，也叫滤波器。卷积实际上是提供了一个权重模板，这个模板在图像上滑动，并将中心依次与图像中每一个像素对齐，然后对这个模板覆盖的所有像素进行加权，并将结果作为这个卷积核在图像上该点的响应。如下图所示，卷积操作可以用来对图像做边缘检测，锐化，模糊等。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/convolution1.png&quot; alt=&quot;&quot;&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/convolution2.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;图17. 卷积操作示例&lt;/p&gt;

&lt;h5&gt;什么是卷积神经网络&lt;/h5&gt;

&lt;p&gt;卷积神经网络是一种特殊的、简化的深层神经网络模型，它的每个卷积层都是由多个卷积滤波器组成。它最先由lecun在LeNet[40]中提出，网络结构如下图所示。在cnn中，图像的一小部分（局部感受区域）作为层级结构的最低层的输入，信息再依次传输到不同的层，每层通过多个卷积滤波器去获得观测数据的最显著的特征。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/lenet5.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;图18. Lenet5网络结构图&lt;/p&gt;

&lt;p&gt;卷积神经网络中的每一个特征提取层（卷积层）都紧跟着一个用来求局部平均与二次提取的计算层（pooling层），这种特有的两次特征提取结构使网络在识别时对输入样本有较高的畸变容忍能力。如下图所示，就是一个完整的卷积过程[21]。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/convolution6.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;图19. 一次完整的卷积过程&lt;/p&gt;

&lt;p&gt;它的特殊性体现在两点：(1)局部感受野(receptive field)，cnn的神经元间的连接是非全连接的；(2)同一层中同一个卷积滤波器的权重是共享的（即相同的）。局部感受野和权重共享这两个特点，使cnn网络结构更类似于生物神经网络，降低了网络模型的复杂度，减少了神经网络需要训练的参数的个数。&lt;/p&gt;

&lt;h5&gt;卷积神经网络的一些细节&lt;/h5&gt;

&lt;p&gt;接下来结合文献[25]，再讲讲卷积神经网络的一些注意点和问题。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;激励函数，要选择非线性函数，譬如tang，sigmoid，rectified liner。在CNN里，relu用得比较多，原因在于：(1)简化BP计算；(2)使学习更快。(3)避免饱和问题(saturation issues)&lt;/li&gt;
&lt;li&gt;Pooling：其作用在于(1)对一些小的形态改变保持不变性，Invariance to small transformations；(2)拥有更大的感受域，Larger receptive fields。pooling的方式有sum or max。&lt;/li&gt;
&lt;li&gt;Normalization：Equalizes the features maps。它的作用有：(1)  Introduces local competition between features；(2)Also helps to scale activations at each layer better for learning；(3)Empirically, seems to help a bit (1-2%) on ImageNet&lt;/li&gt;
&lt;li&gt;训练CNN：back-propagation；stochastic gradient descent；Momentum；Classification loss，cross-entropy；Gpu实现。&lt;/li&gt;
&lt;li&gt;预处理：Mean removal；Whitening(ZCA)&lt;/li&gt;
&lt;li&gt;增强泛化能力：Data augmentation；Weight正则化；在网络里加入噪声，包括DropOut，DropConnect，Stochastic pooling。

&lt;ul&gt;
&lt;li&gt;DropOut：只在全连接层使用，随机的将全连接层的某些神经元的输出置为0。&lt;/li&gt;
&lt;li&gt;DropConnect：也只在全连接层使用，Random binary mask on weights&lt;/li&gt;
&lt;li&gt;Stochastic Pooling：卷积层使用。Sample location from multinomial。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;模型不work，怎么办？结合我自身的经验，learning rate初始值设置得太大，开始设置为0.01，以为很小了，但实际上0.001更合适。&lt;/li&gt;
&lt;/ul&gt;

&lt;h5&gt;卷积神经网络在文本上的应用&lt;/h5&gt;

&lt;p&gt;卷积神经网络在image classify和image detect上得到诸多成功的应用，后文将再详细阐述。但除了图片外，它在文本分析上也取得一些成功的应用。&lt;/p&gt;

&lt;p&gt;基于CNN，可以用来做文本分类，情感分析，本体分类等[36,41,84]。传统文本分类等任务，一般基于bag of words或者基于word的特征提取，此类方法一般需要领域知识和人工特征。利用CNN做，方法也类似，但一般都是基于raw text，CNN模型的输入可以是word series，可以是word vector，还可以是单纯的字符。比起传统方法，CNN不需要过多的人工特征。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;将word series作为输入，利用CNN做文本分类。如下图所示[36]，该CNN很简单，共分四层，第一层是词向量层，doc中的每个词，都将其映射到词向量空间，假设词向量为k维，则n个词映射后，相当于生成一张n*k维的图像；第二层是卷积层，多个滤波器作用于词向量层，不同滤波器生成不同的feature map；第三层是pooling层，取每个feature map的最大值，这样操作可以处理变长文档，因为第三层输出只依赖于滤波器的个数；第四层是一个全连接的softmax层，输出是每个类目的概率。除此之外，输入层可以有两个channel，其中一个channel采用预先利用word2vec训练好的词向量，另一个channel的词向量可以通过backpropagation在训练过程中调整。这样做的结果是：在目前通用的7个分类评测任务中，有4个取得了state-of-the-art的结果，另外3个表现接近最好水平。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/cnn_text_classify.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;图20.基于CNN的文本分类&lt;/p&gt;

&lt;p&gt;利用cnn做文本分类，还可以考虑到词的顺序。利用传统的&amp;quot;bag-of-words + maxent/svm&amp;quot;方法，是没有考虑词之间的顺序的。文献[41]中提出两种cnn模型：seq-cnn, bow-cnn，利用这两种cnn模型，均取得state-of-the-art结果。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;将doc character作为输入，利用CNN做文本分类。文献[86]介绍了一种方法，不利用word，也不利用word vector，直接将字符系列作为模型输入，这样输入维度大大下降(相比于word)，有利于训练更复杂的卷积网络。对于中文，可以将汉字的拼音系列作为输入。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;2.4 文本分类&lt;/h4&gt;

&lt;p&gt;文本分类应该是最常见的文本语义分析任务了。首先它是简单的，几乎每一个接触过nlp的同学都做过文本分类，但它又是复杂的，对一个类目标签达几百个的文本分类任务，90%以上的准确率召回率依旧是一个很困难的事情。这里说的文本分类，指的是泛文本分类，包括query分类，广告分类，page分类，用户分类等，因为即使是用户分类，实际上也是对用户所属的文本标签，用户访问的文本网页做分类。&lt;/p&gt;

&lt;p&gt;几乎所有的机器学习方法都可以用来做文本分类，常用的主要有：lr，maxent，svm等，下面介绍一下文本分类的pipeline以及注意点。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;建立分类体系。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;分类相比于topic model或者聚类，一个显著的特点是：类目体系是确定的。而不像在聚类和LDA里，一个类被聚出来后，但这个类到底是描述什么的，或者这个类与另外的类是什么关系，这些是不确定的，这样会带来使用和优化上的困难。&lt;/li&gt;
&lt;li&gt;一般而言，类目体系是由人工设定的。而类目体系的建立往往需要耗费很多人工研究讨论，一方面由于知识面的限制，人工建立的类目体系可能不能覆盖所有情况；另一方面，还可能存在类目之间instance数的不平衡。比较好的方法，是基于目前已有的类目体系再做一些加工，譬如ODP，FreeBase等。&lt;/li&gt;
&lt;li&gt;还可以先用某种无监督的聚类方法，将训练文本划分到某些clusters，建立这些clusters与ODP类目体系的对应关系，然后人工review这些clusters，切分或者合并cluster，提炼name，再然后根据知识体系，建立层级的taxonomy。&lt;/li&gt;
&lt;li&gt;&lt;p&gt;如果类目标签数目很多的话，我们一般会将类目标签按照一定的层次关系，建立类目树，如下图所示。那么接下来就可以利用层次分类器来做分类，先对第一层节点训练一个分类器，再对第二层训练n个分类器(n为第一层的节点个数)，依次类推。利用层次类目树，一方面单个模型更简单也更准确，另一方面可以避免类目标签之间的交叉影响，但如果上层分类有误差，误差将会向下传导。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/taxonomy.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;图21. 层次类目体系&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;获取训练数据&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;一般需要人工标注训练数据。人工标注，准确率高，但标注工作量大，耗费人力。&lt;/li&gt;
&lt;li&gt;&lt;p&gt;为了减少标注代价，利用无标记的样本，提出了半监督学习(Semi-supervised Learning)，主要考虑如何利用少量的标注样本和大量的未标注样本进行训练和分类的问题。这里介绍两种常见的半监督算法，希望了解更多请参考文献[49]。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Self-learning：两个样本集合，Labeled，Unlabeled。执行算法如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;用Labeled样本集合，生成分类策略F&lt;/li&gt;
&lt;li&gt;用F分类Unlabeled样本，计算误差&lt;/li&gt;
&lt;li&gt;选取Unlabeled中误差小的子集u，加入到Labeled集合。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;接着重复上述步骤。&lt;/p&gt;

&lt;p&gt;举一个例子：以前在做page分类器时，先对每一个类人工筛选一些特征词，然后根据这些特征词对亿级文本网页分类，再然后对每一个明确属于该类的网页提取更多的特征词，加入原有的特征词词表，再去做分类；中间再辅以一定的人工校验，这种方法做下来，效果还是不错的，更关键的是，如果发现那个类有badcase，可以人工根据badcase调整某个特征词的权重，简单粗暴又有效。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Co-training：其主要思想是：每次循环，从Labeled数据中训练出两个不同的分类器，然后用这两个分类器对Unlabeled中数据进行分类，把可信度最高的数据加入到Labeled中，继续循环直到U中没有数据或者达到循环最大次数。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;协同训练，例如Tri-train算法：使用三个分类器.对于一个无标签样本，如果其中两个分类器的判别一致，则将该样本进行标记，并将其纳入另一个分类器的训练样本；如此重复迭代，直至所有训练样本都被标记或者三个分类器不再有变化。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;半监督学习，随着训练不断进行，自动标记的示例中的噪音会不断积累，其负作用会越来越大。所以如term weighting工作里所述，还可以从其他用户反馈环节提取训练数据，类似于推荐中的隐式反馈。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;我们看一个具体的例子，在文献[45]中，twitter利用了三种方法，user-level priors（发布tweet的用户属于的领域），entity-level priors（话题，类似于微博中的#***#），url-level priors（tweet中的url）。利用上面三种数据基于一定规则获取到基本的训练数据，再通过Co-Training获取更多高质量的训练数据。上述获取到的都是正例数据，还需要负例样本。按照常见的方法，从非正例样本里随机抽取作为负例的方法，效果并不是好，文中用到了Pu-learning去获取高质量的负例样本，具体请参考文献[58]。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/training_data_acquisition.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;图22.文献[45]训练数据获取流程图&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;特征提取&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;对于每条instance，运用多种文本分析方法提取特征。常见特征有：

&lt;ul&gt;
&lt;li&gt;分词 or 字的ngram，对词的权重打分，计算词的一些领域特征，又或者计算词向量，词的topic分布。&lt;/li&gt;
&lt;li&gt;文本串的特征，譬如sentence vector，sentence topic等。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;提取的特征，从取值类型看，有二值特征，浮点数特征，离线值特征。&lt;/li&gt;
&lt;li&gt;特征的预处理包括：

&lt;ul&gt;
&lt;li&gt;一般来说，我们希望instance各维特征的均值为0，方差为1或者某个有边界的值。如果不是，最好将该维度上的取值做一个变换。&lt;/li&gt;
&lt;li&gt;特征缺失值和异常值的处理也需要额外注意。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt; 特征选择，下面这些指标都可以用作筛选区分度高的特征。

&lt;ul&gt;
&lt;li&gt;Gini-index: 一个特征的Gini-index越大，特征区分度越高。&lt;/li&gt;
&lt;li&gt;信息增益(Information Gain)&lt;/li&gt;
&lt;li&gt;互信息(Mutual Information)&lt;/li&gt;
&lt;li&gt;相关系数(Correlation)&lt;/li&gt;
&lt;li&gt;假设检验(Hypothesis Testing)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;模型训练&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;模型选择：通常来说，常用的有监督模型已经足够了，譬如lr, svm, maxent, naive-bayes，决策树等。这些基本模型之间的效果差异不大，选择合适的即可。上一小节讲到cnn时，提到深度神经网络也可以用来做文本分类。深度神经网络相比较于传统方法，特征表示能力更强，还可以自学习特征。&lt;/li&gt;
&lt;li&gt;语言模型也可用于分类，针对不同的label，训练两个不同的语言模型p+(x|y=+1)和p-(x|y=-1)。对于一个testcase x，求解r= p+(x|y=+1)/p-(x|y=-1)*p(y=+1)/p(y=-1)，如果r&amp;gt;1，则x属于label(+1)，否则x属于label(-1)。&lt;/li&gt;
&lt;li&gt;模型的正则化：一般来说，L1正则化有特征筛选的作用，用得相对较多，除此外，L2正则化，ElasticNet regularization(L1和L2的组合)也很常用。&lt;/li&gt;
&lt;li&gt;对于多分类问题，可以选择one-vs-all方法，也可以选择multinomial方法。两种选择各有各的优点，主要考虑有：并行训练multiple class model更复杂；不能重新训练 a subset of topics。&lt;/li&gt;
&lt;li&gt;model fine-tuning。借鉴文献[72]的思路(训练深度神经网络时，先无监督逐层训练参数，再有监督调优)，对于文本分类也可以采用类似思路，譬如可以先基于自提取的大规模训练数据训练一个分类模型，再利用少量的有标注训练数据对原模型做调优。下面这个式子是新的loss function，w是新模型参数，\(w^0\)是原模型参数，\(l(w,b|x&lt;em&gt;i,y&lt;/em&gt;i)\)是新模型的likelihood，优化目标就是最小化&amp;quot;新模型参数与原模型参数的差 + 新模型的最大似然函数的负数 + 正则化项&amp;quot;。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$$min&lt;em&gt;{w,b} \frac{\delta}{2}||w-w^0||&lt;/em&gt;2^2 - \frac{1-\delta}{n}\sum&lt;em&gt;{i=1}^nl(w,b|x&lt;/em&gt;i,y&lt;em&gt;i) + \lambda(\alpha||w||&lt;/em&gt;1+\frac{1-\alpha}{2}||w||_2^2)$$
- model ensemble：也称&amp;quot;Multi-Model System&amp;quot;，ensemble是提升机器学习精度的有效手段，各种竞赛的冠军队伍的是必用手段。它的基本思想，充分利用不同模型的优势，取长补短，最后综合多个模型的结果。Ensemble可以设定一个目标函数(组合多个模型)，通过训练得到多个模型的组合参数(而不是简单的累加或者多数)。譬如在做广告分类时，可以利用maxent和决策树，分别基于广告title和描述，基于广告的landing page，基于广告图片训练6个分类模型。预测时可以通过ensemble的方法组合这6个模型的输出结果。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;评测&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;评测分类任务一般参考Accuracy，recall, precision，F1-measure，micro-recall/precision，macro-recall/precision等指标。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;3 图片语义分析&lt;/h3&gt;

&lt;h4&gt;3.1 图片分类&lt;/h4&gt;

&lt;p&gt;图片分类是一个最基本的图片语义分析方法。&lt;/p&gt;

&lt;h5&gt;基于深度学习的图片分类&lt;/h5&gt;

&lt;p&gt;传统的图片分类如下图所示，首先需要先手工提取图片特征，譬如SIFT, GIST，再经由VQ coding和Spatial pooling，最后送入传统的分类模型(例如SVM等)。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/tranditional-imageclassify.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;图23. 传统图片分类流程图&lt;/p&gt;

&lt;p&gt;传统方法里，人工特征提取是一个巨大的消耗性工作。而随着深度学习的进展，不再需要人工特征，通过深度学习自动提取特征成为一种可能。接下来主要讲述卷积神经网络在图片分类上的使用。&lt;/p&gt;

&lt;p&gt;下图是一个经典的卷积神经网络模型图，由Hinton和他的学生Alex Krizhevsky在ILSVRC(Imagenet Large Scale Visual Recognition Competition) 2012中提出。
整个网络结构包括五层卷积层和三层全连接层，网络的最前端是输入图片的原始像素点，最后端是图片的分类结果。一个完整的卷积层可能包括一层convolution，一层Rectified Linear Units，一层max-pooling，一层normalization。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/convolution7.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;图24. 卷积神经网络结构图&lt;/p&gt;

&lt;p&gt;对于每一层网络，具体的网络参数配置如下图所示。InputLayer就是输入图片层，每个输入图片都将被缩放成227*227大小，分rgb三个颜色维度输入。Layer1~ Layer5是卷积层，以Layer1为例，卷积滤波器的大小是11*11，卷积步幅为4，本层共有96个卷积滤波器，本层的输出则是96个55*55大小的图片。在Layer1，卷积滤波后，还接有ReLUs操作和max-pooling操作。Layer6~ Layer8是全连接层，相当于在五层卷积层的基础上再加上一个三层的全连接神经网络分类器。以Layer6为例，本层的神经元个数为4096个。Layer8的神经元个数为1000个，相当于训练目标的1000个图片类别。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/convolution_config.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;图25. CNN网络参数配置图&lt;/p&gt;

&lt;p&gt;基于Alex Krizhevsky提出的cnn模型，在13年末的时候，我们实现了用于广点通的图片分类和图片检索(可用于广告图片作弊判别)，下面是一些示例图。&lt;/p&gt;

&lt;p&gt;图片分类示例：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/image_classify.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;图26. 图片分类示例图&lt;/p&gt;

&lt;p&gt;图片检索示例：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/image_search1.png&quot; alt=&quot;&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/image_search2.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;图27. 图片检索示例图&lt;/p&gt;

&lt;h5&gt;图片分类上的最新进展&lt;/h5&gt;

&lt;p&gt;在ILSVRC 2012中，Krizhevsky基于GPU实现了上述介绍的，这个有60million参数的模型，赢得了第一名。这个工作是开创性的，它引领了接下来ILSVRC的风潮。2013年，Clarifai通过cnn模型可视化技术调整网络架构，赢得了ILSVRC。2014年，google也加入进来，它通过增加模型的层数（总共22层），让深度更深[48]，并且利用multi-scale data training，取得第一名。baidu最近通过更加&amp;quot;粗暴&amp;quot;的模型[44]，在GooLeNet的基础上，又提升了10%，top-5错误率降低至6%以下。具体结果如下图所示。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images//imagenet_result.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;图28. ImageNet Classification Result&lt;/p&gt;

&lt;p&gt;先简单分析一下&amp;quot;GoogLeNet&amp;quot;[48,51]所采用的方法：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;大大增加的网络的深度，并且去掉了最顶层的全连接层：因为全连接层（Fully Connected）几乎占据了CNN大概90%的参数，但是同时又可能带来过拟合（overfitting）的效果。&lt;/li&gt;
&lt;li&gt;模型比以前AlexNet的模型大大缩小，并且减轻了过拟合带来的副作用。Alex模型参数是60M，GoogLeNet只有7M。&lt;/li&gt;
&lt;li&gt;对于google的模型，目前已有开源的实现，有兴趣请点击&lt;a href=&quot;https://github.com/BVLC/caffe/tree/master/models/bvlc_googlenet&quot;&gt;Caffe+GoogLeNet&lt;/a&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;再分析一下&amp;quot;Deep Image by baidu[44]&amp;quot;所采用的方法：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Hardware/Software Co-design。baidu基于GPU，利用36个服务节点开发了一个专为深度学习运算的supercompter(名叫Minwa，敏娲)。这台supercomputer具备TB级的host memory，超强的数据交换能力，使能训练一个巨大的深层神经网络成为可能。&lt;/p&gt;

&lt;p&gt;而要训练如此巨大的神经网络，除了硬件强大外，还需要高效的并行计算框架。通常而言，都要从data-parallelism和model-data parallelism两方面考虑。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;data-parallelism：训练数据被分成N份。每轮迭代里，各个GPU基于各自的训练数据计算梯度，最后累加所有梯度数据并广播到所有GPU。&lt;/li&gt;
&lt;li&gt;model-data parallelism：考虑到卷积层参数较少但消耗计算量，而全连接层参数相对比较多。所以卷积层参数以local copy的形式被每个GPU所持有，而全连接层的参数则被划分到各个CPU。每轮迭代里，卷积层计算可以由各个GPU独立完成，全连接层计算需要由所有GPU配合完成，具体方法请参考[46]。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Data augmentation。训练一个如此巨大的神经网络(100billion个参数)，如果没有充分的训练数据，模型将很大可能陷入过拟合，所以需要采用众多data augmentation方法增加训练数据，例如：剪裁，不同大小，调亮度，饱和度，对比度，偏色等(color casting, vignetting, lens distortion, rotation, flipping, cropping)。举个例子，一个彩色图片，增减某个颜色通道的intensity值，就可以生成多张图片，但这些图片和原图的类目是一致的，相当于增加了训练数据。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Multi-scale training：训练不同输入图片尺度下(例如512*512，256*256)的多个模型，最后ensemble多个模型的输出结果。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;3.2 Image2text，Image2sentence&lt;/h4&gt;

&lt;p&gt;上面讲述的图片分类对图片语义的理解比较粗粒度，那么我们会想，是否可以将图片直接转化为一堆词语或者一段文本来描述。转化到文本后，我们积累相对深的文本处理技术就都可以被利用起来。&lt;/p&gt;

&lt;h5&gt;Image2text&lt;/h5&gt;

&lt;p&gt;首先介绍一种朴素的基于卷积神经网络的image to text方法。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;首先它利用深度卷积神经网络和深度自动编码器提取图片的多层特征，并据此提取图片的visual word，建立倒排索引，产生一种有效而准确的图片搜索方法。&lt;/li&gt;
&lt;li&gt;再充分利用大量的互联网资源，预先对大量种子图片做语义分析，然后利用相似图片搜索，根据相似种子图片的语义推导出新图片的语义。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;其中种子图片，就是可以覆盖目前所有待研究图片的行业，但较容易分析语义的图片集。这种方法产生了更加丰富而细粒度的语义表征结果。虽说简单，但效果仍然不错，方法的关键在于种子图片。利用比较好的种子图片(例如paipai数据)，简单的方法也可以work得不错。下图是该方法的效果图。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/image_semantic.png&quot; alt=&quot;&quot;&gt;  &lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/image_semantic2.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;图29. 图片语义tag标注示例图&lt;/p&gt;

&lt;p&gt;上面的baseline方法，在训练数据优质且充分的情况下，可以取得很不错的图片tag提取效果，而且应用也非常广泛。但上面的方法非常依赖于训练数据，且不善于发现训练数据之外的世界。&lt;/p&gt;

&lt;p&gt;另一个直观的想法，是否可以通过word embedding建立image与text的联系[26]。例如，可以先利用CNN训练一个图片分类器。每个类目label可以通过word2vec映射到一个embedding表示。对于一个新图片，先进行分类，然后对top-n类目label所对应的embedding按照权重(这里指这个类目所属的概率)相加，得到这个图片的embedding描述，然后再在word embedding空间里寻找与图片embedding最相关的words。&lt;/p&gt;

&lt;h5&gt;Image detection&lt;/h5&gt;

&lt;p&gt;接下来再介绍下image detection。下图是一个image detection的示例，相比于图片分类，提取到信息将更加丰富。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/image_detection.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;图30. 图片detection示例&lt;/p&gt;

&lt;p&gt;目前最先进的detection方法应该是Region-based CNN(简称R-CNN)[75]，是由Jeff Donahue和Ross Girshick提出的。R-CNN的具体想法是，将detection分为寻找object和识别object两个过程。在第一步寻找object，可以利用很多region detection算法，譬如selective search[76]，CPMC，objectness等，利用很多底层特征，譬如图像中的色块，图像中的边界信息。第二步识别object，就可以利用&amp;quot;CNN+SVM&amp;quot;来做分类识别。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/r-cnn.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;图31. Image detection系统框图&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;给定一张图片，利用selective search方法[76]来产生2000个候选窗口。&lt;/li&gt;
&lt;li&gt;然后利用CNN进行对每一个候选窗口提取特征(取全连接层的倒数第一层)，特征长度为4096。&lt;/li&gt;
&lt;li&gt;最后用SVM分类器对这些特征进行分类（每一个目标类别一个SVM分类器），SVM的分类器的参数个数为：4096*N，其中N为目标的类别个数，所以比较容易扩展目标类别数。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这里有R-CNN的实现，请点击&lt;a href=&quot;https://github.com/rbgirshick/rcnn&quot;&gt;rcnn code&lt;/a&gt;&lt;/p&gt;

&lt;h5&gt;Image2sentence&lt;/h5&gt;

&lt;p&gt;那能否通过深度学习方法，直接根据image产生sentence呢？我们先看一组实际效果，如下图所示(copy from 文献[43])。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/image2sentence_example.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;图32. image2sentence示例图&lt;/p&gt;

&lt;p&gt;关于这个方向，最近一年取得了比较大的突破，工业界(Baidu[77]，Google[43]，Microsoft[80,81]等)和学术界(Stanford[35]，Borkeley[79]，UML[19]，Toronto[78]等)都发表了一系列论文。&lt;/p&gt;

&lt;p&gt;简单归纳一下，对这个问题，主要有两种解决思路：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Pipeline方法。这个思路相对直观一点，先学习到image中visual object对应的word(如上一节image detection所述)，再加上language model，就可以生成sentence。这种方法各个模块可以独立调试，相对来说，更灵活一点。如下图所示，这是microsoft的一个工作[81]，它分为三步：(1)利用上一节提到的思路detect words；(2)基于language model(RNN or LSTM)产生句子；(3)利用相关性模型对句子打分排序。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/AIC.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;图33. &amp;quot;pipeline&amp;quot; image captioning&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;End-to-end方法，即通过一个模型直接将image转换到sentence。google基于CNN+RNN开发了一个Image Caption Generator[43]。这个工作主要受到了基于RNN的机器翻译[27][42]的启发。在机器翻译中，&amp;quot;encoder&amp;quot; RNN读取源语言的句子，将其变换到一个固定长度的向量表示，然后&amp;quot;decoder&amp;quot; RNN将向量表示作为隐层初始值，产生目标语言的句子。&lt;/p&gt;

&lt;p&gt;那么一个直观的想法是，能否复用上面的框架，考虑到CNN在图片特征提取方面的成功应用，将encoder RNN替换成CNN，先利用CNN将图片转换到一个向量表示，再利用RNN将其转换到sentence。可以通过图片分类提前训练好CNN模型，将CNN最后一个隐藏层作为encoder RNN的输入，从而产生句子描述。如下图所示。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/cnn_rnn.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/cnn_lstm.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;图34. &amp;quot;CNN+LSTM&amp;quot; Image Caption Generator&lt;/p&gt;

&lt;p&gt;Li-Feifei团队在文献[35]也提到一种image2sentence方法，如下图所示。与google的做法类似，图片的CNN特征作为RNN的输入。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/cnn-rnn.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;图35. &amp;quot;CNN+RNN&amp;quot;生成图片描述&lt;/p&gt;

&lt;p&gt;此方法有开源实现，有兴趣请参考：&lt;a href=&quot;https://github.com/karpathy/neuraltalk&quot;&gt;neuraltalk&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;3.3 训练深度神经网络的tricks&lt;/h4&gt;

&lt;p&gt;考虑到图片语义分析的方法大部分都是基于深度学习的，Hinton的学生Ilya Sutskever写了一篇深度学习的综述文章[47]，其中提到了一些训练深度神经网络的tricks，整理如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;保证训练数据的质量&lt;/li&gt;
&lt;li&gt;使训练数据各维度数值的均值为0，方差为一个比较小的值&lt;/li&gt;
&lt;li&gt;训练时使用minbatch，但不要设得过大，在合理有效的情况下，越小越好。&lt;/li&gt;
&lt;li&gt;梯度归一化，将梯度值除于minbatch size。&lt;/li&gt;
&lt;li&gt;设置一个正常的learning rate，validation无提升后，则将原learning rate除于5继续&lt;/li&gt;
&lt;li&gt;模型参数随机初始化。如果是深层神经网络，不要设置过小的random weights。&lt;/li&gt;
&lt;li&gt;如果是在训练RNN or LSTM，对梯度设置一个限值，不能超过15 or 5。&lt;/li&gt;
&lt;li&gt;注意检查梯度计算的正确性&lt;/li&gt;
&lt;li&gt;如果是训练LSTM，initialize the biases of the forget gates of the LSTMs to large values&lt;/li&gt;
&lt;li&gt;Data augmentation很实用。&lt;/li&gt;
&lt;li&gt;Dropout在训练时很有效，不过记得测试时关掉Dropout。&lt;/li&gt;
&lt;li&gt;Ensembling。训练多个神经网络，最后计算它们的预测值的平均值。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;4 总结&lt;/h3&gt;

&lt;h4&gt;4.1 语义分析方法在实际业务中的使用&lt;/h4&gt;

&lt;p&gt;前面讲述了很多语义分析方法，接下来我们看看如何利用这些方法帮忙我们的实际业务，这里举一个例子，用户广告的语义匹配。&lt;/p&gt;

&lt;p&gt;在广点通系统中，用户与广告的关联是通过定向条件来匹配的，譬如某些广告定向到&amp;quot;北京+男性&amp;quot;，那么当&amp;quot;北京+男性&amp;quot;的用户来到时，所有符合定向的广告就将被检索出，再按照&amp;quot;ecpm*quality&amp;quot;排序，将得分最高的展示给用户。但是凭借一些人口属性，用户与广告之间的匹配并不精确，做不到&amp;quot;广告就是想用户所想&amp;quot;，所以用户和广告的语义分析就将派上用场了，可以从这样两方面来说明：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;特征提取。基于上面介绍的方法，提取用户和广告的语义特征。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;用户语义特征。可以从用户的搜索，购物，点击，阅读记录中发现用户兴趣。考虑到最终的用户描述都是文本，那么文本topic分析，文本分类，文本keyword提取，文本核心term提取都可以运用起来，分析出用户的语义属性，还可以利用矩阵分解和文本分类找到相似用户群。&lt;/li&gt;
&lt;li&gt;广告语义特征。在广点通里，广告可以从两个维度来描述，一方面是文本，包括广告title和landing page，另一方面是广告展示图片。利用文本和图片的语义分析方法，我们可以提取出广告的topic，类目，keyword，tag描述。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;语义匹配。提取到相应的语义特征之后，怎么用于改善匹配呢？&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;用户-广告的语义检索。基于keyword、类目以及topic，对广告建立相应的倒排索引，直接用于广告检索。&lt;/li&gt;
&lt;li&gt;用户-广告的语义特征。分别提取用户和广告的语义特征，用于计算用户-广告的relevance，pctr，pcvr，达到精确排序。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;4.2 Future&lt;/h4&gt;

&lt;p&gt;对于文本和图片的语义分析，可以看到：最近几年，在某些任务上，基于深度学习的方法逐渐赶上并超过了传统方法的效果。但目前为止，对于深度学习的发掘才刚刚开始，比较惊艳的神经网络方法，也只有有限几种，譬如CNN，RNN，RBM等。&lt;/p&gt;

&lt;p&gt;上文只是介绍了我们在工作中实践过的几个小点，还有更多方法需要我们去挖掘：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Video。Learn about 3D structure from motion。如文献[19]所示，研究将视频也转换到自然语言。&lt;/li&gt;
&lt;li&gt;Deep Learning + Structured Prediction，用于syntactic representation。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;4.3 总结&lt;/h4&gt;

&lt;p&gt;上文主要从文本、图片这两方面讲述了语义分析的一些方法，并结合个人经验做了一点总结。&lt;/p&gt;

&lt;p&gt;原本想写得更全面一些，但写的时候才发现上面所述的只是沧海一粟，后面还有更多语义分析的内容之后再更新。另外为避免看到大篇理论就头痛，文中尽可能不出现复杂的公式和理论推导。如果有兴趣，可以进一步阅读参考文献，获得更深的理解。谢谢。&lt;/p&gt;

&lt;h3&gt;5 参考文献&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;http://comminfo.rutgers.edu/%7Emuresan/IR/Docs/Articles/ipmSalton1988.pdf&quot;&gt;Term-weighting approaches in automatic text retrieval，Gerard Salton et.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.sandia.gov/%7Etgkolda/pubs/pubfiles/ornl-tm-13756.pdf&quot;&gt;New term weighting formulas for the vector space method in information retrieval&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf&quot;&gt;A neural probabilistic language model 2003&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://licstar.net/archives/328&quot;&gt;Deep Learning in NLP-词向量和语言模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf&quot;&gt;Recurrent neural network based language models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Statistical Language Models based on Neural Networks，mikolov博士论文&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.fit.vutbr.cz/%7Eimikolov/rnnlm/&quot;&gt;Rnnlm library&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://brown.cl.uni-heidelberg.de/%7Esourjiko/NER_Literatur/survey.pdf&quot;&gt;A survey of named entity recognition and classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.aclweb.org/anthology/D13-1061&quot;&gt;Deep learning for Chinese word segmentation and POS tagging&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://aclweb.org/anthology/P14-1028&quot;&gt;Max-margin tensor neural network for chinese word segmentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.cogsci.ucsd.edu/%7Eajyu/Teaching/Cogs202_sp12/Readings/hinton86.pdf&quot;&gt;Learning distributed representations of concepts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.cs.colorado.edu/%7Ejbg/docs/2014_book_chapter_care_and_feeding.pdf&quot;&gt;Care and Feeding of Topic Models: Problems, Diagnostics, and Improvements&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://arxiv.org/abs/1412.1576&quot;&gt;LightLda&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://code.google.com/p/word2vec/&quot;&gt;word2vec&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://arxiv.org/pdf/1301.3781v3.pdf&quot;&gt;Efficient Estimation of Word Representations in Vector Space&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://techblog.youdao.com/?p=915&quot;&gt;Deep Learning实战之word2vec&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://suanfazu.com/t/word2vec-zhong-de-shu-xue-yuan-li-xiang-jie-duo-tu-wifixia-yue-du/178&quot;&gt;word2vec中的数学原理详解&lt;/a&gt; &lt;a href=&quot;http://blog.csdn.net/itplus/article/details/37969519&quot;&gt;出处2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://52opencourse.com/111/%E6%96%AF%E5%9D%A6%E7%A6%8F%E5%A4%A7%E5%AD%A6%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%AC%E5%9B%9B%E8%AF%BE-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88language-modeling%EF%BC%89&quot;&gt;斯坦福课程-语言模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://arxiv.org/abs/1412.4729&quot;&gt;Translating Videos to Natural Language Using Deep Recurrent Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://arxiv.org/pdf/1405.4053v2.pdf&quot;&gt;Distributed Representations of Sentences and Documents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/zouxy09/article/details/8781543&quot;&gt;Convolutional Neural Networks卷积神经网络&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://research.microsoft.com/en-us/news/features/spp-102914.aspx&quot;&gt;A New, Deep-Learning Take on Image Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://arxiv.org/pdf/1406.4729v1.pdf&quot;&gt;Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.toptal.com/machine-learning/an-introduction-to-deep-learning-from-perceptrons-to-deep-networks&quot;&gt;A Deep Learning Tutorial: From Perceptrons to Deep Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://cs.nyu.edu/%7Efergus/presentations/nips2013_final.pdf&quot;&gt;Deep Learning for Computer Vision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://arxiv.org/pdf/1312.5650.pdf&quot;&gt;Zero-shot leanring by convex combination of semantic embeddings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://arxiv.org/pdf/1409.3215v3.pdf&quot;&gt;Sequence to sequence learning with neural network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://arxiv.org/pdf/1309.4168.pdf&quot;&gt;Exploting similarities among language for machine translation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Grammar as Foreign Language Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, Geoffrey Hinton, arXiv 2014&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://ceur-ws.org/Vol-1204/papers/paper_4.pdf&quot;&gt;Deep Semantic Embedding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;张家俊. DNN Applications in NLP&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://cl.naist.jp/%7Ekevinduh/notes/cwmt14tutorial.pdf&quot;&gt;Deep learning for natural language processing and machine translation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;&quot;&gt;Distributed Representations for Semantic Matching&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;distributed&lt;em&gt;representation&lt;/em&gt;nlp&lt;/li&gt;
&lt;li&gt;Deep Visual-Semantic Alignments for Generating Image Descriptions&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://arxiv.org/pdf/1408.5882v2.pdf&quot;&gt;Convolutional Neural Networks for Sentence Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://ml.nec-labs.com/senna&quot;&gt;Senna&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://arxiv.org/pdf/1409.0575v1.pdf&quot;&gt;ImageNet Large Scale Visual Recognition Challenge&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Krizhevsky A, Sutskever I, Hinton G E. ImageNet Classification with Deep Convolutional Neural Networks&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://turing.iimas.unam.mx/%7Eelena/CompVis/Lecun98.pdf&quot;&gt;Gradient-Based Learning Applied to Document Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Effetive use of word order for text categorization with convolutional neural network，Rie Johnson&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://arxiv.org/pdf/1406.1078.pdf&quot;&gt;Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://arxiv.org/pdf/1411.4555v1.pdf&quot;&gt;Show and Tell: A Neural Image Caption Generator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://arxiv.org/ftp/arxiv/papers/1501/1501.02876.pdf&quot;&gt;Deep Image: Scaling up Image Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Large-Scale High-Precision Topic Modeling on Twitter&lt;/li&gt;
&lt;li&gt;A. Krizhevsky. One weird trick for parallelizing convolutional neural networks. arXiv:1404.5997, 2014&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://yyue.blogspot.com/2015/01/a-brief-overview-of-deep-learning.html&quot;&gt;A Brief Overview of Deep Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Going deeper with convolutions. Christian Szegedy. Google Inc. &lt;a href=&quot;http://www.gageet.com/2014/09203.php&quot;&gt;阅读笔记&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Long Short-Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://pages.cs.wisc.edu/%7Ejerryzhu/pub/sslicml07.pdf&quot;&gt;Semi-Supervised Learning Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;http://www.zhihu.com/question/24904450&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://arxiv.org/pdf/1402.1128.pdf&quot;&gt;LONG SHORT-TERM MEMORY BASED RECURRENT NEURAL NETWORK ARCHITECTURES FOR LARGE VOCABULARY SPEECH RECOGNITION&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.248.4448&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;LSTM Neural Networks for Language Modeling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf&quot;&gt;LONG SHORT-TERM MEMORY&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Bengio, Y., Simard, P., Frasconi, P., “Learning long-term dependencies with gradient descent is difficult” IEEE Transactions on Neural Networks 5 (1994), pp. 157–166&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.sravi.org/pubs/fastlda-kdd2014.pdf&quot;&gt;AliasLDA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;www.umiacs.umd.edu/%7Eresnik/pubs/LAMP-TR-153.pdf&quot;&gt;Gibbs sampling for the uninitiated&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.eecs.tufts.edu/%7Enoto/pub/kdd08/elkan.kdd08.poster.pdf&quot;&gt;Learning classifiers from only positive and unlabeled data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://cda.ornl.gov/publications/ICMLA06.pdf&quot;&gt;TF-ICF: A New Term Weighting Scheme for Clustering Dynamic Data Streams&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.flickering.cn/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1/2014/06/%E3%80%90lda%E6%95%B0%E5%AD%A6%E5%85%AB%E5%8D%A6%E3%80%91%E7%A5%9E%E5%A5%87%E7%9A%84gamma%E5%87%BD%E6%95%B0/&quot;&gt;LDA数学八卦&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.aclweb.org/anthology/W06-0132&quot;&gt;Chinese Word Segmentation and Named Entity Recognition Based on Conditional Random Fields Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://repository.upenn.edu/cgi/viewcontent.cgi?article=1162&amp;amp;context=cis_papers&quot;&gt;Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://scholarworks.umass.edu/cgi/viewcontent.cgi?article=1091&amp;amp;context=cs_faculty_pubs&quot;&gt;Chinese Segmentation and New Word Detection using Conditional Random Fields&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.arbylon.net/publications/text-est.pdf&quot;&gt;Gregor Heinrich. Parameter estimation for text analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://km.oa.com/group/14352/articles/show/213192&quot;&gt;Peacock：大规模主题模型及其在腾讯业务中的应用&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;L. Yao, D. Mimno, and A. McCallum. Efficient methods for topic model inference on streaming document collections. In KDD, 2009.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.jmlr.org/papers/volume10/newman09a/newman09a.pdf&quot;&gt;David Newman. Distributed Algorithms for Topic Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.flickering.cn/nlp/2014/07/lda%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5%E4%B9%8B%E7%AE%97%E6%B3%95%E7%AF%87-1%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0%E6%AD%A3%E7%A1%AE%E6%80%A7%E9%AA%8C%E8%AF%81/&quot;&gt;Xuemin. LDA工程实践之算法篇&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.cs.unm.edu/%7Epdevineni/papers/Lott.pdf&quot;&gt;Brian Lott. Survey of Keyword Extraction Techniques&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Yi Wang, Xuemin Zhao, Zhenlong Sun, Hao Yan, Lifeng Wang, Zhihui Jin, Liubin Wang, Yang Gao, Ching Law, and Jia Zeng. Peacock: Learning Long-Tail Topic Features for Industrial Applications. TIST’2015.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://nlp.csai.tsinghua.edu.cn/%7Elzy/publications/phd_thesis.pdf&quot;&gt;刘知远. 基于文档主题结构的关键词抽取方法研究&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.cs.toronto.edu/%7Ehinton/science.pdf&quot;&gt;Hinton. Reducing the Dimensionality of Data with Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://dl.acm.org/citation.cfm?id=2396863&quot;&gt;Samaneh Moghaddam. On the design of LDA models for aspect-based opinion mining&lt;/a&gt;；&lt;/li&gt;
&lt;li&gt;The FLDA model for aspect-based opinion mining: addressing the cold start problem&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.cs.berkeley.edu/%7Erbg/papers/r-cnn-cvpr.pdf&quot;&gt;Ross Girshick et. Rich feature hierarchies for accurate object detection and semantic segmentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;J. Uijlings, K. van de Sande, T. Gevers, and A. Smeulders. Selective search for object recognition. IJCV, 2013.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://arxiv.org/abs/1410.1090&quot;&gt;Baidu/UCLA: Explain Images with Multimodal Recurrent Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://arxiv.org/abs/1411.2539&quot;&gt;Toronto: Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://arxiv.org/abs/1411.4389&quot;&gt;Berkeley: Long-term Recurrent Convolutional Networks for Visual Recognition and Description&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://arxiv.org/abs/1411.5654&quot;&gt;Xinlei Chen et. Learning a Recurrent Visual Representation for Image Caption Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://arxiv.org/pdf/1411.4952v2&quot;&gt;Hao Fang et. From Captions to Visual Concepts and Back&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.cs.toronto.edu/%7Enitish/uai13.pdf&quot;&gt;Modeling Documents with a Deep Boltzmann Machine&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://nikhilbuduma.com/2015/01/11/a-deep-dive-into-recurrent-neural-networks/&quot;&gt;A Deep Dive into Recurrent Neural Nets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://arxiv.org/abs/1502.01710?utm_source=dlvr.it&amp;amp;utm_medium=tumblr&quot;&gt;Xiang zhang et. Text Understanding from Scratch&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sun, 01 Feb 2015 00:00:00 +0800</pubDate>
        <link>http://yourdomain.com/2015/02/01/semantic_analysis_detail.html</link>
        <guid isPermaLink="true">http://yourdomain.com/2015/02/01/semantic_analysis_detail.html</guid>
        
        <category>machine learning</category>
        
        <category>nlp</category>
        
        
      </item>
    
      <item>
        <title>deep learning的坑</title>
        <description>&lt;h2&gt;deep learning的坑&lt;/h2&gt;

&lt;p&gt;转自：&lt;a href=&quot;http://www.zhihu.com/question/27608272/answer/37318565&quot;&gt;知乎问答&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;要说深度学习还有什么坑，就要看看目前的深度学习都从哪些方面去研究。个人觉得当前深度学习领域的学术研究可以包含四部分：优化（Optimization），泛化（Generalization），表达（Representation）以及应用（Applications）。除了应用（Applications）之外每个部分又可以分成实践和理论两个方面。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;优化（Optimization）：深度学习的问题最后似乎总能变成优化问题，这个时候数值优化的方法就变得尤其重要。&lt;/p&gt;

&lt;p&gt;从实践方面来说，现在最为推崇的方法依旧是随机梯度递减，这样一个极其简单的方法以其强悍的稳定性深受广大研究者的喜爱，而不同的人还会结合动量（momentum）、伪牛顿方法（Pseudo-Newton）以及自动步长等各种技巧。此外，深度学习模型优化过程的并行化也是一个非常热的点，近年在分布式系统的会议上相关论文也逐渐增多。&lt;/p&gt;

&lt;p&gt;在理论方面，目前研究的比较清楚的还是凸优化（Convex Optimization），而对于非凸问题的理论还严重空缺，然而深度学习大多数有效的方法都是非凸的。现在有一些对深度学习常用模型及其目标函数的特性研究，期待能够发现非凸问题中局部最优解的相关规律。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;泛化（Generalization）：一个模型的泛化能力是指它在训练数据集上的误差是否能够接近所有可能测试数据误差的均值。泛化误差大致可以理解成测试数据集误差和训练数据集误差之差。在深度学习领域变流行之前，如何控制泛化误差一直是机器学习领域的主流问题。&lt;/p&gt;

&lt;p&gt;从实践方面来说，之前许多人担心的深度神经网络泛化能力较差的问题，在现实使用中并没有表现得很明显。这一方面源于大数据时代样本巨大的数量，另一方面近年出现了一些新的在实践上比较有效的控制泛化误差（Regularization）的方法，比如Dropout和DropConnect，以及非常有效的数据扩增（Data Agumentation）技术。是否还有其它实践中会比较有效的泛化误差控制方法一直是研究者们的好奇点，比如是否可以通过博弈法避免过拟合，以及是否可以利用无标记（Unlabeled）样本来辅助泛化误差的控制。&lt;/p&gt;

&lt;p&gt;从理论方面来说，深度学习的有效性使得PAC学习（Probably Approximately Correct Learning）相关的理论倍受质疑。这些理论无一例外地属于“上界的上界”的一个证明过程，而其本质无外乎各种集中不等式（Concentration Inequality）和复杂性度量（Complexity Measurement）的变种，因此它对深度学习模型有相当不切实际的估计。这不应该是泛函理论已经较为发达的当下出现的状况，因此下一步如何能够从理论上分析深度学习模型的泛化能力也会是一个有趣的问题。而这个研究可能还会牵涉表达（Representation，见下）的一些理论。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;表达（Representation）：这方面主要指的是深度学习模型和它要解决的问题之间的关系，比如给出一个设计好的深度学习模型，它适合表达什么样的问题，以及给定一个问题是否存在一个可以进行表达的深度学习模型。&lt;/p&gt;

&lt;p&gt;这方面的实践主要是两个主流，一方面那些笃信无监督学习（Unsupervised Learning）可行性的研究者们一直在寻找更好的无监督学习目标及其评价方法，以使得机器能够自主进行表达学习变得可能。这实际上包括了受限波尔兹曼模型（Restricted Boltzmann Machine），稀疏编码（Sparse Coding）和自编码器（Auto-encoder）等。另一方面，面对实际问题的科学家们一直在凭借直觉设计深度学习模型的结构来解决这些问题。这方面出现了许多成功的例子，比如用于视觉和语音识别的卷积神经网络（Convolutional Neural Network），以及能够进行自我演绎的深度回归神经网络（Recurrent Neural Network）和会自主玩游戏的深度强化学习（Reinforcement Learning）模型。绝大多数的深度学习研究者都集中在这方面，而这些也恰恰能够带来最大的学术影响力。&lt;/p&gt;

&lt;p&gt;然而，有关表达（Representation）的理论，除了从认知心理学和神经科学借用的一些启发之外，几乎是空白。这主要是因为是否能够存在表达的理论实际上依赖于具体的问题，而面对具体问题的时候目前唯一能做的事情就是去类比现实存在的智能体（人类）是如何解决这一问题的，并设计模型来将它归约为学习算法。我直觉上认为，终极的表达理论就像是拉普拉斯幽灵（Laplace&amp;#39;s Demon）一样，如果存在它便无所不知，也因此它的存在会产生矛盾，使得这一理论实际上只能无限逼近。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;应用（Applications）：深度学习的发展伴随着它对其它领域的革命过程。在过去的数年中，深度学习的应用能力几乎是一种“敢想就能成”的状态。这当然得益于现今各行各业丰富的数据集以及计算机计算能力的提升，同时也要归功于过去近三十年的领域经验。未来，深度学习将继续解决各种识别（Recognition）相关的问题，比如视觉（图像分类、分割，计算摄影学），语音（语音识别），自然语言（文本理解）；同时，在能够演绎（Ability to Act）的方面如图像文字描述、语音合成、自动翻译、段落总结等也会逐渐出现突破，更可能协助寻找NP难（NP-Hard）问题在限定输入集之后的可行算法。所有的这些都可能是非常好的研究点，能够带来经济和学术双重的利益。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr&gt;

&lt;p&gt;从应用角度来看,NLP是一个重要阵地，Nlp目前还没有像别的领域那样被dl席卷。成效明显的有MT和language model。Speech也很明显但那更偏向信号处理而非语言分析。&lt;/p&gt;

&lt;p&gt;DL的representation很吸引人，但是在constituent parsing上，Dan Klein撰文分析认为很难从word embedding得到好处。目前我听到的一些讨论认为这是由于人类对语言现象的解释比较好（向对于图像跟声波），也在此理解上搭了很丰富的理论框架，neural net这种自动学的feature未必占优。&lt;/p&gt;

&lt;p&gt;学者对此态度也明显份分阵地，比如Noah Smith经常调侃之，比如称之为derp learning。Chris manning 则大力支持，Michael Jordan 认为现在的成果不显著但此方向值得做。Ed Hovy则认为5年内Deep Learning 将全盘取胜，现在必须做。&lt;/p&gt;

&lt;p&gt;我比较倾向于Manning跟Hovy，他们对领域了解很透彻，下的判断是根据长年在领域里面浸淫的经验。两位都是功力深厚的linguistic兼cs专家，“见得多了”，“早已看穿了一切”。Noah则非常执着于神经网络的可解释性，认为其无法理解，但是某种程度的解释在未来应该是有可能的。所以要跳坑请尽早。&lt;/p&gt;
</description>
        <pubDate>Sat, 31 Jan 2015 00:00:00 +0800</pubDate>
        <link>http://yourdomain.com/2015/01/31/deep-learning.html</link>
        <guid isPermaLink="true">http://yourdomain.com/2015/01/31/deep-learning.html</guid>
        
        
      </item>
    
      <item>
        <title>最优化方法</title>
        <description>&lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;h1&gt;最优化方法&lt;/h1&gt;

&lt;p&gt;一般我们接触到的最优化分为两类：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;无约束最优化&lt;/li&gt;
&lt;li&gt;有约束最优化&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;无约束最优化&lt;/h2&gt;

&lt;p&gt;通常对于无约束最优化，首先要判断是否为凸函数。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/unconstrained-optimization-one&quot;&gt;无约束最优化&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.cnblogs.com/daniel-D/p/3377840.html&quot;&gt;机器学习中导数最优化方法&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://cseweb.ucsd.edu/%7Eelkan/250B/logreg.pdf&quot;&gt;最大似然、逻辑回归和随机梯度训练&lt;/a&gt;&lt;/p&gt;

&lt;h3&gt;梯度下降法&lt;/h3&gt;

&lt;h3&gt;牛顿法&lt;/h3&gt;

&lt;h3&gt;拟牛顿法&lt;/h3&gt;

&lt;h3&gt;共轭梯度法&lt;/h3&gt;

&lt;h2&gt;有约束最优化&lt;/h2&gt;

&lt;p&gt;一般采用拉格朗日方程，kkt，对偶问题求解。&lt;a href=&quot;http://www.moozhi.com/topic/show/54a8a261c555c08b3d59d996&quot;&gt;关于拉格朗日乘子法与KKT条件&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;譬如svm里，最大化几何间隔 max y(wx+b)/||w||&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://blog.csdn.net/v_july_v/article/details/7624837&quot;&gt;支持向量机&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;首先写出cost function：min [ 1/2*w^2 + max(0, 1 - y(wx+b) ) ]&lt;/p&gt;

&lt;p&gt;可以看出，这是一个有约束的问题，那么就可以用到&amp;quot;拉普拉斯+KKT+对偶&amp;quot;来求解了。&lt;/p&gt;

&lt;h2&gt;最优化算法的并行化&lt;/h2&gt;

&lt;h3&gt;Logistic Regression&lt;/h3&gt;

&lt;p&gt;这里主要以Logistic regression为例，讲一讲最优化算法的并行化实现。&lt;/p&gt;

&lt;p&gt;先看一下Logistic regression的损失函数：&lt;/p&gt;

&lt;p&gt;Logistic函数（或称为Sigmoid函数）：
$$sigmoid(z)=\frac{1}{1+e^{-z}}$$&lt;/p&gt;

&lt;p&gt;对于线性回归来说，其分类函数为：
$$h(x)=w&lt;em&gt;0+\sum&lt;/em&gt;{i=1}^p{w&lt;em&gt;ix&lt;/em&gt;i}=w^Tx$$
其中p是输入向量x的维度，也就是特征向量维度；w是特征权重向量。&lt;/p&gt;

&lt;p&gt;逻辑回归本质上是一个被logistic函数归一化后的线性回归，即在特征到结果的映射中加入了一层sigmoid函数映射。相比于线性回归，模型输出取值范围为[0，1]，&lt;/p&gt;

&lt;p&gt;如果y的取值是0或1，定义事件发生的条件概率为：
$$P(y=1|x)=\pi(x)=\frac{1}{1+exp(-h(x))}$$&lt;/p&gt;

&lt;p&gt;定义事件不发生的条件概率为：
$$P(y=0|x)=1-P(y=1|x)=\frac{1}{1+exp(h(x))}$$&lt;/p&gt;

&lt;p&gt;假设有n个观测样本，分别为：(\(x&lt;em&gt;1\),\(y&lt;/em&gt;1\))，(\(x&lt;em&gt;2\),\(y&lt;/em&gt;2\)) ... (\(x&lt;em&gt;n\),\(y&lt;/em&gt;n\))。&lt;/p&gt;

&lt;p&gt;得到一个观测值(\(x&lt;em&gt;i\),\(y&lt;/em&gt;i\))的概率为：
$$P(y&lt;em&gt;i)=p&lt;/em&gt;i^{y&lt;em&gt;i}(1-p&lt;/em&gt;i)^{1-y&lt;em&gt;i}$$  其中\(p&lt;/em&gt;i=P(y&lt;em&gt;i=1|x&lt;/em&gt;i)=\pi(x_i)\)&lt;/p&gt;

&lt;p&gt;由于各项观测独立，所以它们的联合分布可以表示为各边际分布的乘积：
$$l(w)=\prod&lt;em&gt;{i=1}^n{p&lt;/em&gt;i^{y&lt;em&gt;i}(1-p&lt;/em&gt;i)^{1-y_i}}$$&lt;/p&gt;

&lt;p&gt;对上述函数取对数，根据最大似然估计，得到最优化目标为：
$$\max{L(w)}=\max{log[l(w)]}=\max{\sum&lt;em&gt;{i=1}^n{y&lt;/em&gt;i&lt;em&gt;log[\pi(x&lt;em&gt;i)] + (1-y&lt;/em&gt;i)&lt;/em&gt;log[1-\pi(x_i)]}}$$
而如果y的取值是1或-1，则最优化目标为：&lt;/p&gt;

&lt;p&gt;$$\max{L(w)}=\max{\sum&lt;em&gt;{i=1}^n{-log[1+exp(-y&lt;/em&gt;iw^Tx_i)]}}$$&lt;/p&gt;

&lt;p&gt;加上正则项后则是：&lt;/p&gt;

&lt;p&gt;$$\min&lt;em&gt;w \frac{1}{2}||w||^2+C\sum&lt;/em&gt;{i=1}^{n}log[1+exp(-y&lt;em&gt;iw^Tx&lt;/em&gt;i)]$$&lt;/p&gt;

&lt;h3&gt;LR的MapReduce并行&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/lbfgs.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/lbfgs_two_loops.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/lbfgs_two_loops_vf.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://www.csie.ntu.edu.tw/%7Ecjlin/libsvmtools/distributed-liblinear/&quot;&gt;Distributed LIBLINEAR&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://nips.cc/Conferences/2014/Program/event.php?ID=4831&quot;&gt;Large scale learning spotlights&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Loss Function&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://web.mit.edu/lrosasco/www/publications/loss.pdf&quot;&gt;loss.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/JohnLangford/vowpal_wabbit/wiki/Loss-functions&quot;&gt;vowpal_wabbit Loss-functions &lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Loss_function&quot;&gt;Loss function wiki&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://image.diku.dk/shark/sphinx_pages/build/html/rest_sources/tutorials/concepts/library_design/losses.html&quot;&gt;shark loss function&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;adaboost，svm，lr三个算法的关系：&lt;/p&gt;

&lt;p&gt;三种算法的分布对应exponential loss（指数损失函数），hinge loss，log loss（对数损失函数），无本质区别。应用凸上界取代0、1损失，即凸松弛技术。从组合优化到凸集优化问题。凸函数，比较容易计算极值点。&lt;/p&gt;
</description>
        <pubDate>Thu, 15 Jan 2015 00:00:00 +0800</pubDate>
        <link>http://yourdomain.com/2015/01/15/optimal_method.html</link>
        <guid isPermaLink="true">http://yourdomain.com/2015/01/15/optimal_method.html</guid>
        
        
      </item>
    
      <item>
        <title>gbdt_adaboost_bootstrap</title>
        <description>&lt;h2&gt;gbdt 与 adaboost&lt;/h2&gt;

&lt;p&gt;这两个算法在一定程度上其实是有些接近的，我们不妨分别来看看。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://suanfazu.com/t/gbdt-die-dai-jue-ce-shu-ru-men-jiao-cheng/135&quot;&gt;文档学习链接&lt;/a&gt;
&lt;a href=&quot;http://www.schonlau.net/publication/05stata_boosting.pdf&quot;&gt;Boosting Decision Tree入门教程&lt;/a&gt;
&lt;a href=&quot;http://research.microsoft.com/pubs/132652/MSR-TR-2010-82.pdf&quot;&gt;LambdaMART用于搜索排序入门教程&lt;/a&gt;&lt;/p&gt;

&lt;h3&gt;gbdt&lt;/h3&gt;

&lt;p&gt;意为 gradient boost decision tree。又叫MART（Multiple Additive Regression Tree)&lt;/p&gt;

&lt;h4&gt;分类树和回归树&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;分类树：预测分类标签；C4.5；选择划分成两个分支后熵最大的feature；&lt;/li&gt;
&lt;li&gt;回归树：预测实数值；回归树的结果是可以累加的；最小化均方差；&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;boosting&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;gbdt的核心在于：每一棵树学的是之前所有树的结论和残差。每一步的残差计算其实变相地增大了分错instance的权重，而已经分对的instance则都趋向于0。&lt;/li&gt;
&lt;li&gt;Adaboost：是另一种boost方法，它按分类对错，分配不同的weight，计算cost function时使用这些weight，从而让“错分的样本权重越来越大，使它们更被重视”。&lt;/li&gt;
&lt;li&gt;Bootstrap也有类似思想，它在每一步迭代时不改变模型本身，也不计算残差，而是从N个instance训练集中按一定概率重新抽取N个instance出来（单个instance可以被重复sample），对着这N个新的instance再训练一轮。由于数据集变了迭代模型训练结果也不一样，而一个instance被前面分错的越厉害，它的概率就被设的越高，这样就能同样达到逐步关注被分错的instance，逐步完善的效果。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;Shrinkage&lt;/h4&gt;

&lt;p&gt;Shrinkage（缩减）的思想认为，每次走一小步逐渐逼近结果的效果，要比每次迈一大步很快逼近结果的方式更容易避免过拟合。即它不完全信任每一个棵残差树，它认为每棵树只学到了真理的一小部分，累加的时候只累加一小部分，通过多学几棵树弥补不足。&lt;/p&gt;
</description>
        <pubDate>Thu, 15 Jan 2015 00:00:00 +0800</pubDate>
        <link>http://yourdomain.com/2015/01/15/gbdt_adaboost_bootstrap.html</link>
        <guid isPermaLink="true">http://yourdomain.com/2015/01/15/gbdt_adaboost_bootstrap.html</guid>
        
        
      </item>
    
      <item>
        <title>推荐算法实战</title>
        <description>&lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;h1&gt;推荐算法实战&lt;/h1&gt;

&lt;h4&gt;author: vincentyao@tencent.com&lt;/h4&gt;

&lt;p&gt;推荐在计算广告上有很多的运用，这里计划把推荐算法总结一下。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;value_of_recommender.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;h2&gt;推荐算法介绍&lt;/h2&gt;

&lt;h3&gt;推荐问题定义&lt;/h3&gt;

&lt;p&gt;Estimate a utility function that automatically predicts how a user will like an item。&lt;/p&gt;

&lt;h3&gt;Memory-based Collaborative Filtering&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;协同（collaborating）是群体行为，过滤（filtering）则是针对个人的行为。&lt;/li&gt;
&lt;li&gt;假设有m个user，n个item。每个用户都有一个关联的item list，并且通过显式或隐式的反馈，对每个item都有一个rating \(v_{i,j}\)。&lt;/li&gt;
&lt;li&gt;显式评分指用户使用系统提供的方式进行评分或者评价; 隐式评分则根据使用者的行为模式由系统代替使用者完成评价，行为模式包括用户的浏览行为、购买行为等等。&lt;/li&gt;
&lt;li&gt;&lt;p&gt;基本步骤为：(1)收集用户评分；(2)最近邻搜索；(3)产生推荐结果。第3步中，较常见的推荐算法有Top-N推荐和关联推荐。Top-N推荐比较熟悉，关联推荐是对最近邻使用者的记录进行关联规则(association rules)挖掘。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;长处：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;minimal knowledge，content-agnostic，可以在不理解item和user的情况下，做任何类型的推荐。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;短处：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;需要大量可信的用户反馈数据&lt;/li&gt;
&lt;li&gt;不考虑上下文信息&lt;/li&gt;
&lt;li&gt;商品都是标准化的(Users should have bought exactly the same product)&lt;/li&gt;
&lt;li&gt;content-agnostic（与内容无关的），容易推荐popular items，即Popularity Bias。&lt;/li&gt;
&lt;li&gt;new and unpopular items cannot be recommended，即cold-start problem。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;User-based&lt;/h4&gt;

&lt;p&gt;先找最近邻user，再基于最近邻user预测。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;User之间相似度计算
&lt;img src=&quot;user_based_similarity.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Prediction for user i and items j：
&lt;img src=&quot;user_based_prediction.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;Item-based&lt;/h4&gt;

&lt;p&gt;先计算item similarity，再基于user rated items预测。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Normalization/Bias，rate bias：\(b&lt;em&gt;{ui} = μ(global) + b&lt;/em&gt;u(user bias) + b&lt;em&gt;i(item bias)\)，\(s&lt;/em&gt;k(i,u)\) is k-nearest neighbors to i that were rated by user u
&lt;img src=&quot;cf_formula.png&quot; alt=&quot;&quot;&gt;
&lt;img src=&quot;user_item_bias.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;item-based计算流程：
&lt;img src=&quot;./item_similarity.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;Association rules(关联规则)&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;关联规则分析 (Association Rules，又称 Basket Analysis) 用于从大量数据中挖掘出有价值的数据项之间的相关关系。经典论文&lt;a href=&quot;&quot;&gt;Mining Association Rules between Sets of Items in Large Databases&lt;/a&gt;。关联规则解决的常见问题如：“如果一个消费者购买了产品A，那么他有多大机会购买产品B?”以及“如果他购买了产品C和D，那么他还将购买什么产品？”&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;常见算法有：Apriori算法和FP-growth算法。请参考&lt;a href=&quot;http://zh.wikipedia.org/wiki/%E5%85%88%E9%AA%8C%E7%AE%97%E6%B3%95&quot;&gt;wiki&lt;/a&gt;。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;关联规则面向的是transaction，而User-based or Item based面向的是用户偏好（评分），协同过滤在计算相似商品的过程中可以使用关联规则分析。具体请参考&lt;a href=&quot;http://www.zhihu.com/question/22404652&quot;&gt;协同过滤和关联规则分析的区别是什么&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;总结&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Problem: sparsity，scalability&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;利用latent models做降维。Methods of dimensionality reduction: Matrix Factorization, Clustering，Projection(PCA) ...&lt;/li&gt;
&lt;li&gt;瓶颈点：相关性计算。将最近邻产生与预测分为两个步骤，其中相关性计算时间复杂度很高。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;个人总结，一般来讲，item-based方法更好用，因为item之间的similarity是相对静态，但user之间的similarity相对动态。当item base is smaller than user or changes rapidly时，采用user-based方法更合适；相反，当user base is small时，Item-based方法更合适；&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Model-based Collaborative Filtering&lt;/h3&gt;

&lt;h4&gt;SVD/MF&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;SVD&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;U是user-factor矩阵，V是item-factor矩阵。
&lt;img src=&quot;model_based_svd.png&quot; alt=&quot;&quot;&gt;&lt;/li&gt;
&lt;li&gt;基于svd的rating过程：
&lt;img src=&quot;svd_rating.png&quot; alt=&quot;&quot;&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;先写出loss function，再利用SGD or Alternating least squares求解
&lt;img src=&quot;svd_object_function.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;增加bias后：
&lt;img src=&quot;svd_object_function_bias.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;关于user/item biases的作用，补充几点：(1)偏好信息的充分利用；(2)能充分利用用户、物品的profile等属性信息；(3)属性之间能方便的进行各种组合。&lt;/p&gt;

&lt;p&gt;关于求解算法，Alternating least squares方法的原理是：首先固定item vectors，最优化user vectors，再固定user vectors，最优化item vectors。SGD就是梯度下降法，相对更快。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;算法优点：(1)将用户和物品用隐特征(latentfeature)连接在一起；(2)MatrixFactorization有明确的数学理论基础(singularvalue)和优 化目标,容易逼近最优解；(3)对数据稀疏性(datasparsity)和抗噪音干扰的处理效果较好; (4)延展性(scalability)很好;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;算法缺点：(1)可解释性弱；(2)难以实时更新(适用于离线计算)；(3)Overfitting without regularization，特别是fewer reviews than dimensions。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;SVD++&lt;/p&gt;

&lt;p&gt;SVD++ 是SVD模型的加强版，除了打分关系，SVD++还可以对隐含的回馈(implicit feedback) 进行建模。
除了在SVD中定义的向量外，每个item对应一个向量yi，来通过user隐含回馈过的item的集合来刻画用户的偏好。&lt;/p&gt;

&lt;p&gt;$$\hat{r}&lt;em&gt;{ui} = \mu + b&lt;/em&gt;i + b&lt;em&gt;u + q&lt;/em&gt;i^T (p&lt;em&gt;u + |R(u)|^{-\frac{1}{2}} \sum&lt;/em&gt;{j\in R(u)} y&lt;em&gt;i)$$
其中， R(u) 代表user隐含回馈（打分过的）过的item的集合。
可以看到，现在user被建模为\( p&lt;/em&gt;u + |R(u)|^{-\frac{1}{2}} \sum&lt;em&gt;{j\in R(u)} y&lt;/em&gt;i \)，
具体请参考&lt;a href=&quot;http://www.superchun.com/machine-learning/svd1.html&quot;&gt;文章SVD/SVD++&lt;/a&gt;，&lt;a href=&quot;http://research.yahoo.com/files/kdd08koren.pdf&quot;&gt;论文Factorization Meets the Neighborhood: a Multifaceted Collaborative Filtering Model&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;SVDfeature&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.jmlr.org/papers/volume13/chen12a/chen12a.pdf&quot;&gt;SVDFeature: A Toolkit for Feature-based Collaborative Filtering&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;svdfeature.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;svdfeature2.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;开源库，请参考&lt;a href=&quot;http://blog.csdn.net/cserchen/article/details/14231153&quot;&gt;推荐系统开源软件列表汇总和点评&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://www.libfm.org&quot;&gt;libFM&lt;/a&gt;，by Steffen Rendle。特点是实现了MCMC（Markov Chain Monte Carlo）优化算法，比常见的SGD（随即梯度下降）优化方法精度要高（当然也会慢一些）。&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://svdfeature.apexlab.org/wiki/Main_Page&quot;&gt;svdfeature&lt;/a&gt;，by 上海交大。&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.csie.ntu.edu.tw/%7Ecjlin/libmf/&quot;&gt;libMF&lt;/a&gt;，by国立台湾大学。参考论文&lt;a href=&quot;http://www.csie.ntu.edu.tw/%7Ecjlin/papers/libmf/libmf_journal.pdf&quot;&gt;Y. Zhuang, W.-S. Chin, Y.-C. Juan, and C.-J. Lin. A Fast Parallel SGD for Matrix Factorization in Shared Memory Systems&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.csie.ntu.edu.tw/%7Ecjlin/nmf/&quot;&gt;NMF&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;RBM&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;&quot;&gt;论文 Restricted Boltzmann Machines for Collaborative Filtering&lt;/a&gt;&lt;/p&gt;

&lt;h4&gt;Clustering&lt;/h4&gt;

&lt;p&gt;将用户聚类后，再基于传统CF的方法(此时user不是单独的用户，而是cluster)&lt;/p&gt;

&lt;h4&gt;Locality-sensitive hashing&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Method for grouping similar items in highly dimensional spaces；&lt;/li&gt;
&lt;li&gt;Find a hashing function s.t. similar items are grouped in the same buckets;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;Classifiers&lt;/h4&gt;

&lt;p&gt;Classifiers can be used in CF and CB Recommenders。优点：可以和其他方法结合使用。缺点是：需要一份训练集。&lt;/p&gt;

&lt;h3&gt;Content-based Recommenders&lt;/h3&gt;

&lt;p&gt;item/user profiles, category, tag/keyword&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;content_based.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;h3&gt;New approaches&lt;/h3&gt;

&lt;h4&gt;Learning to Rank&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;一个机器学习问题，目标是从训练数据里构建一个ranking model。排序学习指在排序生成 (ranking creation) 和排序整合 (ranking aggregation) 中用于构建排序模型的机器学习方法。&lt;a href=&quot;http://www.icst.pku.edu.cn/lcwm/course/WebDataMining/slides2012/8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8F%8A%E6%8E%92%E5%BA%8F%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80.pdf&quot;&gt;slice&lt;/a&gt;，&lt;a href=&quot;http://www.cnblogs.com/kemaswill/archive/2013/06/01/3109497.html&quot;&gt;Learning to Rank简介&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Learning to rank is a key element for personalization&lt;/li&gt;
&lt;li&gt;Treat the problem as a standard supervised classification problem&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://people.cs.umass.edu/%7Evdang/ranklib.html&quot;&gt;Ranklib code&lt;/a&gt;，&lt;a href=&quot;http://svmlight.joachims.org&quot;&gt;svmlight&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Pointwise

&lt;ul&gt;
&lt;li&gt;Ranking score based on regression or classification&lt;/li&gt;
&lt;li&gt;LR, SVM, GBDT, McRank ...&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Pairwise

&lt;ul&gt;
&lt;li&gt;Randing problem是二分类问题，pair-wise的比较，将排序问题转换为分类问题&lt;/li&gt;
&lt;li&gt;RankSVM, RankBoost, RankNet, FRank ...&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Listwise&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ListNet: KL-divergence as loss function by define a probability distribution&lt;/li&gt;
&lt;li&gt;RankCosine: similarity between ranking list and ground truth as loss function&lt;/li&gt;
&lt;li&gt;Lambda Rank，ListNet，ListMLE，AdaRank，SVMap ...&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;learning_to_rank_compare.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;Context-aware Recommendations&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://ids.csom.umn.edu/faculty/gedas/NSFCareer/CARS-chapter-2010.pdf&quot;&gt;论文Context-Aware Recommender Systems&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;R: User * Item -&amp;gt; Rating    比较  R: User * Item * Context -&amp;gt; Rating&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;传统推荐过程框图
&lt;img src=&quot;general-recommender.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;将context纳入推荐系统后
&lt;img src=&quot;context-recommender1.png&quot; alt=&quot;&quot;&gt;
&lt;img src=&quot;context-recommender2.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;两种方法：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Tensor Factorization
&lt;img src=&quot;hosvd.png&quot; alt=&quot;&quot;&gt;&lt;/li&gt;
&lt;li&gt;Factorization Machines&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;Deep learning: ANN training, Recurrent Networks&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://erikbern.com/?p=589&quot;&gt;Recurrent Neural Networks for Collaborative Filtering&lt;/a&gt;，&lt;a href=&quot;http://www.slideshare.net/erikbern/collaborative-filtering-at-spotify-16182818?related=1&quot;&gt;Collaborative Filtering at Spotify&lt;/a&gt;
Recurrent neural networks have a simple model that tries to predict the next item given all previous ones。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;cf-RNN-1.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://benanne.github.io/2014/08/05/spotify-cnns.html&quot;&gt;Recommending music on Spotify with deep learning&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;Similarity: graph-based similarity(simrank)&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Graph-based similarities

&lt;ul&gt;
&lt;li&gt;SimRank: two objects are similar if they are referenced by similar objects&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www-cs-students.stanford.edu/%7Eglenj/simrank.pdf&quot;&gt;论文 SimRank&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.spiral.pro/big_data/simrank-intro.html&quot;&gt;SimRank原理&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;Social Recommendations&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Social and Trust-based recommenders
&lt;img src=&quot;trust_based_recommenders.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;关系链 Friendship Demographic methods&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;Ranking and Session Modeling&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Independent click model&lt;/li&gt;
&lt;li&gt;Logistic click model。Exponential family model for click; user looks at all&lt;/li&gt;
&lt;li&gt;Sequential click model; User traverses list&lt;/li&gt;
&lt;li&gt;Skip click model&lt;/li&gt;
&lt;li&gt;Context skip click model&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Hybrid Approaches&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Online-Nearline-Offline Recommendation（在线-近线-离线）三层混合机制&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.52ml.net/318.html&quot;&gt;推荐系统中所使用的混合技术介绍&lt;/a&gt;
&lt;img src=&quot;./hybridization.png&quot; alt=&quot;&quot;&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;推荐算法对比&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://dataunion.org/bbs/forum.php?mod=viewthread&amp;amp;tid=835&amp;amp;extra=&quot;&gt;推荐系统的常用算法对比&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/oopsoom/article/details/33740799&quot;&gt;推荐算法总结&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;常用推荐算法点评(by 陈运文)
- Item-based collaborative filtering
    - 应用最为广泛的方法
    - 存在各种计算方法的改进;但Similarity计算随意性大
- Content-based algorithm
    - 实现简单、直观,常用于处理冷启动问题
    - 推荐精度低
- Latent Factor Model
    - 单一模型效果最好的方法;但难以实时更新模型
    - KDD-Cup,Netflix Prize ...
- Statistics-based
    - 简陋,直观,非个性化,被大量使用
    - 可用于补足策略&lt;/p&gt;

&lt;h3&gt;效果评估&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;MAP/nDCG: top-N推荐&lt;/li&gt;
&lt;li&gt;RMSE/MAE: 评分预测问题&lt;/li&gt;
&lt;li&gt;A/B Testing: 点击率、转化率&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;推荐系统实战Case&lt;/h2&gt;

&lt;h3&gt;Netflix&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;top-2 algorithms：SVD，RBM&lt;/li&gt;
&lt;li&gt;具体请参考&lt;a href=&quot;http://buzzard.ups.edu/courses/2014spring/420projects/math420-UPS-spring-2014-gower-netflix-SVD.pdf&quot;&gt;Netflix Prize and SVD&lt;/a&gt;，&lt;a href=&quot;http://www.netflixprize.com/assets/GrandPrize2009_BPC_BellKor.pdf&quot;&gt;The BellKor Solution to the Netflix Grand Prize&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;&lt;a href=&quot;https://breezedeus.github.io/2012/11/01/breezedeus-yuanquan-etao.html&quot;&gt;个性化推荐技术#总结-袁全&lt;/a&gt;&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;相关性推荐，点击数据更有用；补充性推荐，购买数据更有用；要根据用户行为意图选择不同的推荐方法。&lt;/li&gt;
&lt;li&gt;对于不同种类的产品，当用户处在同一购物流程时，其理想的相关性推荐/补充性推荐的概率也差别很大。&lt;/li&gt;
&lt;li&gt;Mixture Logistic Regression&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;&lt;a href=&quot;https://breezedeus.github.io/2012/11/10/breezedeus-jiangshen.html&quot;&gt;面向广告主的推荐，江申@百度&lt;/a&gt;&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;技术目标要正确；譬如对于拍卖词推荐，其数学目标的烟花过程为：推荐最相关的词-&amp;gt;推荐广告主采用率最高的词-&amp;gt;推荐采用率最高且产生推广效果最佳的词。&lt;/li&gt;
&lt;li&gt;在拍卖词推荐中主要涉及到三种模型：相关性模型、采用率模型和推广效果模型。&lt;/li&gt;
&lt;li&gt;负反馈：按照item已经对user展示的次数指数级降低其权重，避免同一个item多次重复被展示给一个用户。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;&lt;a href=&quot;https://breezedeus.github.io/2012/11/12/breezedeus-wenguozhu.html&quot;&gt;个性化推荐技术#总结 稳国柱@豆瓣&lt;/a&gt;&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;电影推荐：首先把电影按照电影标签进行分组（比如分成动作片，剧情片等）；然后在每个组里面使用CF算法产生推荐结果；最后把每组中获得的推荐按照加权组合的方式组合在一块。&lt;/li&gt;
&lt;li&gt;图书推荐：图书有一定的阶梯性，在大部分的场合，我们需要的并不是与自己相似的用户的推荐，而是与自己相似的专家的推荐。&lt;/li&gt;
&lt;li&gt;电台的音乐推荐：必须使用一个算法系统（其中包含多个算法）来针对不同的用户进行不同的算法调度&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;&lt;a href=&quot;http://www.douban.com/note/472267231/&quot;&gt;年终总结 &amp;amp; 算法数据的思考 by 飞林沙&lt;/a&gt;&lt;/h3&gt;

&lt;h3&gt;&lt;a href=&quot;https://breezedeus.github.io/2015/01/31/breezedeus-review-for-year-2014-tech.html&quot;&gt;世纪佳缘用户推荐系统的发展历史&lt;/a&gt;&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&amp;quot;总结、温习，这两点让人成长。而不是你走得有多快！&amp;quot;&lt;/li&gt;
&lt;li&gt;天真的算法年：item-based kNN。推荐以前看过的item的相似item。可逆（Reciprocal）推荐算法，是什么东西？&lt;a href=&quot;http://search.aol.com/aol/search?s_it=topsearchbox.search&amp;amp;v_t=opensearch&amp;amp;q=Reciprocal+recommendation&quot;&gt;Reciprocal recommendation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;技术为产品服务，而不是直接面向用户；数据质量是地基，保证好的质量很不容易；如何制定正确的优化指标真的很难；业务理解 &amp;gt; 工程实现；数据 &amp;gt; 系统 &amp;gt; 算法；快速试错；&lt;/li&gt;
&lt;li&gt;Dirichlet Process 和 Dirichlet Process Mixture&lt;/li&gt;
&lt;li&gt;Alternating Direction Method of Multipliers(ADMM)&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://breezedeus.github.io/2014/11/19/breezedeus-feature-mining-gbdt.html&quot;&gt;利用GBDT模型构造新特征&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://breezedeus.github.io/2014/11/20/breezedeus-feature-hashing.html&quot;&gt;特征哈希（Feature Hashing）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;不平衡数据的抽样方法。参考文献：William Fithian, Trevor Hastie, Local Case-Control Sampling Efficient Subsampling in Imbalanced Data Sets, 2014.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.douban.com/note/484853135/&quot;&gt;世纪佳缘推荐系统之我见&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;明确推荐评价指标：对于婚恋推荐系统来说，最核心的指标无外乎付费的转换率 - 我们倒着来推，把问题转换为识别出最愿意付费的那些用户，然后找到这些用户感兴趣的用户，通过产品引导让这些用户发信&lt;/li&gt;
&lt;li&gt;能不能从数据跳出来对产品提出一些创意性改进从而产生的产品模式和收费模式的变革。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;&lt;a href=&quot;http://www.wsdm-conference.org/2015/wp-content/uploads/2015/02/WSDM-2015-PE-Leskovec.pdf&quot;&gt;New Directions in Recommender Systems&lt;/a&gt;&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://www.douban.com/note/484692347/&quot;&gt;飞林沙-读后总结&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;需要理解 可替换 和 可补充，这两种推荐形式。&lt;/li&gt;
&lt;li&gt;怎样生成替代品的推荐理由，应该是更好，而不是他们包含同一关键词&lt;/li&gt;
&lt;li&gt;推荐一整套装备&lt;/li&gt;
&lt;li&gt;Inferring Networks from Opinions阅读总结：

&lt;ul&gt;
&lt;li&gt;Product Graph：Building networks from product text       - Understand the notions of substitute and complement goods     - Generate explanations of why certain products are preferred

&lt;ul&gt;
&lt;li&gt;Recommends baskets of related items&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;learn x and y are related?

&lt;ul&gt;
&lt;li&gt;Attempt 1: Text features；缺点：High-dimensional，Prone to overfitting，Too fine-grained&lt;/li&gt;
&lt;li&gt;Attempt 2: Features from Topics。也就是把第一种方法，用topic vector替换，相当于降维了。&lt;/li&gt;
&lt;li&gt;Attempt 3: Learn ‘good’ topics。Learn to discover topics that explain the graph structure；

&lt;ul&gt;
&lt;li&gt;Idea: Learn both simultaneously；we want to learn to project documents (reviews) into topic space such that related products are nearby；&lt;/li&gt;
&lt;li&gt;Combining topic models with link prediction；topic和link利用一个目标函数，一起训练。&lt;/li&gt;
&lt;li&gt;Issue 1: Relationships we want to learn are not symmetric；Solution: We solve this issue by learning “relatedness” in addition to “directedness”&lt;/li&gt;
&lt;li&gt;Issue 3: The model has a too many parameters；Solution: Product hierarchy；Associate each node in the category tree with a small number of topics&lt;/li&gt;
&lt;li&gt;整个模型用EM算法来求解，类似于PLSA的EM算法。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;&lt;a href=&quot;http://tech.meituan.com/machinelearning-data-feature-process.html&quot;&gt;美团推荐团队-机器学习中的数据清洗与特征处理综述&lt;/a&gt;&lt;/h3&gt;

&lt;h3&gt;&lt;a href=&quot;http://www.csdn.net/article/2015-01-30/2823783&quot;&gt;美团推荐算法实践：机器学习重排序模型成亮点&lt;/a&gt;&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://tech.meituan.com/mt-recommend-practice.html&quot;&gt;美团推荐算法实践&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;本文介绍了美团网推荐系统的构建和优化过程中的一些做法，包括数据层、触发层、融合过滤层和排序层五个层次，采用了HBase、Hive、storm、Spark和机器学习等技术。两个优化亮点是将候选集进行融合与引入重排序模型。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;&lt;a href=&quot;http://benanne.github.io/2014/08/05/spotify-cnns.html&quot;&gt;文章-Recommending music on Spotify with deep learning&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;基于作者的实习经历讲Spotify的音乐推荐，内容涉及：协同过滤、基于内容的推荐、基于深度学习的品味预测、convnets规模扩展、convnets的学习内容、推荐的具体应用等&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Collaborative filtering：content-agnostic（与内容无关的），容易推荐popular items。另外，new and unpopular songs cannot be recommended，即cold-start problem。&lt;/li&gt;
&lt;li&gt;Content-based：tags, artist and album information, lyrics, text mined from the web (reviews, interviews, …), and the audio signal itself（e.g. the mood of the music）。&lt;/li&gt;
&lt;li&gt;Predicting listening preferences with deep learning。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;&lt;a href=&quot;http://www.aszxqw.com/work/2014/06/01/tuijian-xitong-de-nadianshi.html&quot;&gt;推荐系统的那点事&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;分析了推荐系统中使用算法的误区，确实规则带来的好处简单有效。 当一个做推荐系统的部门开始重视【数据清理，数据标柱，效果评测，数据统计，数据分析】这些所谓的脏活累活，这样的推荐系统才会有救。&lt;/p&gt;

&lt;h3&gt;&lt;a href=&quot;http://data-artisans.com/computing-recommendations-with-flink.html&quot;&gt;文章 Computing Recommendations at Extreme Scale with Apache Flink and Google Compute Engine&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;Flink实例！用Flink和GAE做面向大规模数据集的协同推荐，从中可看出Flink的巨大应用潜力，文中引用的材料值得一读（作者说了，细节文章即将推出敬请期待，感兴趣请持续关注）&lt;/p&gt;

&lt;h3&gt;&lt;a href=&quot;http://www.52ml.net/318.html&quot;&gt;推荐系统中所使用的混合技术介绍&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;系统架构层面一般使用多段组合混合推荐框架，算法层面则使用加权型混合推荐技术，包括LR、RBM、GBDT系列。此外还介绍分级型混合推荐技术，交叉调和技术，瀑布型混合方法，推荐基础特征混合技术，推荐模型混合技术，整体式混合推荐框架等。&lt;/p&gt;

&lt;h3&gt;&lt;a href=&quot;http://www.52nlp.cn/cikm-competition-topdata&quot;&gt;CIKM Competition数据挖掘竞赛夺冠算法陈运文&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;该文讲述的不是推荐算法，而是一个分类问题，不过也有一些对我个人有启发的地方：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;考虑到样本分布不均匀，在计算Macro Precision和Recall时，由于分母是该category的Query number，所以越是稀少的类别，其每个Query的预测精度对最终F1值的影响越大。换句话说冷门类别对结果的影响更大，需要格外关注。&lt;/li&gt;
&lt;li&gt;多分类问题的处理方式，没有将跨类的样本进行拆分，而是将多类的训练样本单独作为一个类别，实践验证效果更好。&lt;/li&gt;
&lt;li&gt;社交网络中的智能推荐的思想也可以在这里运用。类似推荐系统中的&lt;User, Item&gt;关系对，这里&lt;Query, Title&gt;的关系可以使用协同过滤（Collaborative Filtering）的思想，当两个Query所点击的Title列表相似时，则另外Query的category可以被“推荐”给当前Query。&lt;/li&gt;
&lt;li&gt;&lt;p&gt;在Ensemble框架下，分类器分为两个Level: L1层和L2层。L1层是基础分类器，前面所提到的方法均可以作为L1层分类器来使用；L2层基于L1层，将L1层的分类结果形成特征向量，再组合一些其他的特征后，形成L2层分类器（如SVM）的输入。这里需要特别留意的是用于L2层的训练的样本必须没有在训练L1层时使用过。&lt;/p&gt;

&lt;p&gt;在设计Ensemble L1层算法的过程中，有很多种设计思路，我们选择了不同的分类算法训练多个分类模型，而另外有队伍则为每一个类别设计了专用的二分分类器，每个分类器关注其中一个category的分类(one-vs-all)；也可以选择同一种分类算法，但是用不同的特征训练出多个L1层分类器；另外设置不同的参数也能形成多个L1层分类器等&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;&lt;a href=&quot;http://www.csdn.net/article/2014-08-27/2821403-the-top-9-of-ali-bigdata-competition/9&quot;&gt;学生强则国强，访天猫推荐算法大赛Top 9团队&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;根据用户4个月在天猫的行为日志，预测用户u在将来一个月是否会购买某个品牌b&lt;/p&gt;

&lt;p&gt;模型的训练思想：&lt;/p&gt;

&lt;p&gt;由于这个问题中正负样本比例悬殊，我们使用了级联的思想过滤掉大量的样本来提升训练速度，同时也提升了模型准确率。在第一级选用训练和预测速度都较快的逻辑回归模型，过滤掉&amp;gt;80%的样本。在第二级选用拟合能力更强的GBRT、RF、神经网络等非线性模型。最后选用神经网络将第二级的非线性模型融合起来。&lt;/p&gt;

&lt;h3&gt;&lt;a href=&quot;http://www.slideshare.net/scmyyan/large-scale-recommendation-in-ecommerce-qiang-yan&quot;&gt;Large scale recommendation in e-commerce -- qiang yan&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;Justin:online match + online learning works very well
赞processing stack。我们最近在豆瓣fm上也做了online learning，improve10个点左右&lt;/p&gt;

&lt;h2&gt;推荐系统总结&lt;/h2&gt;

&lt;h3&gt;实践中的关键点&lt;/h3&gt;

&lt;h4&gt;数据预处理&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;更多的有效数据，更好的推荐效果&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;隐式反馈的使用&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;显式反馈(explicit feedbacks):

&lt;ul&gt;
&lt;li&gt;购买、评分、接受推荐、点击喜欢。。。&lt;/li&gt;
&lt;li&gt;数量稀疏(用户是最懒的人)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;隐式反馈(implicit Feedbacks):

&lt;ul&gt;
&lt;li&gt;浏览、收听、点击、下载。。。&lt;/li&gt;
&lt;li&gt;User/item相关的profile、keyword、tags&lt;/li&gt;
&lt;li&gt;反馈中占大多数(往往被忽略)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;如何利用好隐式反馈?

&lt;ul&gt;
&lt;li&gt;对提高推荐精度有良好效果(SVD-&amp;gt;SVD++)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;SNS关系的使用&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;SNS关系包括：

&lt;ul&gt;
&lt;li&gt;follower/followee，好友，群关系&lt;/li&gt;
&lt;li&gt;user-user actions，e.g. &amp;quot;retweet/at/comment...&amp;quot;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;SNS关系的使用

&lt;ul&gt;
&lt;li&gt;用于user之间的关系计算(graph-based)&lt;/li&gt;
&lt;li&gt;作为隐式反馈，用于SVD++&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;时间因素的使用&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;user的行为受时间影响&lt;/li&gt;
&lt;li&gt;Item的状态也受时间影响&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;利用地域信息&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;特定的应用场景&lt;/li&gt;
&lt;li&gt;基于规则&lt;/li&gt;
&lt;li&gt;基于地域信息的关联规则挖掘&lt;/li&gt;
&lt;li&gt;Item-based协同过滤，item similarity计算时加入距离属性&lt;/li&gt;
&lt;li&gt;Latent factor，user-location作为隐式反馈使用&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;User冷启动的处理&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;热门推荐(排行榜)永远都是一个可用的方案

&lt;ul&gt;
&lt;li&gt;点击总量最多&lt;/li&gt;
&lt;li&gt;最近点击最多&lt;/li&gt;
&lt;li&gt;评分最高&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;充分利用任何用户信息

&lt;ul&gt;
&lt;li&gt;性别、年龄&lt;/li&gt;
&lt;li&gt;来自其他应用的数据&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;口味测试

&lt;ul&gt;
&lt;li&gt;有代表性的选项&lt;/li&gt;
&lt;li&gt;热门/大部分用户熟知的选项&lt;/li&gt;
&lt;li&gt;有区分度的选项&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;Item冷启动的处理&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Content-basedmethods永远都是一个可用方案

&lt;ul&gt;
&lt;li&gt;Category&lt;/li&gt;
&lt;li&gt;Tags&lt;/li&gt;
&lt;li&gt;Topic&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;相关技术(NLP、ML)

&lt;ul&gt;
&lt;li&gt;自动分类&lt;/li&gt;
&lt;li&gt;自动标签提取&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;倒排索引的使用

&lt;ul&gt;
&lt;li&gt;适用于item数量庞大&lt;/li&gt;
&lt;li&gt;索引的查询与合并&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;个人总结&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;技术目标要正确。&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;1. 首先要定义好什么叫”好的推荐“，这是解决任何一个技术问题的前提。
2. 在有了明确的定义之后，实际问题一般会蜕变为一个优化问题，用数学工具给出它最好的解答。
3. 数学上的解答可能在技术上无法实现，或者说有可能复杂度太高，那么需要一个比较好的近似解。不要小看这一步，大部分问题出在这里。
4. 迭代改进。算法实现以后可能实际表现与预想的不同，需要重新定义”好的推荐“。这样一个周期下来，推荐效果应当有肉眼可见的改进。
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;推荐算法选择&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;各取所长，互相补充&lt;/li&gt;
&lt;li&gt;算法无好坏之分，只有是否合适&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://www.quora.com/Recommendation-Systems/What-developments-have-occurred-in-recommender-systems-after-the-Netflix-Prize/answer/Xavier-Amatriain?srid=z0Q5&amp;amp;share=1&quot;&gt;Recommendation Systems: What developments have occurred in recommender systems after the Netflix Prize?&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;在 Quora 上关于目前推荐系统研究总结，涵盖了推荐系统的多样性，基于上下文环境推荐，社交信息的引入，评分预测已经不是主流，LTR的应用会更符合推荐的初衷等&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Implicit feedback from usage has proven to be a better and more reliable way to capture user preference.&lt;/li&gt;
&lt;li&gt;Rating prediction is not the best formalization of the &amp;quot;recommender problem&amp;quot;. Other approaches, and in particular personalized Learning to Rank, are much more aligned with the idea of recommending the best item for a user.&lt;/li&gt;
&lt;li&gt;It is important to find ways to balance the trade-off between exploration and exploitation. Approaches such as Multi-Armed Bandit Algorithms offer an informed way to address this issue.&lt;/li&gt;
&lt;li&gt;Issues such as diversity, and novelty can be as important as relevance.&lt;/li&gt;
&lt;li&gt;It is important to address the presentation bias caused by users only being able to give feedback to those items previously decided where good for them.&lt;/li&gt;
&lt;li&gt;The recommendation problem is not only a two dimensional problem of users and items but rather a multi-dimensional problem that includes many contextual dimensions such as time of the day or day of the week. Algorithms such as Tensor Factorization or Factorization Machines come in very handy for this.&lt;/li&gt;
&lt;li&gt;Users decide to select items not only based on how good they think they are, but also based on the possible impact on their social network. Therefore, social connections can be a good source of data to add to the recommendation system.&lt;/li&gt;
&lt;li&gt;It is not good enough to design algorithms that select the best items for users, these items need to be presented with the right form of explanations for users to be attracted to them.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;参考文献&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;http://wenku.baidu.com/view/0607e780d0d233d4b14e699e.html&quot;&gt;数据挖掘技术在推荐系统的应用，陈运文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www2007.org/papers/paper570.pdf&quot;&gt;Google News Personalization: Scalable Online Collaborative Filtering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://ijcai13.org/files/tutorial_slides/td3.pdf&quot;&gt;Tutorial: Recommender Systems; Dietmar Jannach&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://ai.stanford.edu/%7Eronnyk/WEBKDD2000/papers/sarwar.pdf&quot;&gt;Application of Dimensionality Reduction in Recommender System -- A Case Study&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://vdisk.weibo.com/s/DaKXoKQC5TSH&quot;&gt;Up Next: Retrieval Methods for Large Scale Related Video Suggestion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://alex.smola.org/teaching/berkeley2012/slides/8_Recommender.pdf&quot;&gt;Alex-recommendation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.slideshare.net/scmyyan/large-scale-recommendation-in-ecommerce-qiang-yan&quot;&gt;Large scale recommendation in e-commerce -- qiang yan&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.slideshare.net/xamat/recommender-systems-machine-learning-summer-school-2014-cmu&quot;&gt;Recommender System slices. MLSS14&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://videolectures.net/kdd2014_amatriain_mobasher_recommender_problem/&quot;&gt;Recommender System video. MLSS14&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.kdd.org/kdd2014/tutorials/KDD-%20The%20RecommenderProblemRevisited-Part2.pdf&quot;&gt;Context Aware Recommendation. Bamshad Mobasher&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.kdd.org/kdd2014/tutorials/KDD%20-%20The%20Recommender%20Problem%20Revisited.pdf&quot;&gt;KDD - The Recommender Problem Revisited&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;其他参考文献&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;http://blog.sina.com.cn/s/blog_804abfa70101btrv.html&quot;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;这个资料分享主要分享的都是非学术的Paper，都来自商业公司，Google, YouTube, Amazon, LinkedIn等等。
我个人非常喜欢这些文章，基本上，这些文章描述的都是在系统中的实际能工作的东西。&lt;/p&gt;

&lt;p&gt;这个是Google的一篇论文http://t.cn/zl0zxPZ这个里面有很多有意思的想法。
推荐的结果是三个算法的融合，即MinHash, PLSI, covisitation.
融合的方式是分数线性加权
一个主要的思想是“online”的进行更新，所以这个地方一定要减少规模，索引使用了User Clustering的算法，包括Min Hash和PLSI。
在新数据来的时候，关键是不要去更新User Cluster，而是直接更新所属的Cluster对于URL的点击数据
对于新用户，使用covisitation的方法进行推荐&lt;/p&gt;

&lt;p&gt;这个是上一篇Paper的进阶paper。 http://t.cn/zl0zqDO
这篇Paper在上一篇的基础上增加了一些内容，主要包括Topic部分的内容，Google News是有Topic信息的。
这篇Paper通过用户喜欢的Topic这个信息以及Topic Trend这个信息一起进行分析。
热门的topic会被更多的展现给用户，其中用户只会看到他喜欢的Topic
这个方法和上面的方法相比，可能对于解决热门News的问题，有更大的帮助&lt;/p&gt;

&lt;p&gt;这个是Youtube的文章 vdisk.weibo.com/s/fcbuu
这篇Paper的的方法更直观，它只使用了covisitation的信息，但是对于covisitation的方法做了N次扩展，即找一个Seed的多次邻居。
在这个的基础上，做了一些后处理的工作，尤其是Diversity的工作&lt;/p&gt;

&lt;p&gt;这个是对于Amazon商品推荐算法的一个Paper的翻译版
http://blog.sina.com.cn/s/blog&lt;em&gt;586631940100pduh.html
  这个Paper比较老了，但是是item-Based推荐的经典文章了。
这个是IBM的两位同学对于推荐的一个综述，属于入门级的，看看也不错。
  http://www.ibm.com/developerworks/cn/web/1103&lt;/em&gt;zhaoct_recommstudy1/index.html
这个比较有营养，是高级货，是LinkedIn的兄弟们在KDD2012上发布的，有用！进阶以后值得看看，尤其是搞真系统的。http://t.cn/zl0ZTN1
这个更是高级货了，Recommendations as a Conversation with the User
这个的角度更多的是推荐系统的HCI设计，前面是一堆哲学，看不懂可以跳过，后面的例子还是比较给力的。有几个数字很给力：
Amazon: 35% of sales result from recommendations
75% of Netflix views result from recommendations&lt;/p&gt;
</description>
        <pubDate>Sat, 03 Jan 2015 00:00:00 +0800</pubDate>
        <link>http://yourdomain.com/2015/01/03/recommendation_algorithms.html</link>
        <guid isPermaLink="true">http://yourdomain.com/2015/01/03/recommendation_algorithms.html</guid>
        
        <category>machine learning</category>
        
        
      </item>
    
      <item>
        <title>decision_tree</title>
        <description>&lt;h2&gt;决策树与GBDT&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://cvchina.net/post/107.html&quot;&gt;决策树模型组合之随机森林与GBDT&lt;/a&gt;
格灵深瞳   2014-10-03 11:10
模型组合与决策树相关的算法比较多，这些算法最终的结果是生成N棵树，这样可以大大的减少单决策树带来的毛病，有点类似于三个臭皮匠等于一个诸葛亮的做法，虽然这几百棵决策树中的每一棵都很简单，但是他们组合起来确是很强大。【决策树模型组合之随机森林与GBDT】http://t.cn/Rh1uZ1Y
好东西传送门 转发于2014-10-03 12:37
经典文章 Greedy function approximation : A Gradient Boosting Machine http://t.cn/Rh1dW44 并行实现推荐 @陈天奇怪 的xgboost，实际例子见@phunter_lau 最近的文章 http://t.cn/RhKAWac 更多GBDT http://t.cn/Rh1dW4y&lt;/p&gt;
</description>
        <pubDate>Sat, 03 Jan 2015 00:00:00 +0800</pubDate>
        <link>http://yourdomain.com/2015/01/03/decision_tree.html</link>
        <guid isPermaLink="true">http://yourdomain.com/2015/01/03/decision_tree.html</guid>
        
        
      </item>
    
      <item>
        <title>topic model</title>
        <description>&lt;h2&gt;topic model 思考&lt;/h2&gt;

&lt;p&gt;topic model通常指的是lda, plsa等模型，但从广义上将，类目也是一种topic。本文主要把以前接触到的plsa, lda等知识回顾一下，顺便把类目型的topic model也做一个总结。&lt;/p&gt;

&lt;h3&gt;常见topic model&lt;/h3&gt;

&lt;p&gt;plsa&lt;/p&gt;

&lt;p&gt;LDA:
采样算法：collapsed gibbs sampler，SparseLDA，&lt;a href=&quot;http://www.sravi.org/pubs/fastlda-kdd2014.pdf&quot;&gt;AliasLDA&lt;/a&gt;，Metropolis-Hastings sampler
&lt;a href=&quot;www.umiacs.umd.edu/%7Eresnik/pubs/LAMP-TR-153.pdf&quot;&gt;gibbs sampling for the uninitiated&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;data-parallelism(splitting documents over machines) versus model-parallelism (splitting the word-topic distributions over machines).&lt;/p&gt;

&lt;p&gt;data-parallelism:YahooLDA, Scaling distributed machine learning with the parameter server&lt;/p&gt;

&lt;p&gt;model-parallelism:PLDA+, peacock&lt;/p&gt;

&lt;p&gt;lightLDA:LightLDA adopts a different data-and-model-parallel strategy to maximize memory and CPU efficiency: we slice the word-topic distributions (the LDA model) in a structure-aware modelparallel manner [9, 24], and we fix blocks of documents to workers while transferring needed model parameters to them via a bounded-asynchronous data-parallel scheme [8].&lt;/p&gt;

&lt;p&gt;Metropolis-Hastings sampler&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;接下去的重点就是把最新的采样算法和怎么样做并行化好好研究一下，并写成文章&lt;/strong&gt;&lt;/p&gt;

&lt;h3&gt;topic model on twitter 阅读笔记&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;这篇论文其实和前段时间做过的广告分类有些相似，区别在于难度要大一些，考虑的点要更丰富一些。文中提到的很多点我以前都或多或少实践过，接触过，不过没有这么系统的总结出来，所以把这篇论文细细研读一遍，对自己的知识回顾与整理也是有帮助的。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;文中说到topic model，为啥没有直接用lda等模型。我自己总结，主要原因是lda模型可控性可解释性相对比较差：对于每个topic，不能用很明确的语义归纳出这个topic在讲什么；重新训练一遍lda模型，每个topic id所对应的语义可能发生了变化；有些topic的准确性比较好，有些比较差，而对于比较差的topic，没有特别好的针对性的方法去优化它；但lda等模型也有好处，无监督训练。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;在我们以往的运用中，lda模型比较适合用于做某些机器学习任务的特征，譬如pctr，relevance等，而不适合作为一种独立的方法去解决某种特定的问题，例如触发，分类。Blei是这样评价lda的：it can easily be used as a module in more complicated models for more complicated goals.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;有监督的topic model中，第一个任务就是确定topic taxonomy。通常情况里，taxonomy都是由人工创建的。人工创建的最大问题是，知识面有限，可能不能覆盖住所有情形。文中有提到一种方法：ODP和Freebase。我的理解是，可以先用某种无监督的聚类方法，将待分类的文本划分到某些clusters，然后人工review这些clusters，切分或者合并cluster，提炼topic name，再然后根据知识体系，建立层级的taxonomy。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;文本分类&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;过滤聊天类文本，也就是筛选出un-labeled data。文中有提到一种方法:&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;训练数据获取。这其实是一种常见的方法，记得以前在afs做分类器时，也有一种方法，先对每一个类人工筛选一些特征词，然后根据这些特征词对亿级文本网页分类，再然后对每一个明确属于该类的网页提取更多的特征词，加入原有的特征词词表，再去做分类；中间再辅以一定的人工校验，这种方法做下来，效果还是蛮不错的，更关键的是，如果发现那个类有badcase，可以人工根据badcase调整某个特征词的权重，简单粗暴又有效。还有一个任务，term weighting工作里，我们是利用logistic regression的方法来做的，其中面临的最主要任务就是训练数据的获取，这里我们采用了三种方法。
twitter利用了三种方法，user-level priors（发布tweet的用户属于的领域），entity-level priors（话题，类似于微博中的#***#），url-level priors（tweet中的url）。通过上述基于规则的方法获取到的训练数据是有较大噪声的，这时可以利用co-learning。
获取到正例样本后，还需要负例样本，按照常见的方法，从非正例样本里随机抽取作为负例的方法，效果并不是好，文中用到了pu-learning去获取高质量的负例样本。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/training_data_acquisition.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;特征提取。为了考虑实时性，jubjub并没有用到很复杂的特征，主要用到了两种特征：binary hashed byte 4gram，hashed unigram frequency。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;model pretraing。
在正则化上，文中采用了ElasticNet regularization，也就是L1和L2的组合。模型上，选择了one-be-all LR，而不是Multinomial LR，主要考虑是：并行训练multiple topics model更复杂；不能重新训练 a subset of topics。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;relation regularization。因为是层级分类器，需要考虑label之间的relation。譬如Label expansion。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;model calibration。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;quality evaluation。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;model fine-tuning。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/fine_tuning.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/fine_tuning_formula.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 01 Jan 2015 00:00:00 +0800</pubDate>
        <link>http://yourdomain.com/2015/01/01/topic-model.html</link>
        <guid isPermaLink="true">http://yourdomain.com/2015/01/01/topic-model.html</guid>
        
        <category>machine learning</category>
        
        <category>topic model</category>
        
        
      </item>
    
      <item>
        <title>matrix similarity</title>
        <description>&lt;h2&gt;矩阵相似度计算&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;在机器学习任务中，矩阵是一个很重要的表述形式。文档与词，用户与其购买的商品，用户与好友关系等都可以描述成一个矩阵。为了描述方便，下文中矩阵都以U*I代替，U代表user，I代表item，矩阵维数为m*n。
对这个矩阵，一个最基础的任务就是找到最相似的用户或最相似的文档，也就是&lt;a href=&quot;http://zh.wikipedia.org/wiki/%E6%9C%80%E8%BF%91%E9%84%B0%E5%B1%85%E6%B3%95&quot;&gt;k最近邻问题&lt;/a&gt;(数据挖掘十大经典算法之一)。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;相似度计算方法&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;相似度计算方法：cosine距离，jaccard距离，bm25模型，proximity模型。具体请参考&lt;a href=&quot;http://www.cnblogs.com/heaad/archive/2011/03/08/1977733.html&quot;&gt;机器学习中的相似性度量&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;降维方法&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;计算任意两个user之间的相似度，需要O(m*n)的复杂度。当n很大的时候，首先想到的办法是能否降维，将原矩阵变为m*k维(k&amp;lt;&amp;lt;n)。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;降维的方法有：svd，nmf，lsa，lda等。将一个大矩阵分解为两个小矩阵(m*n分解为两个矩阵m*k，k*n)，或者分解为三个小矩阵(m*n分解为两个矩阵m*k，k*k，k*n)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;minhash+lsh&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;除此之外，还有一种降维+局部敏感hash的算法。
也就是minhash + lsh。参考&lt;a href=&quot;http://en.wikipedia.org/wiki/MinHash&quot;&gt;MinHash wiki&lt;/a&gt;，&lt;a href=&quot;http://blog.csdn.net/sunlylorn/article/details/7835411&quot;&gt;文本去重之MinHash算法&lt;/a&gt;，&lt;a href=&quot;http://www.cnblogs.com/bourneli/archive/2013/04/04/2999767.html&quot;&gt;利用Minhash和LSH寻找相似的集合&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;我们可以根据MinHash来计算两个集合的相似度了。一般有两种方法：&lt;/li&gt;
&lt;li&gt;第一种：使用多个hash函数。
为了计算集合A、B具有最小哈希值的概率，我们可以选择一定数量的hash函数，比如K个。然后用这K个hash函数分别对集合A、B求哈希值，对
每个集合都得到K个最小值。比如Min(A)k={a1,a2,...,ak}，Min(B)k={b1,b2,...,bk}。
那么，集合A、B的相似度为|Min(A)k ∩ Min(B)k| / |Min(A)k  ∪  Min(B)k|，及Min(A)k和Min(B)k中相同元素个数与总的元素个数的比例。&lt;/li&gt;
&lt;li&gt;第二种：使用单个hash函数。
第一种方法有一个很明显的缺陷，那就是计算复杂度高。使用单个hash函数是怎么解决这个问题的呢？请看：
前面我们定义过 hmin(S)为集合S中具有最小哈希值的一个元素，那么我们也可以定义hmink(S)为集合S中具有最小哈希值的K个元素。这样一来，
我们就只需要对每个集合求一次哈希，然后取最小的K个元素。计算两个集合A、B的相似度，就是集合A中最小的K个元素与集合B中最小的K个元素
的交集个数与并集个数的比例。&lt;/li&gt;
&lt;li&gt;对于每个user，利用minhash计算后，则将其从n维降维至K维向量。然后就该LSH出场了。&lt;/li&gt;
&lt;li&gt;LSH:local sensitive hash。将上面K维向量划分到n个桶，每个桶有K/n维。两个user，只要有一个桶的元素是一样的，那么就认为他们是相似候选。这里有一个公式来衡量n的选值。请参考论文&lt;a href=&quot;http://infolab.stanford.edu/%7Eullman/mmds/ch3.pdf&quot;&gt;find similar items&lt;/a&gt;，&lt;a href=&quot;http://1.guzili.sinaapp.com/?p=190#more-190&quot;&gt;局部敏感哈希LSH科普&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/lsh.png&quot; alt=&quot;lsh&quot;&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;直接利用map-reduce&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;另外一种方法是不降维，直接通过map-reduce直接计算user之间的相似性。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;假设矩阵U*I，计算两两U之间的相关性，时间复杂度是O(N^3)&lt;/li&gt;
&lt;li&gt;但是我们可以换个思路，将矩阵转置，以item为key。第一轮map-reduce过程，将U*I矩阵转置为I*U矩阵，输出每个item下，与该item有关联的所有user list。第二轮map，将同一个item下user两两组合成pair后输出，第二轮reduce，累加相同user pair的weight，得到任意两个user之间的相似度。&lt;/li&gt;
&lt;li&gt;具体请参考链接&lt;a href=&quot;http://wbj0110.iteye.com/blog/2043700&quot;&gt;大规模矩阵相似度计算&lt;/a&gt;。解决该问题就由两个Map-Reduce过程完成。第一个MR过程称为倒排索引，对每个文档，对其中的每个词语，以词语为键，文档标号与词语在该文档中的权重为值输出，这样，我们就得到如(F4,[(U1,0.1),(U2,0.9),(U7,0.5)])格式的输出。第二个MR过程计算文本相似度，以上一个MR过程的输出为输入，在Map过程中以文本对为键，以权重值的乘积为输出，比如上面的F4输出，map后变为[((U1,U2),0.09),((U1,U7),0.05),((U2,U7),0.45)]，这样，就得到了在所有的在两个文本中共同出现的词语针对该两个文本的权重乘积；然后在reduce过程中将相同键的值相加，就得到了所有的二元文本对的文本相似度。&lt;/li&gt;
&lt;li&gt;文中后面还讲了一些优化手段。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/matrix_similairity.png&quot; alt=&quot;matrix_similairity&quot;&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;矩阵的乘法&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;额外再讲一点内容，矩阵的乘法,一个m*k的矩阵A乘上一个k*n的矩阵B，结果是一个m*n的矩阵C。有两种分解方法：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;其一，把A矩阵按行分，把B矩阵按列分的观点来看矩阵乘法。C矩阵的一个子矩阵块可以看做是A对应多行和B对应多列的矩阵相乘得到的结果；&lt;/li&gt;
&lt;li&gt;其二，把矩阵A按列分块，矩阵B按行分块，A乘B可以等价于A的分块子矩阵乘上B中对应的分块子矩阵的加和。最特殊的情况是把A按列分为k个列向量，B按行分为k个行向量，然后对应的列向量于行向量相乘，得到k个矩阵，他们的和就是A和B的乘积。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;矩阵乘法的并行方法&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt; 第一种方法，按照A行B列的分解，我们将C矩阵分成m*n个子矩阵，每个子矩阵由一个进程来计算。不考虑进程通讯问题，时间减少到单机版本的m*n分之一。&lt;/li&gt;
&lt;li&gt; 第二种方法，按照A列B行的分解，把C矩阵分解成k个同样大小的矩阵之和，分发到k个进程来计算，时间减少到单机版本的k分之一。&lt;/li&gt;
&lt;li&gt; 哪一种方法更快，取决于k和m*n哪个更大。不过方法二要通信的数据量要明显大于方法一。&lt;/li&gt;
&lt;li&gt; 哪一种方法需要存储更少，取决于(k+1)mn和(m+n)k的大小。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;更多矩阵乘法，请参考文章&lt;a href=&quot;http://en.wikipedia.org/wiki/Cannon&amp;#x27;s_algorithm&quot;&gt;Cannon算法&lt;/a&gt;，&lt;a href=&quot;http://www.netlib.org/lapack/lawnspdf/lawn96.pdf&quot;&gt;Scalable Universal Matrix Multiplication Algorithm&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Thu, 01 Jan 2015 00:00:00 +0800</pubDate>
        <link>http://yourdomain.com/2015/01/01/matrix-similarity.html</link>
        <guid isPermaLink="true">http://yourdomain.com/2015/01/01/matrix-similarity.html</guid>
        
        
      </item>
    
  </channel>
</rss>
