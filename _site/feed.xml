<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>100的技术博客</title>
    <description>机器学习，自然语言处理，计算广告学，工作与生活，总结与温习
</description>
    <link>http://yourdomain.com/</link>
    <atom:link href="http://yourdomain.com/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Wed, 25 Feb 2015 17:50:35 +0800</pubDate>
    <lastBuildDate>Wed, 25 Feb 2015 17:50:35 +0800</lastBuildDate>
    <generator>Jekyll v2.5.3</generator>
    
      <item>
        <title>深度学习相关笔记</title>
        <description>&lt;h2 id=&quot;section&quot;&gt;神经网络基础&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://hahack.com/reading/ann1/&quot;&gt;漫谈ANN(1)：M-P模型&lt;/a&gt;，&lt;a href=&quot;http://hahack.com/reading/ann2/&quot;&gt;漫谈ANN(2)：BP神经网络&lt;/a&gt;&lt;/p&gt;

    &lt;p&gt;M-P模型:1943年心理学家W.McCulloch和数学家W.Pitts合作提出了这个模型，所以取了他们两个人的名字（McCulloch-Pitts）&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/mp_model.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;Perceptron: 在1958年,美国心理学家Frank Rosenblatt,基于M-P模型的结构,提出一种具有单层计算单元的神经网络,称为感知器(Perceptron)。&lt;/p&gt;

    &lt;p&gt;单层感知器不能解决非线性问题，譬如”异或”。Kolmogorov理论指出：双隐层感知器就足以解决任何复杂的分类问题。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://deeplearning.stanford.edu/wiki/index.php/神经网络&quot;&gt;UFLDL-神经网络&lt;/a&gt;, &lt;a href=&quot;http://deeplearning.stanford.edu/wiki/index.php/反向传导算法&quot;&gt;反向传播算法-详解&lt;/a&gt;&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/106334/cost-function-of-neural-network-is-non-convex&quot;&gt;为什么多层神经网络是非凸函数&lt;/a&gt; 这里有解释，简单点说，如果交换某一层某两个nodes和weights以及相应连接的nodes，将得到a different set of parameters，但是cost function是一样的，所以是非凸的。&lt;/p&gt;

    &lt;p&gt;神经网络参数必须随机初始化，而不是全部置为0。如果所有参数都用相同的值作为初始值，那么所有隐藏层单元最终会得到与输入值有关的、相同的函数。随机初始化的目的是使对称失效。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://blog.sina.com.cn/s/blog_71329a960102v1eo.html&quot;&gt;神经网络历史&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-1&quot;&gt;深度学习综述&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://deeplearning.net/tutorial/deeplearning.pdf&quot;&gt;Deep Learning Tutorial. LISA lab, University of Montreal&lt;/a&gt;  &lt;a href=&quot;http://deeplearning.net/software_links/&quot;&gt;software&lt;/a&gt;
基于Theano的深度学习教程，内容很新，包括了多层感知器，卷积神经网络，auto encoder，RBM，Deep Belief Networks，Monte-carlo sampling，循环神经网络，LSTM，RNN-RBM，Miscellaneous等，值得学习。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://quantombone.blogspot.com/2015/01/from-feature-descriptors-to-deep.html&quot;&gt;From feature descriptors to deep learning&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://simonwinder.com/2015/01/what-is-deep-learning/&quot;&gt;What is Deep Learning?&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://markus.com/deep-learning-101/&quot;&gt;Deep Learning 101&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.toptal.com/machine-learning/an-introduction-to-deep-learning-from-perceptrons-to-deep-networks&quot;&gt;A Deep Learning Tutorial: From Perceptrons to Deep Networks&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.iro.umontreal.ca/~bengioy/dlbook/&quot;&gt;deep learning book. by Yoshua Bengio&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.cs.toronto.edu/~tijmen/csc321/&quot;&gt;Introduction to Neural Networks and Machine Learning by hinton&lt;/a&gt; &lt;a href=&quot;http://www.cs.toronto.edu/~tijmen/csc321/lecture_notes.shtml&quot;&gt;Lecture notes&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://weibo.com/p/1001603799166017998138&quot;&gt;谷歌科学家、Hinton亲传弟子Ilya Sutskever的深度学习综述及实际建议&lt;/a&gt; 比较喜欢其中关于tricks的建议：包括data, preprocessing, minibatches, gradient normalization, learning rate, weight initialization, data augmentation, dropout和ensemble。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.ee.ucl.ac.uk/sahd2014/resources/LeCun.pdf&quot;&gt;LeCun：The Unreasonable Effectiveness of Deep Learning&lt;/a&gt; LeCun做的300+页的深度学习slides，太棒了！毋需多做介绍，单看标题、作者应该就能作出判断——看看看，必须的！&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://colah.github.io/posts/2015-01-Visualizing-Representations/&quot;&gt;Visualizing Representations: Deep Learning and Human Beings&lt;/a&gt; 利用深度学习和维数约减，可以对整个Wikipedia进行可视化，文中结合Wikipedia训练得出的例子，全面介绍了深度学习、词向量、段落向量、翻译模型以及深度学习可视化方面的知识，理论结合实践，实属不可多得的好文。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://blog.csdn.net/zouxy09/article/details/8775360&quot;&gt;深度学习-笔记整理系列1-8&lt;/a&gt;，&lt;a href=&quot;http://blog.csdn.net/zouxy09/article/details/9993371&quot;&gt;link2&lt;/a&gt; 使用自下向上非监督学习（就是从底层开始，一层一层的往顶层训练）；自顶向下的监督学习（就是通过带标签的数据去训练，误差自顶向下传输，对网络进行微调）。还介绍了Autoencoder，SparseCoding，Restricted Boltzmann Machine，Deep BeliefNetworks，CNN等模型。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://t.cn/RZ8Sqe6&quot;&gt;博文“A Brief Overview of Deep Learning”&lt;/a&gt; 有见解有福利。一些技术总结得不错，例如Practice Advice，有很多干货，谁用谁知道…… 文后还有Bengio的点评及与网友的互动讨论。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.bammf.org&quot;&gt;Bay Area Multimedia Forum&lt;/a&gt; 邓力，贾扬清，Ronan Collobert, Richard Socher 讲用深度学习处理语音、文本和图像。有slides有视频。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;&quot;&gt;Neural network with numpy&lt;/a&gt; Python下(只)用numpy写神经网络，不错的开始 &lt;a href=&quot;https://github.com/FlorianMuellerklein/Machine-Learning/blob/master/BackPropagationNN.py&quot;&gt;github code&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://techjaw.com/2015/02/21/googles-large-scale-deep-neural-networks-project-greg-corrado/&quot;&gt;Google’s Large Scale Deep Neural Networks Project, Greg Corrado&lt;/a&gt; Google的大规模分布式DNN介绍 &lt;a href=&quot;http://pan.baidu.com/s/1kTl76AV&quot;&gt;slide&lt;/a&gt; &lt;a href=&quot;http://pan.baidu.com/s/1qWmJrSo&quot;&gt;视频&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;deeplearing-in-nlp&quot;&gt;Deeplearing in NLP综述&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.jobbole.com/77709/&quot;&gt;深度学习、自然语言处理和表征方法&lt;/a&gt; &lt;a href=&quot;https://github.com/colah/NLP-RNNs-Representations-Post/blob/master/index.md&quot;&gt;English version&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;单隐层神经网络有一个普适性（universality）：给予足够的隐结点，它可以估算任何函数。普适性的真正意义是：一个网络能适应任何你给它的训练数据。这并不代表插入新的数据点的时候它能表现地很理想。&lt;/li&gt;
      &lt;li&gt;在深度学习工具箱里，把从任务A中学到的好表征方法用在任务B上是一个很主要的技巧。根据细节不同，这个普遍的技巧的名称也不同，如：预训练（pretraining），迁移学习(transfer learning)，多任务学习(multi-task learning)等。这种方法的好处之一是可以从多种不同数据中学习特征表示。&lt;/li&gt;
      &lt;li&gt;共享嵌入是一个非常让人兴奋的研究领域，它暗示着为何深度学习中这个注重表征方法的角度是如此的引人入胜。它目前被应用在机器翻译，ImageCaptioning。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;imagenet-classification&quot;&gt;ImageNet classification&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://arxiv.org/pdf/1502.03167v1.pdf&quot;&gt;Googles breakthrough paper shows 10*faster neural nets, and beats a human&lt;/a&gt; Google的最新论文，用”batch normalization”在ImageNet上得到4.82%( top-5 error)，训练速度也大大加快。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://arxiv.org/abs/1502.01852&quot;&gt;Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification&lt;/a&gt; 微软所创建的基于深度卷积神经网络系统首次在ImageNet图像分类上超越人类，实现4.94% top-5 test error。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;image-caption-generation&quot;&gt;Image Caption Generation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://arxiv.org/abs/1502.03044&quot;&gt;Show, Attend and Tell: Neural Image Caption Generation with Visual Attention&lt;/a&gt; 基于视觉焦点用LSTM自动生成图像内容描述。
来自Yoshua Bengio教授团队（22 pages，8页正文+n多附图），文中报道的结果比之前Microsoft、Google的结果更好。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://pdollar.wordpress.com/2015/01/21/image-captioning/&quot;&gt;Image Captioning&lt;/a&gt;  来自于微软的一篇博文，介绍了image caption的最近进展。图像标题生成小综述，文中引用了另一篇进展综述性文章《Rapid Progress in Automatic Image Captioning》http://t.cn/Rzzk2H3 ，列举出最有影响和代表性的进展和成果(论文)，并从数据集、验证(评价)和下一步怎么走三方面进行了讨论，观点很有代表性&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;http://t.cn/RZdBZEe&quot;&gt;Top Microsoft Machine Learning Posts of 2014&lt;/a&gt; 微软14年最佳博文：图像自动描述生成的飞速进展、机器学习欢乐多、Azule ML为用户带来变革、机器学习与文本分析、微软机器学习20年、Vowpal Wabbit快速学习、什么是机器学习、NIPS14机器学习趋势、机器学习与机器视觉等&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;rnn-lstm&quot;&gt;RNN&amp;amp; LSTM&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://xxx.tau.ac.il/abs/1502.02367&quot;&gt;Gated Feedback Recurrent Neural Networks&lt;/a&gt;  又有人设计新的RNN了，这回Cho和Bengio都在．这回介绍的GF-RNN说是能比以前Deep RNN的都好。明摆着说LSTM嘛。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://t.cn/Rww4fbV&quot;&gt;论文 Scaling Recurrent Neural Network Language Models》(2015) W Williams, N Prasad, D Mrva&lt;/a&gt; 讨论如何使用GPU训练大型RNN，以及#RNN##语言模型#(RNNLM)在进行扩展时模型大小、训练集规模和运算开销方面的问题；使用维基和新闻语料训练的大规模RNNLM效果明显。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://t.cn/RZNfoLt&quot;&gt;Passage：用RNN做文本分析的python库&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://nikhilbuduma.com/2015/01/11/a-deep-dive-into-recurrent-neural-networks/&quot;&gt;深入讨论RNN&lt;/a&gt;  &lt;a href=&quot;http://www.csdn.net/article/2015-01-28/2823747&quot;&gt;译文&lt;/a&gt;非常好的讨论递归神经网络的文章，覆盖了RNN的概念、原理、训练及优化等各个方面内容，强烈推荐！本文作者Nikhil Buduma，也是《Deep Learning in a Nutshell》的作者。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;深度RNN/LSTM用于结构化学习 0)序列标注&lt;a href=&quot;http://t.cn/RZZs9Io&quot;&gt;Connectionist Temporal Classification ICML06&lt;/a&gt; 1)机器翻译&lt;a href=&quot;http://t.cn/RZZs9Jk&quot;&gt;Sequence to Sequence NIPS14&lt;/a&gt; 2)成分句法&lt;a href=&quot;http://t.cn/RZZs9Ia&quot;&gt;GRAMMAR AS FOREIGN LANGUAGE&lt;/a&gt; 再次用到窃取果实distilling&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;[视频]《General Sequence Learning using Recurrent Neural Networks》http://t.cn/RwiEI9d Alec Radford讲的用RNN做文本序列分析(学习) 云:http://t.cn/RwinOCb Alec Radford的Passage:http://weibo.com/1402400261/BFVLkxtfw&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://people.idsia.ch/~juergen/lstm/index.htm&quot;&gt;幻灯 Long Short-Term Memory: Tutorial on LSTM Recurrent Networks&lt;/a&gt;  J. Schmidhuber的LSTM递归网络教程&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;[幻灯] 《Theano and LSTM for Sentiment Analysis》http://t.cn/RwofVaF Next.ML 2015上用Theano和LSTM做情感分析的报告幻灯和练习 GitHub:http://t.cn/RwofqDC 云:http://t.cn/Rwofo4i&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;最近很火的两篇文章，Neural Turing Machines和Learning to Execute，都讲到用LSTM RNN做sequence copy，来测试网络保持长时记忆的能力。sequence copy到底怎么定义为一个supervised learning任务呢？和机器翻译一样吗？&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;cnn&quot;&gt;CNN&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://dataunion.org/?p=5395&quot;&gt;深度学习：CNN的反向求导及练习&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Character-based CNN。&lt;a href=&quot;http://arxiv.org/abs/1502.01710&quot;&gt;论文 Text Understanding from Scratch 2015 Xiang Zhang, Yann LeCun&lt;/a&gt;  深度学习在NLP领域最新进展！使用temporal ConvNets对大规模文本语料进行学习，在本体分类、情感分析、文本分类任务中取得了“astonishing performance”，不依赖任何语言知识，中英文均适用。必读！&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://arxiv.org/abs/1404.2188&quot;&gt;论文 A Convolutional Neural Network for Modelling Sentences. Nal Kalchbrenner等&lt;/a&gt; 用动态卷积神经网络(DCNN)对句子进行语义建模，该方法不依赖解析树也不受语种限制，效果也很明显，推荐学习。其项目主页上http://t.cn/RZd9HOE 提供了源码(Matlab) 云:http://t.cn/RZdCx1o&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;word2vec&quot;&gt;Word2vec&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;[问答]《What are some interesting Word2Vec results?》http://t.cn/RZrOfYp Quora上的主题，讨论Word2Vec的有趣应用，Omer Levy提到了他在CoNLL2014最佳论文里的分析结果和新方法（稍后单独推荐），Daniel Hammack给出了找特异词的小应用并提供了(Python)代码http://t.cn/zQgLQ20&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Ensemble of Generative and Discriminative Techniques for Sentiment Analysis of Movie Reviews。&lt;a href=&quot;https://github.com/mesnilgr/iclr15&quot;&gt;code，include sentence2vec&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;隐含主题模型LDA的学习过程可为文档每个词分配隐含主题，我组本科生刘扬同学利用LDA为词汇提供的补充信息，提出topical word embeddings，在词汇相似度计算和文本分类上得到一些有趣的结果。&lt;a href=&quot;https://github.com/largelymfs/topical_word_embeddings&quot;&gt;github code&lt;/a&gt;，&lt;a href=&quot;http://nlp.csai.tsinghua.edu.cn/~lzy/publications/aaai2015_twe.pdf&quot;&gt;论文Topical Word Embeddings&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/idio/wiki2vec&quot;&gt;Wiki2Vec: Generating Vectors for DBpedia Entities via Word2Vec and Wikipedia Dumps&lt;/a&gt; 从维基百科Dumps生成Word2Vec向量的工具，包括词向量和主题向量&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://andyljones.tumblr.com/post/111299309808/why-word2vec-works&quot;&gt;Why word2vec works&lt;/a&gt; word2vec的工作原理&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www-personal.umich.edu/~ronxin/pdf/w2vexp.pdf&quot;&gt;word2vec Parameter Learning Explained&lt;/a&gt; word2vec梯度推导详解&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Mon, 23 Feb 2015 00:00:00 +0800</pubDate>
        <link>http://yourdomain.com/2015/02/23/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E7%AC%94%E8%AE%B0.html</link>
        <guid isPermaLink="true">http://yourdomain.com/2015/02/23/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E7%AC%94%E8%AE%B0.html</guid>
        
        
      </item>
    
      <item>
        <title>文献综合阅读</title>
        <description>&lt;h2 id=&quot;section&quot;&gt;图像视觉&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://blog.csdn.net/morewindows/article/details/8225783&quot;&gt;OpenCV入门指南&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://quantombone.blogspot.com/2015/01/from-feature-descriptors-to-deep.html&quot;&gt;From feature descriptors to deep learning: 20 years of computer vision&lt;/a&gt; 从特征描述子到深度学习——机器视觉20年回顾。通俗易懂，回顾了很多特征描述子的内容，也介绍了很多计算机视觉、机器学习特别是深度学习方面的大牛。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://t.cn/RZkbwoz&quot;&gt;图像处理中的全局优化技术&lt;/a&gt; 最近打算好好学习一下几种图像处理和计算机视觉中常用的 global optimization (或 energy minimization) 方法，这里总结一下学习心得。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://blog.csdn.net/carson2005/article/details/9502053&quot;&gt;Retinex算法详解&lt;/a&gt; - 计算机视觉小菜鸟的专栏 - 博客频道 - CSDN.NET&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;本团队雕琢多年的人脸检测库现以MIT协议发布 &lt;a href=&quot;https://github.com/ShiqiYu/libfacedetection&quot;&gt;github code&lt;/a&gt; 供商业和非商业无限制使用,包含正面和多视角人脸检测两个算法.优点:速度快(OpenCV haar+adaboost的2-3倍), 准确度高 (FDDB非公开类评测排名第二），能估计人脸角度. 例子看下图. 希望能帮助到有需要的个人和公司。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.guokr.com/article/439945/&quot;&gt;计算机视觉：让冰冷的机器看懂多彩的世界&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-1&quot;&gt;高频交易&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://queue.acm.org/detail.cfm?id=2534976&quot;&gt;文章 Online Algorithms in High-frequency Trading - The challenges faced by competing HFT algorithms》2013&lt;/a&gt; 介绍高频交易(HFT)中的在线学习算法，重点解决流动性估计、波动性估计和线性回归问题，HFT算法简单了解可参考&lt;a href=&quot;http://www.zhihu.com/question/23667442&quot;&gt;知乎主题-高频交易都有哪些著名的算法&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-2&quot;&gt;博客推荐&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;《Frequently updated Machine Learning blogs》http://t.cn/RwbHZpy 活跃机器学习博客推荐，真有点怀念Google Reader呢&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;《面试经验分享之机器学习、大数据问题》如今，好多机器学习、数据挖掘的知识都逐渐成为常识，要想在竞争中脱颖而出，就必须做到：保持学习热情，关心热点，深入学习，会用，也要理解，在实战中历练总结等等。http://t.cn/RzMtL3j（来自： Blog of 太极雪 ）&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://chuansong.me/n/306480&quot;&gt;FLAGBR 面经+offer&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-3&quot;&gt;其他推荐资料&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://breezedeus.github.io/2015/01/31/breezedeus-review-for-year-2014-tech.html&quot;&gt;世纪佳缘用户推荐系统的发展历史&lt;/a&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;“总结、温习，这两点让人成长。而不是你走得有多快！”&lt;/li&gt;
      &lt;li&gt;天真的算法年：item-based kNN。推荐以前看过的item的相似item。可逆（Reciprocal）推荐算法，是什么东西？&lt;a href=&quot;http://search.aol.com/aol/search?s_it=topsearchbox.search&amp;amp;v_t=opensearch&amp;amp;q=Reciprocal+recommendation&quot;&gt;Reciprocal recommendation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;技术为产品服务，而不是直接面向用户；数据质量是地基，保证好的质量很不容易；如何制定正确的优化指标真的很难；业务理解 &amp;gt; 工程实现；数据 &amp;gt; 系统 &amp;gt; 算法；快速试错；&lt;/li&gt;
      &lt;li&gt;Dirichlet Process 和 Dirichlet Process Mixture&lt;/li&gt;
      &lt;li&gt;Alternating Direction Method of Multipliers(ADMM)&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://breezedeus.github.io/2014/11/19/breezedeus-feature-mining-gbdt.html&quot;&gt;利用GBDT模型构造新特征&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://breezedeus.github.io/2014/11/20/breezedeus-feature-hashing.html&quot;&gt;特征哈希（Feature Hashing）&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;不平衡数据的抽样方法。参考文献：William Fithian, Trevor Hastie, Local Case-Control Sampling Efficient Subsampling in Imbalanced Data Sets, 2014.&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://www.douban.com/note/484853135/&quot;&gt;世纪佳缘推荐系统之我见&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;明确推荐评价指标：对于婚恋推荐系统来说，最核心的指标无外乎付费的转换率&lt;/li&gt;
          &lt;li&gt;我们倒着来推，把问题转换为识别出最愿意付费的那些用户，然后找到这些用户感兴趣的用户，通过产品引导让这些用户发信&lt;/li&gt;
          &lt;li&gt;能不能从数据跳出来对产品提出一些创意性改进从而产生的产品模式和收费模式的变革。&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.wsdm-conference.org/2015/wp-content/uploads/2015/02/WSDM-2015-PE-Leskovec.pdf&quot;&gt;New Directions in Recommender Systems&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://www.douban.com/note/484692347/&quot;&gt;飞林沙-读后总结&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;需要理解 可替换 和 可补充，这两种推荐形式。&lt;/li&gt;
      &lt;li&gt;怎样生成替代品的推荐理由，应该是更好，而不是他们包含同一关键词&lt;/li&gt;
      &lt;li&gt;推荐一整套装备&lt;/li&gt;
      &lt;li&gt;Inferring Networks from Opinions阅读总结：
        &lt;ul&gt;
          &lt;li&gt;Product Graph：Building networks from product text
            &lt;ul&gt;
              &lt;li&gt;Understand the notions of substitute and complement goods&lt;/li&gt;
              &lt;li&gt;Generate explanations of why certain products are preferred&lt;/li&gt;
              &lt;li&gt;Recommends baskets of related items&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;learn x and y are related?
            &lt;ul&gt;
              &lt;li&gt;Attempt 1: Text features；缺点：High-dimensional，Prone to overfitting，Too fine-grained&lt;/li&gt;
              &lt;li&gt;Attempt 2: Features from Topics。也就是把第一种方法，用topic vector替换，相当于降维了。&lt;/li&gt;
              &lt;li&gt;Attempt 3: Learn ‘good’ topics。Learn to discover topics that explain the graph structure；
                &lt;ul&gt;
                  &lt;li&gt;Idea: Learn both simultaneously；we want to learn to project documents (reviews) into topic space such that related products are nearby；&lt;/li&gt;
                  &lt;li&gt;Combining topic models with link prediction；topic和link利用一个目标函数，一起训练。&lt;/li&gt;
                  &lt;li&gt;Issue 1: Relationships we want to learn are not symmetric；Solution: We solve this issue by learning “relatedness” in addition to “directedness”&lt;/li&gt;
                  &lt;li&gt;Issue 3: The model has a too many parameters；Solution: Product hierarchy；Associate each node in the category tree with a small number of topics&lt;/li&gt;
                  &lt;li&gt;整个模型用EM算法来求解，类似于PLSA的EM算法。&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Recommender Systems，Alex Smola 阅读笔记
    &lt;ul&gt;
      &lt;li&gt;Neighborhood methods
        &lt;ul&gt;
          &lt;li&gt;Collaborative filtering；User-based (item base is smaller than user or changes rapidly)；Item-based (user base is small)；&lt;/li&gt;
          &lt;li&gt;Normalization/Bias; rate bias-&amp;gt; bui = μ(global) + bu(user bias) + bi(item bias);
  &lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/cf_formula.png&quot; alt=&quot;&quot; /&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/item_similarity.png&quot; alt=&quot;&quot; /&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Matrix Factorization
        &lt;ul&gt;
          &lt;li&gt;SVD，常用优化方法：SGD，alternating optimization；问题是Overfitting without regularization，特别是fewer reviews than dimensions&lt;/li&gt;
          &lt;li&gt;Risk Minimization。利用Alternating least squares，比较适合MR。
  &lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/matrix_factorization_risk.png&quot; alt=&quot;&quot; /&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/matrix_factorization_risk_addbias.png&quot; alt=&quot;&quot; /&gt;&lt;/li&gt;
          &lt;li&gt;优化方法：Add bias，who rated what， temporal effects … P63&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Theoretical Motivation
        &lt;ul&gt;
          &lt;li&gt;Rating matrix is (row, column) exchangeable&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Ranking and Session Modeling
        &lt;ul&gt;
          &lt;li&gt;Independent click model&lt;/li&gt;
          &lt;li&gt;Logistic click model。Exponential family model for click; user looks at all&lt;/li&gt;
          &lt;li&gt;Sequential click model; User traverses list&lt;/li&gt;
          &lt;li&gt;Skip click model&lt;/li&gt;
          &lt;li&gt;Context skip click model&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Features
        &lt;ul&gt;
          &lt;li&gt;social network = friendship + interests&lt;/li&gt;
          &lt;li&gt;Latent dense (Bayesian Probabilistic Matrix Factorization)&lt;/li&gt;
          &lt;li&gt;Latent sparse (Dirichlet process factorization)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Hashing&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://breezedeus.github.io/2012/11/01/breezedeus-yuanquan-etao.html&quot;&gt;个性化推荐技术#总结-袁全&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;相关性推荐，点击数据更有用；补充性推荐，购买数据更有用；要根据用户行为意图选择不同的推荐方法。&lt;/li&gt;
      &lt;li&gt;对于不同种类的产品，当用户处在同一购物流程时，其理想的相关性推荐/补充性推荐的概率也差别很大。&lt;/li&gt;
      &lt;li&gt;Mixture Logistic Regression&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://breezedeus.github.io/2012/11/10/breezedeus-jiangshen.html&quot;&gt;面向广告主的推荐，江申@百度&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;技术目标要正确；譬如对于拍卖词推荐，其数学目标的烟花过程为：推荐最相关的词-&amp;gt;推荐广告主采用率最高的词-&amp;gt;推荐采用率最高且产生推广效果最佳的词。&lt;/li&gt;
      &lt;li&gt;在拍卖词推荐中主要涉及到三种模型：相关性模型、采用率模型和推广效果模型。&lt;/li&gt;
      &lt;li&gt;负反馈：按照item已经对user展示的次数指数级降低其权重，避免同一个item多次重复被展示给一个用户。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://breezedeus.github.io/2012/11/12/breezedeus-wenguozhu.html&quot;&gt;个性化推荐技术#总结 稳国柱@豆瓣&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;电影推荐：首先把电影按照电影标签进行分组（比如分成动作片，剧情片等）；然后在每个组里面使用CF算法产生推荐结果；最后把每组中获得的推荐按照加权组合的方式组合在一块。&lt;/li&gt;
      &lt;li&gt;图书推荐：图书有一定的阶梯性，在大部分的场合，我们需要的并不是与自己相似的用户的推荐，而是与自己相似的专家的推荐。&lt;/li&gt;
      &lt;li&gt;电台的音乐推荐：必须使用一个算法系统（其中包含多个算法）来针对不同的用户进行不同的算法调度&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.douban.com/note/472267231/&quot;&gt;年终总结 &amp;amp; 算法数据的思考 by 飞林沙&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.aszxqw.com/work/2014/06/01/tuijian-xitong-de-nadianshi.html&quot;&gt;推荐系统的那点事&lt;/a&gt; 分析了推荐系统中使用算法的误区，确实规则带来的好处简单有效。 当一个做推荐系统的部门开始重视【数据清理，数据标柱，效果评测，数据统计，数据分析】这些所谓的脏活累活，这样的推荐系统才会有救。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;WSDM2015 上传了Michaol Franklin和Thorsten Joachims的主题报告slides http://t.cn/R7Jyy0g 还有Jure Leskovec和Tushar Chandra的实践与经验报告 slides &lt;a href=&quot;http://www.wsdm-conference.org/2015/practice-and-experience-talks/&quot;&gt;Practice and Experience Talks&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;[文章]《Computing Recommendations at Extreme Scale with Apache Flink and Google Compute Engine》http://t.cn/RZemQe9 Flink实例！用Flink和GAE做面向大规模数据集的协同推荐，从中可看出Flink的巨大应用潜力，文中引用的材料值得一读（作者说了，细节文章即将推出敬请期待，感兴趣请持续关注）&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;[幻灯]《Recommender Systems: Super Overview》http://t.cn/R7WtFwY 来自Netflix的Xavier Amatriain在Summer School 2014 @ CMU上长达4小时的报告，共248页，是对推荐系统发展的一次全面综述，其中还包括Netflix在个性化推荐方面的一些经验介绍，强烈推荐! 云盘:http://t.cn/RZuLoSS&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;【干货丨美团推荐算法实践：机器学习重排序模型成亮点】本文介绍了美团网推荐系统的构建和优化过程中的一些做法，包括数据层、触发层、融合过滤层和排序层五个层次，采用了HBase、Hive、storm、Spark和机器学习等技术。两个优化亮点是将候选集进行融合与引入重排序模型。 http://t.cn/RZrgB5u&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://tech.meituan.com/machinelearning-data-feature-process.html&quot;&gt;美团推荐团队-机器学习中的数据清洗与特征处理综述&lt;/a&gt;。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://benanne.github.io/2014/08/05/spotify-cnns.html&quot;&gt;文章-Recommending music on Spotify with deep learning&lt;/a&gt; 基于作者的实习经历讲Spotify的音乐推荐，内容涉及：协同过滤、基于内容的推荐、基于深度学习的品味预测、convnets规模扩展、convnets的学习内容、推荐的具体应用等
    &lt;ul&gt;
      &lt;li&gt;Collaborative filtering：content-agnostic（与内容无关的），容易推荐popular items。另外，new and unpopular songs cannot be recommended，即cold-start problem。&lt;/li&gt;
      &lt;li&gt;Content-based：tags, artist and album information, lyrics, text mined from the web (reviews, interviews, …), and the audio signal itself（e.g. the mood of the music）。&lt;/li&gt;
      &lt;li&gt;Predicting listening preferences with deep learning。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;【推荐系统中所使用的混合技术介绍】http://t.cn/8sKdQFq 系统架构层面一般使用多段组合混合推荐框架，算法层面则使用加权型混合推荐技术，包括LR、RBM、GBDT系列。此外还介绍分级型混合推荐技术，交叉调和技术，瀑布型混合方法，推荐基础特征混合技术，推荐模型混合技术，整体式混合推荐框架等。&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Mon, 23 Feb 2015 00:00:00 +0800</pubDate>
        <link>http://yourdomain.com/2015/02/23/%E6%96%87%E7%8C%AE%E7%BB%BC%E5%90%88%E9%98%85%E8%AF%BB.html</link>
        <guid isPermaLink="true">http://yourdomain.com/2015/02/23/%E6%96%87%E7%8C%AE%E7%BB%BC%E5%90%88%E9%98%85%E8%AF%BB.html</guid>
        
        
      </item>
    
      <item>
        <title>系统知识总结</title>
        <description>&lt;h2 id=&quot;section&quot;&gt;系统设计&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Optimal Space-time Tradeoffs for Inverted Indexes，Github http://t.cn/RZgGiiN 倒排的最新压缩设计，作者是去年创新索引压缩算法Partitioned Elias-Fano的发明人，今年继续给出各种情形下的最佳选择，http://t.cn/RZgGiiC &lt;a href=&quot;http://www.di.unipi.it/~ottavian/files/wsdm15_index.pdf&quot;&gt;论文 Optimal Space-time Tradeoffs for Inverted Indexes&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://fex.baidu.com/blog/2014/04/traffic-hijack/&quot;&gt;流量劫持是如何产生的&lt;/a&gt; 流量劫持，这种古老的攻击沉寂了一段时间后，最近又开始闹的沸沸扬扬。众多知名品牌的路由器相继爆出存在安全漏洞，引来国内媒体纷纷报道。只要用户没改默认密码，打开一个网页甚至帖子，路由器配置就会被暗中修改。互联网一夜间变得岌岌可危。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://network.51cto.com/art/201103/252335.htm&quot;&gt;输入facebook的URL按下回车后究竟发生了什么&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.infoq.com/cn/articles/cache-coherency-primer&quot;&gt;缓存一致性 Cache Coherency&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.cnblogs.com/dolphin0520/archive/2011/08/25/2153720.html&quot;&gt;二叉树的非递归遍历&lt;/a&gt; 利用stack来实现非递归遍历。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-1&quot;&gt;编程语言&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.iteye.com/magazines/130&quot;&gt;编程精华资源-ITeye优秀专栏-大汇总&lt;/a&gt;
Java学习，Java框架，Web 前端，编程语言，开源项目研究，编程经验之谈，数据库，设计模式，项目管理，移动开发，云计算与大数据等&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://coolshell.cn/articles/9104.html&quot;&gt;sed 简明教程&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.vaikan.com/bash-scripting/&quot;&gt;bash脚本15分钟进阶&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sun, 22 Feb 2015 00:00:00 +0800</pubDate>
        <link>http://yourdomain.com/2015/02/22/%E7%B3%BB%E7%BB%9F%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93.html</link>
        <guid isPermaLink="true">http://yourdomain.com/2015/02/22/%E7%B3%BB%E7%BB%9F%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93.html</guid>
        
        
      </item>
    
      <item>
        <title>搜索广告的情怀</title>
        <description>&lt;p&gt;工作这些年，搜索广告是自己第一个接触的工作项目。从搜索广告一路走来，蓦然回首，有好些经历都值得记录与总结。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;搜索广告历程&lt;/h2&gt;

&lt;h3 id=&quot;soso&quot;&gt;soso&lt;/h3&gt;
&lt;p&gt;当时paulyan从google空降到soso，成立搜索广告平台部。我在新部门成立不到半年后，入职了，入职后我被分到扩展匹配组。记得那个时候，竞价系统正在进行大重构(要将明拍转换为暗拍)；部门里还只有两个中心(一个商务，一个后台)；扩展匹配组也还没有冠名。。。&lt;/p&gt;

&lt;h3 id=&quot;paipai&quot;&gt;paipai&lt;/h3&gt;

&lt;h3 id=&quot;myapp&quot;&gt;myapp&lt;/h3&gt;

&lt;h3 id=&quot;section-1&quot;&gt;说说情怀&lt;/h3&gt;
&lt;p&gt;张小龙说：微信就像一颗人的大脑，我也不知道它在想什么？
这就是程序员的情怀。&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;搜索广告系统&lt;/h2&gt;

&lt;h3 id=&quot;section-3&quot;&gt;系统介绍&lt;/h3&gt;

&lt;h3 id=&quot;section-4&quot;&gt;算法模块&lt;/h3&gt;

&lt;h4 id=&quot;query&quot;&gt;query分析&lt;/h4&gt;

&lt;h4 id=&quot;query-1&quot;&gt;query扩展&lt;/h4&gt;

&lt;h4 id=&quot;query-bidword&quot;&gt;query-bidword匹配&lt;/h4&gt;

&lt;h4 id=&quot;query-bidwordad&quot;&gt;query-bidword/ad相关性&lt;/h4&gt;

&lt;h4 id=&quot;section-5&quot;&gt;广告粗选&lt;/h4&gt;

&lt;h4 id=&quot;pctr&quot;&gt;精选/pCtr&lt;/h4&gt;

&lt;h4 id=&quot;section-6&quot;&gt;竞价&lt;/h4&gt;

</description>
        <pubDate>Sun, 22 Feb 2015 00:00:00 +0800</pubDate>
        <link>http://yourdomain.com/2015/02/22/%E6%90%9C%E7%B4%A2%E5%B9%BF%E5%91%8A%E7%9A%84%E6%83%85%E6%80%80.html</link>
        <guid isPermaLink="true">http://yourdomain.com/2015/02/22/%E6%90%9C%E7%B4%A2%E5%B9%BF%E5%91%8A%E7%9A%84%E6%83%85%E6%80%80.html</guid>
        
        
      </item>
    
      <item>
        <title>计算广告相关笔记</title>
        <description>&lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;h2 id=&quot;rtb&quot;&gt;RTB&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://tutorial.computational-advertising.org/images/WSDM2015.pdf&quot;&gt;Real-Time Bidding&lt;/a&gt;&lt;/p&gt;

    &lt;p&gt;除了有各类DSP竞价算法、SSP底价算法外，还加入了anti-fraud和广告期权交易的内容。另外也罗列了做互联网广告研究的公开数据集和开源工具。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;在RTB广告出价过程中，两个重点因素是：&lt;/p&gt;
    &lt;ol&gt;
      &lt;li&gt;广告展示的效益(点击率/转化率)；&lt;/li&gt;
      &lt;li&gt;展示花销。
以这两项因素为输入，我们的框架能优化出最优的RTB出价策略。
论文&lt;a href=&quot;http://www0.cs.ucl.ac.uk/staff/w.zhang/papers/ortb-kdd.pdf&quot;&gt;Optimal Real-Time Bidding for Display Advertising&lt;/a&gt; 实验基于@品友互动 &lt;a href=&quot;http://t.cn/RP3pPQO&quot;&gt;RTB数据集&lt;/a&gt;&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section&quot;&gt;计算广告&lt;/h2&gt;

&lt;h3 id=&quot;section-1&quot;&gt;关于广告的讨论：&lt;/h3&gt;

&lt;p&gt;广告一般是model based的方法，搜索rank一般是rule bases的方法，推荐通常也是广告那种玩法，不过我倾向于面向用户的产品应该rule based思路做。@梁斌penny&lt;/p&gt;

&lt;p&gt;把推荐看成user和item的dyad，用该user过去行为预测ctr来评估推荐系统，可以用广告这一套。但推荐看重的所谓novelty，diversity等等无法体现了，一般而言推荐是一堆策略的合成，广告相对单一一些，是否用广告架构与否不重要，关键要看问题是怎么定义的。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/izenecloud/laser/&quot;&gt;A Scalable Response Prediction Platform For Online Advertising&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;看了Hector Molina在Recsys’14上提的Search/Recomm/Ads三合一观点，挺赞同。这三个问题共同点都是matching a context to a collection of info obj, 核心的模块是filtering/ranking/personalisation, 从理论的cleanness和架构统一性来都很赞，实际中最大难度是各个应用各有自身调优需求，不好协调。&lt;/p&gt;

&lt;p&gt;最终的目标不一样，搜索求准，推荐求新，广告求财…还是很难统一的，当然，底层框架是可以共用的&lt;/p&gt;

&lt;p&gt;三个很不一样，搜索是站在用户角度识别用户想要什么。推荐是站在平台角度看想要引导什么。广告是以局外人看广告主、平台、用户分别想要什么。&lt;/p&gt;

&lt;h2 id=&quot;ctr&quot;&gt;CTR预估&lt;/h2&gt;

&lt;h3 id=&quot;section-2&quot;&gt;特征处理&lt;/h3&gt;
&lt;p&gt;在pCtr/pCvr/relevance问题中，最基本的问题就是特征处理。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://breezedeus.github.io/2014/11/15/breezedeus-feature-processing.html&quot;&gt;特征处理入门 by BreezeDeus&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.flickering.cn/ads/2014/08/转化率预估-4特征选择－简介/&quot;&gt;特征选择 by ubiwang&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://tech.meituan.com/machinelearning-data-feature-process.html&quot;&gt;美团推荐团队-机器学习中的数据清洗与特征处理综述&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-3&quot;&gt;特征降维&lt;/h3&gt;
&lt;p&gt;主要特征降维方法有：聚类，PCA，hashing&lt;/p&gt;

&lt;p&gt;下面看一下&lt;a href=&quot;https://breezedeus.github.io/2014/11/20/breezedeus-feature-hashing.html&quot;&gt;特征hash法&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;特征哈希法的目标是把原始的高维特征向量压缩成较低维特征向量，且尽量不损失原始特征的表达能力。&lt;/li&gt;
  &lt;li&gt;特征hash法可用于 多任务学习(multitask learning)&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;大数据机器学习专家John Langford总结了一下关于使用hashing trick来处理大数据的重要文章：http://t.cn/RzbMxm5 值得一看。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;参考文献：
    &lt;ul&gt;
      &lt;li&gt;Kilian Weinberger et al. Feature Hashing for Large Scale Multitask Learning,2010.  or Feature hashing for large scale multitask learning&lt;/li&gt;
      &lt;li&gt;Joshua Attenberg et al. Collaborative Email-Spam Filtering with the Hashing-Trick, 2009.&lt;/li&gt;
      &lt;li&gt;Alekh Agarwal, Oliveier Chapelle, Miroslav Dud ́ık, and John Langford. A Reliable Effective Terascale Linear Learning System. Journal of Machine Learning Research, 15:1111–1133, 2014.&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;&quot;&gt;Large-scale L-BFGS using MapReduce&lt;/a&gt; 该论文讲述了运用了hashing技术后，AUC的变化。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-4&quot;&gt;特征选择&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.datadive.net/selecting-good-features-part-i-univariate-selection/&quot;&gt;Selecting good features&lt;/a&gt; 《Selecting good features》特征工程系列文章：Part1.单变量选取http://t.cn/Rw4B2vO Part2.线性模型和正则化http://t.cn/Rw4Blqc Part3.随机森林http://t.cn/Rw4BEAQ Part4.稳定性选择法、递归特征排除法(RFE)及综合比较http://t.cn/Rw4rndV 内容很赞，还有Python代码示例，强烈推荐！&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-5&quot;&gt;预估模型&lt;/h3&gt;

&lt;p&gt;竞价搜索广告中的点击率预估法: 特征和模型&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www2007.org/papers/paper784.pdf&quot;&gt;经典LR WWW07&lt;/a&gt;，&lt;a href=&quot;http://t.cn/RZOSmEl&quot;&gt;L1正则版ICML07&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;高质量特征+组合LR和BDT &lt;a href=&quot;http://t.cn/RZOSmET&quot;&gt;ADKDD14&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;加LDA话题特征 &lt;a href=&quot;http://arxiv.org/pdf/1405.4402v3.pdf&quot;&gt;Peacock: Learning Long-Tail Topic Features for Industrial Applications&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;组合ANN和BDT &lt;a href=&quot;http://arxiv.org/pdf/1412.6601v1.pdf&quot;&gt;USING NEURAL NETWORKS FOR CLICK PREDICTION OF SPONSORED SEARCH&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;http://quinonero.net/Publications/predicting-clicks-facebook.pdf&quot;&gt;Practical Lessons from Predicting Clicks on Ads at Facebook&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://breezedeus.github.io/2014/11/19/breezedeus-feature-mining-gbdt.html&quot;&gt;利用GBDT模型构造新特征&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;特征决定了所有算法效果的上限，而不同的算法只是离这个上限的距离不同而已。&lt;/li&gt;
  &lt;li&gt;这篇论文的思想：先用已有特征训练GBDT模型，然后利用GBDT模型学习到的树来构造新特征，最后把这些新特征加入原有特征一起训练模型。构造的新特征向量是取值0/1的，向量的每个元素对应于GBDT模型中树的叶子结点。当一个样本点通过某棵树最终落在这棵树的一个叶子结点上，那么在新特征向量中这个叶子结点对应的元素值为1，而这棵树的其他叶子结点对应的元素值为0。新特征向量的长度等于GBDT模型里所有树包含的叶子结点数之和。&lt;/li&gt;
  &lt;li&gt;已经有人利用这种方法赢得了Kaggle一个CTR预估比赛的冠军，&lt;a href=&quot;https://github.com/guestwalk/kaggle-2014-criteo&quot;&gt;代码&lt;/a&gt;，里面有这种方法的具体实现。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-6&quot;&gt;在线学习&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;http://www.52ml.net/16256.html&quot;&gt;在线学习算法FTRL详解&lt;/a&gt; &lt;a href=&quot;http://www.cnblogs.com/EE-NovRain/p/3810737.html&quot;&gt;link2&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;现在做在线学习和CTR常常会用到逻辑回归（ Logistic Regression），而传统的批量（batch）算法无法有效地处理超大规模的数据集和在线数据流，google先后三年时间（2010年-2013年）从理论研究到实际工程化实现的FTRL（Follow-the-regularized-Leader）算法，在处理诸如逻辑回归之类的带非光滑正则化项（例如1范数，做模型复杂度控制和稀疏化）的凸优化问题上性能非常出色。&lt;/p&gt;

&lt;p&gt;损失函数 LogLoss (logistic loss)为：
&lt;script type=&quot;math/tex&quot;&gt;l_t(w_t) = −y_t\log(p_t) − (1−y_t)log(1−p_t)&lt;/script&gt;
其中\(x_t\)是instance feature vector at round t。&lt;/p&gt;

&lt;p&gt;其梯度为：
&lt;script type=&quot;math/tex&quot;&gt;\nabla l_t(w) = (σ(w·x_t) − y_t)x_t = (p_t − y_t)x_t&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;一般而言，求解online问题通常用:OGD(online gradient descent)算法，OGS的梯度更新公式为：&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;w_{t+1} = w_t − η_t.g_t&lt;/script&gt; 其中&lt;script type=&quot;math/tex&quot;&gt;η_t=\frac{1}{\sqrt(t)}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;OGD算法准确性较好，但稀疏性不强。所以为此提出一些改进方法，例如&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;FOBOS &lt;a href=&quot;&quot;&gt;J. Duchi and Y. Singer. Efficient learning using forward-backward splitting&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;truncated gradient 20 &lt;a href=&quot;&quot;&gt;J. Langford, L. Li, and T. Zhang. Sparse online learning via truncated gradient&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Regularized Dual Averaging (RDA) &lt;a href=&quot;&quot;&gt;L. Xiao. Dual averaging method for regularized stochastic learning and online optimization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;结合OGD的准确性和RDA算法的稀疏性，从而得到”Follow The (Prox-imally) Regularized Leader”算法。&lt;/p&gt;

&lt;p&gt;FTRL的梯度更新公式为：
&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/ftrl_gradient.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;FTRL的完整算法为：
&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/ftrl_algorithm.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;上面值得一提的是learning rate的设定，利用了per-coordinate方法。意思是对w的每一维分开训练更新的，每一维使用的是不同的学习速率，如下面公式所示：
&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/ftrl_per_coordinate.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;与w所有特征维度使用统一的学习速率相比，这种方法考虑了训练样本本身在不同特征上分布的不均匀性，如果包含w某个维度特征的训练样本很少，每一个样本都很珍贵，那么该特征维度对应的训练速率可以独自保持比较大的值，每来一个包含该特征的样本，就可以在该样本的梯度上前进一大步，更快的对新样本数据做出响应。而如果包含w某个维度特征的训练样本很多，则训练速率可以下降得比较快。&lt;/p&gt;

&lt;p&gt;除此，论文还提到几种节省存储的方法：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;在线去除训练数据中很少出现的特征，两种方法：(1)以一定概率p接受新特征；(2)利用counting bloom filter，出现次数大约n次的特征才会加入模型。&lt;/li&gt;
  &lt;li&gt;浮点数利用q2.13编码(只需要16bit)而不是64-bit floating point。&lt;/li&gt;
  &lt;li&gt;同时训练若干相似model，方便feature和模型评估。&lt;/li&gt;
  &lt;li&gt;single value struct。&lt;/li&gt;
  &lt;li&gt;使用正负样本的数目来近似计算梯度的和。&lt;/li&gt;
  &lt;li&gt;对负样本数据进行抽样，在训练时计算loss和gradient时，再用大于1的权重弥补负样本。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;关于CALIBRATING PREDICTIONS&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;简单方法，矫正公式为: \(\tau(p) = \gamma p^{\kappa}\)，再利用Poisson regression学习参数γ和κ。&lt;/li&gt;
  &lt;li&gt;piecewise linear or piecewise constant correction function，利用isotonic regression。参考文献:
&lt;a href=&quot;http://fastml.com/classifier-calibration-with-platts-scaling-and-isotonic-regression/&quot;&gt;Classifier calibration with Platt’s scaling and isotonic regression&lt;/a&gt;
&lt;a href=&quot;&quot;&gt;A. Niculescu-Mizil and R. Caruana. Predicting good probabilities with supervised learning&lt;/a&gt;，
&lt;a href=&quot;&quot;&gt;R. Luss, S. Rosset, and M. Shahar. Efficient regularized isotonic regression with application to gene–gene interaction search&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-7&quot;&gt;其他方面&lt;/h3&gt;

&lt;p&gt;** &lt;a href=&quot;http://blog.sina.com.cn/s/blog_6cb8e53d0101oetv.html&quot;&gt;并行逻辑回归&lt;/a&gt; &lt;a href=&quot;http://www.csdn.net/article/1970-01-01/2818400&quot;&gt;csdn链接&lt;/a&gt;**&lt;/p&gt;

&lt;p&gt;这里的并行实现我已经也做过，赶明儿可以把代码拿出来再看一看。归纳如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- 棋盘式划分，从数据和feature两个维度做划分。
- 并行LR实际上就是在求解损失函数最优解的过程中，针对寻找损失函数下降方向中的梯度方向计算作了并行化处理，而在利用梯度确定下降方向的过程中也可以采用并行化。
- 先在各自单元上做计算，然后做行归并，相当于得到点积和，再把点积和分发到同一行的机器上，再各自计算，最后做列归并，得到下降方向。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;http://www.infoq.com/cn/presentations/large-scale-machine-learning-of-advertisement-data&quot;&gt;广告数据上的大规模机器学习 夏粉&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;以广告点击率预估问题为例，介绍如何利用大规模机器学习技术搭建一个容纳万亿特征数据的、分钟级别模型更新的、自动高效深度学习。&lt;/li&gt;
  &lt;li&gt;主要内容包括：数据采样，对负样本以r(0,1]采样，训练时，给予抽样的负样本权重为1/r；噪音检测，计算点击率随时间变化趋势 –百度首创:SA算法；特征删减，google的做法是新特征按概率p加入 Bloom Filter+次数超过n，百度是Fea-G算法；模型的时效性，从小时减少到分钟，CTR大幅提升；训练算法优化，Shooting算法；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://www.kaggle.com/c/criteo-display-ad-challenge&quot;&gt;Display Advertising Challenge&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;由&lt;a href=&quot;http://www.criteo.com/cn/&quot;&gt;Criteo&lt;/a&gt;发起的移动展示广告点击率预估的竞赛。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Criteo自家的论文&lt;a href=&quot;http://people.csail.mit.edu/romer/papers/TISTRespPredAds.pdf&quot;&gt;Simple and scalable response prediction for display advertising&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.kaggle.com/c/criteo-display-ad-challenge/forums/t/10322/beat-the-benchmark-with-less-then-200mb-of-memory&quot;&gt;FTRL算法 benchemark实现&lt;/a&gt; 参考Google的论文&lt;a href=&quot;http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41159.pdf&quot;&gt;Ad Click Prediction: a View from the Trenches&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;NTU关于Criteo CTR Challenge的&lt;a href=&quot;https://github.com/guestwalk/kaggle-2014-criteo&quot;&gt;winning solution&lt;/a&gt;，有代码有slide。解决方案是用GBDT学intermediate feature，再配合raw feature，用FM训练CTR模型。其中GBDT学特征是参考了&lt;a href=&quot;http://quinonero.net/Publications/predicting-clicks-facebook.pdf&quot;&gt;Facebook ADKDD2014文章&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;NTU他们对预测的CTR进行了一个calibration，使得其与true clicked likelihood/average estimated CTR更接近，如果最后evaluation metric是logloss，这有时会有较大改进。FastML上面介绍这个 &lt;a href=&quot;http://fastml.com/classifier-calibration-with-platts-scaling-and-isotonic-regression/&quot;&gt;Platt’s Scaling (SVM输出概率的方法) and Isotonic Regression&lt;/a&gt;。@范涛_中科大&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-8&quot;&gt;模型评估&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Evaluation。Precision, Recall, AUCs and ROCs 准确率、召回率、AUC(曲线下面积)和ROC(受试者工作特征曲线)，文中就很多Kaggle竞赛分类任务最终多个评判标准间的关系进行了讨论 &lt;a href=&quot;https://shapeofdata.wordpress.com/2015/01/05/precision-recall-aucs-and-rocs/&quot;&gt;Precision, Recall, AUCs and ROCs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-9&quot;&gt;其他资料&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;[Sibyl: 来自Google的大规模机器学习系统] 在上周的IEEE/IFIP可靠系统和网络（DSN）国际会议上，Google软件工程师Tushar Chandra做了一个关于Sibyl系统的主题演讲。 Sibyl是一个监督式机器学习系统，用来解决预测方面的问题，比如YouTube的视频推荐。&lt;a href=&quot;https://users.soe.ucsc.edu/~niejiazhong/slides/chandra.pdf&quot;&gt;PDF&lt;/a&gt;，&lt;a href=&quot;http://www.infoq.com/cn/news/2014/07/google-sibyl&quot;&gt;page&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://weibo.com/p/1001603803645836675771&quot;&gt;汤兴-爱奇艺大脑—视频进化&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;在线实验效果检验&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://facebook.github.io/planout/docs/about-planout.html&quot;&gt;PlanOut facebook实验框架&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Fri, 13 Feb 2015 00:00:00 +0800</pubDate>
        <link>http://yourdomain.com/2015/02/13/%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A%E7%9B%B8%E5%85%B3%E7%AC%94%E8%AE%B0.html</link>
        <guid isPermaLink="true">http://yourdomain.com/2015/02/13/%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A%E7%9B%B8%E5%85%B3%E7%AC%94%E8%AE%B0.html</guid>
        
        
      </item>
    
      <item>
        <title>抽样与统计</title>
        <description>&lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;h2 id=&quot;section&quot;&gt;抽样&lt;/h2&gt;

&lt;h3 id=&quot;section-1&quot;&gt;蒙特卡洛数值积分&lt;/h3&gt;

&lt;p&gt;求f(x)的积分，如\(\int_a^b{f(x)dx}\)。如果f(x)形式比较复杂，则可以通过数值解法来求近似的结果。常用的方法是：蒙特卡洛积分。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\int_a^b{\frac{f(x)}{q(x)}q(x)dx}&lt;/script&gt;

&lt;p&gt;这样把q(x)看做是x在区间[a,b]内的概率分布，而把前面的分数部分看做是一个函数，在q(x)下随机抽取n个样本，当n足够大时，可以用均值来近似：\(\frac{1}{n}\sum_{i=1}^n{\frac{f(x_i)}{q(x_i)}}\)。只要q(x)比较容易采样就可以了。&lt;/p&gt;

&lt;p&gt;随机模拟方法的核心就是如何对一个概率分布得到样本，即抽样(sampling)。&lt;/p&gt;

&lt;h3 id=&quot;section-2&quot;&gt;均匀分布&lt;/h3&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;x_{n+1}=(ax_n+c)\mod m&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;box-muller-&quot;&gt;Box-Muller 变换&lt;/h3&gt;
&lt;p&gt;如果随机变量\(U_1,U_2\)独立，且U_1,U_2 ~ Uniform[0,1]&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;Z_0=\sqrt{-2lnU_1}\cos{(2\pi U_2)}&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;Z_1=\sqrt{-2lnU_1}\sin{(2\pi U_2)}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;则\(Z_0，Z_1\)独立且服从标准正态分布。&lt;/p&gt;

&lt;h3 id=&quot;acceptance-rejection-sampling&quot;&gt;接受-拒绝抽样(Acceptance-Rejection sampling)&lt;/h3&gt;

&lt;h3 id=&quot;importance-sampling&quot;&gt;重要性抽样(Importance sampling)&lt;/h3&gt;

&lt;h3 id=&quot;section-3&quot;&gt;马尔科夫链，马尔科夫稳态&lt;/h3&gt;

&lt;h3 id=&quot;mcmc-gibbs-sampling&quot;&gt;MCMC-Gibbs sampling算法&lt;/h3&gt;

&lt;h2 id=&quot;section-4&quot;&gt;参考资料&lt;/h2&gt;

&lt;h3 id=&quot;section-5&quot;&gt;链接&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cnblogs.com/xbinworld/p/4266146.html&quot;&gt;随机采样方法整理与讲解&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cnblogs.com/daniel-D/p/3388724.html&quot;&gt;从随机过程到马尔科夫链蒙特卡洛方法&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cs.ubc.ca/~arnaud/andrieu_defreitas_doucet_jordan_intromontecarlomachinelearning.pdf&quot;&gt;An Introduction to MCMC for Machine Learning&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://vcla.stat.ucla.edu/old/MCMC/MCMC_tutorial/Lect2_Basic_MCMC.pdf&quot;&gt;Markov chain Monte Carlo Basics&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://vcla.stat.ucla.edu/old/MCMC/MCMC_tutorial.htm&quot;&gt;Markov Chain Monte Carlo for Computer Vision &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www-scf.usc.edu/~mohammab/sampling.pdf&quot;&gt;Sampling Tutorial&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.bb.ustc.edu.cn/jpkc/xiaoji/jswl/skja/chapter2-3a.pdf&quot;&gt;任意分布的伪随机变量的抽样&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;sampling--statistic&quot;&gt;Sampling &amp;amp; Statistic&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/&quot;&gt;Probabilistic Programming &amp;amp; Bayesian Methods for Hackers&lt;/a&gt;&lt;/p&gt;

    &lt;p&gt;黑客们的概率编程和贝叶斯方法书，电子版，有代码，介绍为主，减少推导。同时详细介绍了使用PyMC进行MCMC编程的细节。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;amp;mid=203307140&amp;amp;idx=1&amp;amp;sn=83cf3093d9cd0f6bca8981eae0cecb9e#rd&quot;&gt;统计学公开课大盘点&lt;/a&gt;&lt;/p&gt;

    &lt;p&gt;很多统计学课程，看来习题可以从这里面获取了。推荐可汗学院的“Probability and Statistics（概率与统计)”&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;随机采样方法整理与讲解（MCMC、Gibbs Sampling等） - Bin的专栏 - 博客园 http://t.cn/RZDLdr4&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;[可视化]《Markov Chains》http://t.cn/RZBE1ME 马尔可夫链的交互可视化解释，和以前发过超赞的那个特征值特征向量的可视化同一系列&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://t.cn/RwinD8w&quot;&gt;Variational Inference for Machine Learning&lt;/a&gt; Shakir Mohamed讲面向机器学习的变分推断&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;[图书]《Forecasting, Principles and Practice》http://t.cn/zR4ZGMM 全面介绍预测模型和算法，对证券、电信、交通等时序信号的预测分析都很有用，书中例子为R语言，在线免费阅读&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;How To Implement These 5 Powerful Probability Distributions In 用python实现5种常用的概率分布：二项式分布、泊松分布、正态分布、Beta分布和指数分布 http://t.cn/RZWta4n&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;[文章]《Getting started in data science: My thoughts》http://t.cn/RviewMN 数据科学入门指南——作者从数学、统计学、实验和因果推理、机器学习、软件选择和实践经验积累各方面谈了自己的入门建议。和其他入门指南类文章不同的是，本文没有盲目看好MOOC之类的自学途径，建议都很中肯，推荐阅读&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Fri, 06 Feb 2015 00:00:00 +0800</pubDate>
        <link>http://yourdomain.com/2015/02/06/%E6%8A%BD%E6%A0%B7%E4%B8%8E%E7%BB%9F%E8%AE%A1.html</link>
        <guid isPermaLink="true">http://yourdomain.com/2015/02/06/%E6%8A%BD%E6%A0%B7%E4%B8%8E%E7%BB%9F%E8%AE%A1.html</guid>
        
        
      </item>
    
      <item>
        <title>语义分析的一些方法</title>
        <description>&lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;p&gt;author: vincentyao@tencent.com&lt;/p&gt;

&lt;p&gt;语义分析，本文指运用各种机器学习方法，挖掘与学习文本、图片等的深层次概念。wikipedia上的解释：In machine learning, semantic analysis of a corpus is the task of building structures that approximate concepts from a large set of documents(or images)。&lt;/p&gt;

&lt;p&gt;工作这几年，陆陆续续实践过一些项目，有搜索广告，社交广告，微博广告，品牌广告，内容广告等。要使我们广告平台效益最大化，首先需要理解用户，Context(将展示广告的上下文)和广告，才能将最合适的广告展示给用户。而这其中，就离不开对用户，对上下文，对广告的语义分析，由此催生了一些子项目，例如文本语义分析，图片语义理解，语义索引，短串语义关联，用户广告语义匹配等。&lt;/p&gt;

&lt;p&gt;接下来我将写一写我所认识的语义分析的一些方法，虽说我们在做的时候，效果导向居多，方法理论理解也许并不深入，不过权当个人知识点总结，有任何不当之处请指正，谢谢。&lt;/p&gt;

&lt;p&gt;本文主要由以下四部分组成：文本基本处理，文本语义分析，图片语义分析，语义分析小结。先讲述文本处理的基本方法，这构成了语义分析的基础。接着分文本和图片两节讲述各自语义分析的一些方法，值得注意的是，虽说分为两节，但文本和图片在语义分析方法上有很多共通与关联。最后我们简单介绍下语义分析在广点通”用户广告匹配”上的应用，并展望一下未来的语义分析方法。&lt;/p&gt;

&lt;h3 id=&quot;section&quot;&gt;1 文本基本处理&lt;/h3&gt;

&lt;p&gt;在讲文本语义分析之前，我们先说下文本基本处理，因为它构成了语义分析的基础。而文本处理有很多方面，考虑到本文主题，这里只介绍中文分词以及Term Weighting。&lt;/p&gt;

&lt;h4 id=&quot;section-1&quot;&gt;1.1 中文分词&lt;/h4&gt;
&lt;p&gt;拿到一段文本后，通常情况下，首先要做分词。分词的方法一般有如下几种：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;基于字符串匹配的分词方法。此方法按照不同的扫描方式，逐个查找词库进行分词。根据扫描方式可细分为：正向最大匹配，反向最大匹配，双向最大匹配，最小切分(即最短路径)；总之就是各种不同的启发规则。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;全切分方法。它首先切分出与词库匹配的所有可能的词，再运用统计语言模型决定最优的切分结果。它的优点在于可以解决分词中的歧义问题。下图是一个示例，对于文本串”南京市长江大桥”，首先进行词条检索(一般用Trie存储)，找到匹配的所有词条（南京，市，长江，大桥，南京市，长江大桥，市长，江大桥，江大，桥），以词网格(word lattices)形式表示，接着做路径搜索，基于统计语言模型(例如n-gram)[18]找到最优路径，最后可能还需要命名实体识别。下图中”南京市 长江 大桥”的语言模型得分，即P(南京市，长江，大桥)最高，则为最优切分。&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/rnnlm1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;图1. “南京市长江大桥”语言模型得分&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;由字构词的分词方法。可以理解为字的分类问题，也就是自然语言处理中的sequence labeling问题，通常做法里利用HMM，MAXENT，MEMM，CRF等预测文本串每个字的tag[62]，譬如B，E，I，S，这四个tag分别表示：beginning, ending, inside, single，也就是一个词的开始，结束，中间，以及单个字的词。 例如”南京市长江大桥”的标注结果可能为：”南(B)京(I)市(E)长(B)江(E)大(B)桥(E)”&lt;/p&gt;

    &lt;p&gt;由于CRF既可以像最大熵模型一样加各种领域feature，又避免了HMM的齐次马尔科夫假设，所以基于CRF的分词目前是效果最好的，具体请参考文献[61,62,63]。&lt;/p&gt;

    &lt;p&gt;除了HMM，CRF等模型，分词也可以基于深度学习方法来做，如文献[9][10]所介绍，也取得了state-of-the-art的结果。&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/word_segmentation.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;图2. 基于深度学习的中文分词&lt;/p&gt;

    &lt;p&gt;上图是一个基于深度学习的分词示例图。我们从上往下看，首先对每一个字进行Lookup Table，映射到一个固定长度的特征向量(这里可以利用词向量，boundary entropy，accessor variety等)；接着经过一个标准的神经网络，分别是linear，sigmoid，linear层，对于每个字，预测该字属于B,E,I,S的概率；最后输出是一个矩阵，矩阵的行是B,E,I,S 4个tag，利用viterbi算法就可以完成标注推断，从而得到分词结果。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;一个文本串除了分词，还需要做词性标注，命名实体识别，新词发现等。通常有两种方案，一种是pipeline approaches，就是先分词，再做词性标注；另一种是joint approaches，就是把这些任务用一个模型来完成。有兴趣可以参考文献[9][62]等。&lt;/p&gt;

&lt;p&gt;一般而言，方法一和方法二在工业界用得比较多，方法三因为采用复杂的模型，虽准确率相对高，但耗时较大。在腾讯内部采用的是方法二。&lt;/p&gt;

&lt;h4 id=&quot;section-2&quot;&gt;1.2 语言模型&lt;/h4&gt;
&lt;p&gt;前面在讲”全切分分词”方法时，提到了语言模型，并且通过语言模型，还可以引出词向量，所以这里把语言模型简单阐述一下。&lt;/p&gt;

&lt;p&gt;语言模型是用来计算一个句子产生概率的概率模型，即\(P(w_1,w_2,w_3…w_m)\)，m表示词的总个数。根据贝叶斯公式：\(P(w_1,w_2,w_3 … w_m) = P(w_1)P(w_2|w_1)P(w_3|w_1,w_2) … P(w_m|w_1,w_2 … w_{m-1})\)。&lt;/p&gt;

&lt;p&gt;最简单的语言模型是N-Gram，它利用马尔科夫假设，认为句子中每个单词只与其前n-1个单词有关，即假设产生w_m这个词的条件概率只依赖于前n-1个词，则有
&lt;script type=&quot;math/tex&quot;&gt;P(w_m \|w_1,w_2...w_{m-1}) = P(w_m \|w_{m-n+1},w_{m-n+2} ... w_{m-1})&lt;/script&gt;。&lt;/p&gt;

&lt;p&gt;其中n越大，模型可区别性越强，n越小，模型可靠性越高。&lt;/p&gt;

&lt;p&gt;N-Gram语言模型简单有效，但是它只考虑了词的位置关系，没有考虑词之间的相似度，词法和词语义，并且还存在数据稀疏的问题，所以后来，又逐渐提出更多的语言模型，例如Class-based ngram model，topic-based ngram model，cache-based ngram model，skipping ngram model，指数语言模型（最大熵模型，条件随机域模型）等。若想了解更多请参考文章[18]。&lt;/p&gt;

&lt;p&gt;最近，随着深度学习的兴起，神经网络语言模型也变得火热[4]。用神经网络训练语言模型的经典之作，要数Bengio等人发表的《A Neural Probabilistic Language Model》[3]，它也是基于n-gram的，首先将每个单词 &lt;script type=&quot;math/tex&quot;&gt;w_{m-n+1},w_{m-n+2} ... w_{m-1}&lt;/script&gt; 映射到词向量空间，再把各个单词的词向量组合成一个更大的向量作为神经网络输入，输出是\(P(w_m)\)。本文将此模型简称为ffnnlm（Feed-forward Neural Net Language Model）。ffnnlm解决了传统n-gram的两个缺陷：(1)词语之间的相似性可以通过词向量来体现；(2)自带平滑功能。文献[3]不仅提出神经网络语言模型，还顺带引出了词向量，关于词向量，后文将再细述。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/ffnnlm.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;图3. 基于神经网络的语言模型&lt;/p&gt;

&lt;p&gt;从最新文献看，目前state-of-the-art语言模型应该是基于循环神经网络(recurrent neural network)的语言模型，简称rnnlm[5][6]。循环神经网络相比于传统前馈神经网络，其特点是：可以存在有向环，将上一次的输出作为本次的输入。而rnnlm和ffnnlm的最大区别是：ffnnmm要求输入的上下文是固定长度的，也就是说n-gram中的 n 要求是个固定值，而rnnlm不限制上下文的长度，可以真正充分地利用所有上文信息来预测下一个词，本次预测的中间隐层信息(例如下图中的context信息)可以在下一次预测里循环使用。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/simple_rnn.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;图4. 基于simple RNN(time-delay neural network)的语言模型&lt;/p&gt;

&lt;p&gt;如上图所示，这是一个最简单的rnnlm，神经网络分为三层，第一层是输入层，第二层是隐藏层(也叫context层)，第三层输出层。
假设当前是t时刻，则分三步来预测\(P(w_m)\)：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;单词\(w_{m-1}\)映射到词向量，记作input(t)&lt;/li&gt;
  &lt;li&gt;连接上一次训练的隐藏层context(t-1)，经过sigmoid function，生成当前t时刻的context(t)&lt;/li&gt;
  &lt;li&gt;利用softmax function，预测\(P(w_m)\)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;参考文献[7]中列出了一个rnnlm的library，其代码紧凑。利用它训练中文语言模型将很简单，上面”南京市 长江 大桥”就是rnnlm的预测结果。&lt;/p&gt;

&lt;p&gt;基于RNN的language model利用BPTT(BackPropagation through time)算法比较难于训练，原因就是深度神经网络里比较普遍的vanishing gradient问题[55]（在RNN里，梯度计算随时间成指数倍增长或衰减，称之为Exponential Error Decay）。所以后来又提出基于LSTM(Long short term memory)的language model，LSTM也是一种RNN网络，关于LSTM的详细介绍请参考文献[54,49,52]。LSTM通过网络结构的修改，从而避免vanishing gradient问题，具体分析请参考文献[83]。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/lstm_unit.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;图5. LSTM memory cell&lt;/p&gt;

&lt;p&gt;如上图所示，是一个LSTM unit。如果是传统的神经网络unit，output activation bi = activation_function(ai)，但LSTM unit的计算相对就复杂些了，它保存了该神经元上一次计算的结果，通过input gate，output gate，forget gate来计算输出，具体过程请参考文献[53，54]。&lt;/p&gt;

&lt;h4 id=&quot;term-weighting&quot;&gt;1.3 Term Weighting&lt;/h4&gt;
&lt;p&gt;##### Term重要性
对文本分词后，接下来需要对分词后的每个term计算一个权重，重要的term应该给与更高的权重。举例来说，”什么产品对减肥帮助最大？”的term weighting结果可能是: “什么 0.1，产品 0.5，对 0.1，减肥 0.8，帮助 0.3，最大 0.2”。Term weighting在文本检索，文本相关性，核心词提取等任务中都有重要作用。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Term weighting的打分公式一般由三部分组成：local，global和normalization [1,2]。即
&lt;script type=&quot;math/tex&quot;&gt;TermWeight=L_{i,j} G_i N_j&lt;/script&gt;。\(L_{i,j}\)是term i在document j中的local weight，\(G_i\)是term i的global weight，\(N_j\)是document j的归一化因子。
常见的local，global，normalization weight公式[2]有：&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/local_weight.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;图6. Local weight formulas&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/global_weight.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;图7. Global weight formulas&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/normlization_weight.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;图8. Normalization factors&lt;/p&gt;

    &lt;p&gt;Tf-Idf是一种最常见的term weighting方法。在上面的公式体系里，Tf-Idf的local weight是FREQ，glocal weight是IDFB，normalization是None。tf是词频，表示这个词出现的次数。df是文档频率，表示这个词在多少个文档中出现。idf则是逆文档频率，idf=log(TD/df)，TD表示总文档数。tf-idf在很多场合都很有效，但缺点也比较明显，以”词频”度量重要性，不够全面，譬如在搜索广告的关键词匹配时就不够用。&lt;/p&gt;

    &lt;p&gt;除了TF-IDF外，还有很多其他term weighting方法，例如Okapi，MI，LTU，ATC，TF-ICF[59]等。通过local，global，normalization各种公式的组合，可以生成不同的term weighting计算方法。不过上面这些方法都是无监督计算方法，有一定程度的通用性，但在一些特定场景里显得不够灵活，不够准确，所以可以基于有监督机器学习方法来拟合term weighting结果。&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/okapi.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;图9. Okapi计算公式&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;利用有监督机器学习方法来预测weight。这里类似于机器学习的分类任务，对于文本串的每个term，预测一个[0,1]的得分，得分越大则term重要性越高。&lt;/p&gt;

    &lt;p&gt;既然是有监督学习，那么就需要训练数据。如果采用人工标注的话，极大耗费人力，所以可以采用训练数据自提取的方法，利用程序从搜索日志里自动挖掘。从海量日志数据里提取隐含的用户对于term重要性的标注，得到的训练数据将综合亿级用户的”标注结果”，覆盖面更广，且来自于真实搜索数据，训练结果与标注的目标集分布接近，训练数据更精确。下面列举三种方法(除此外，还有更多可以利用的方法)：&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;从搜索session数据里提取训练数据，用户在一个检索会话中的检索核心意图是不变的，提取出核心意图所对应的term，其重要性就高。&lt;/li&gt;
      &lt;li&gt;从历史短串关系资源库里提取训练数据，短串扩展关系中，一个term出现的次数越多，则越重要。&lt;/li&gt;
      &lt;li&gt;从搜索广告点击日志里提取训练数据，query与bidword共有term的点击率越高，它在query中的重要程度就越高。&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;通过上面的方法，可以提取到大量质量不错的训练数据（数十亿级别的数据，这其中可能有部分样本不准确，但在如此大规模数据情况下，绝大部分样本都是准确的）。&lt;/p&gt;

    &lt;p&gt;有了训练数据，接下来提取特征，基于逻辑回归模型来预测文本串中每个term的重要性。所提取的特征包括：&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;term的自解释特征，例如term专名类型，term词性，term idf，位置特征，term的长度等；&lt;/li&gt;
      &lt;li&gt;term与文本串的交叉特征，例如term与文本串中其他term的字面交叉特征，term转移到文本串中其他term的转移概率特征，term的文本分类、topic与文本串的文本分类、topic的交叉特征等。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;section-3&quot;&gt;核心词、关键词提取&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;短文本串的核心词提取。对短文本串分词后，利用上面介绍的term weighting方法，获取term weight后，取一定的阈值，就可以提取出短文本串的核心词。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;长文本串(譬如web page)的关键词提取。这里简单介绍几种方法。想了解更多，请参考文献[69]。&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;采用基于规则的方法。考虑到位置特征，网页特征等。&lt;/li&gt;
      &lt;li&gt;基于广告主购买的bidword和高频query建立多模式匹配树，在长文本串中进行全字匹配找出候选关键词，再结合关键词weight，以及某些规则找出优质的关键词。&lt;/li&gt;
      &lt;li&gt;类似于有监督的term weighting方法，也可以训练关键词weighting的模型。&lt;/li&gt;
      &lt;li&gt;基于文档主题结构的关键词抽取，具体可以参考文献[71]。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-4&quot;&gt;2 文本语义分析&lt;/h3&gt;
&lt;p&gt;前面讲到一些文本基本处理方法。一个文本串，对其进行分词和重要性打分后（当然还有更多的文本处理任务），就可以开始更高层的语义分析任务。&lt;/p&gt;

&lt;h4 id=&quot;topic-model&quot;&gt;2.1 Topic Model&lt;/h4&gt;
&lt;p&gt;首先介绍主题模型。说到主题模型，第一时间会想到pLSA，NMF，LDA。关于这几个目前业界最常用的主题模型，已经有相当多的介绍了，譬如文献[60，64]。在这里，主要想聊一下主题模型的应用以及最新进展(考虑到LDA是pLSA的generalization，所以下面只介绍LDA)。&lt;/p&gt;

&lt;h5 id=&quot;lda&quot;&gt;LDA训练算法简单介绍&lt;/h5&gt;
&lt;p&gt;LDA的推导这里略过不讲，具体请参考文献[64]。下面我们主要看一下怎么训练LDA。&lt;/p&gt;

&lt;p&gt;在Blei的原始论文中，使用variational inference和EM算法进行LDA推断(与pLSA的推断过程类似，E-step采用variational inference)，但EM算法可能推导出局部最优解，且相对复杂。目前常用的方法是基于gibbs sampling来做[57]。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Step1: 随机初始化每个词的topic，并统计两个频率计数矩阵：Doc-Topic 计数矩阵N(t,d)，描述每个文档中的主题频率分布；Word-Topic 计数矩阵N(w,t)，表示每个主题下词的频率分布。&lt;/li&gt;
  &lt;li&gt;Step2: 遍历训练语料，按照概率公式(下图所示)重新采样每个词所对应的topic, 更新N(t,d)和N(w,t)的计数。&lt;/li&gt;
  &lt;li&gt;Step3: 重复 step2，直到模型收敛。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对文档d中词w的主题z进行重新采样的公式有非常明确的物理意义，表示为P(w|z)P(z|d)，直观的表示为一个“路径选择”的过程。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/lda_sampling.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;图10. gibbs sampling过程图&lt;/p&gt;

&lt;p&gt;以上描述过程具体请参考文献[65]。&lt;/p&gt;

&lt;p&gt;对于LDA模型的更多理论介绍，譬如如何实现正确性验证，请参考文献[68]，而关于LDA模型改进，请参考Newman团队的最新文章《Care and Feeding of Topic Models》[12]。&lt;/p&gt;

&lt;h5 id=&quot;section-5&quot;&gt;主题模型的应用点&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;在广点通内部，主题模型已经在很多方面都得到成功应用[65]，譬如文本分类特征，相关性计算，ctr预估，精确广告定向，矩阵分解等。具体来说，基于主题模型，可以计算出文本，用户的topic分布，将其当作pctr，relevance的特征，还可以将其当作一种矩阵分解的方法，用于降维，推荐等。不过在我们以往的成功运用中，topic模型比较适合用做某些机器学习任务的特征，而不适合作为一种独立的方法去解决某种特定的问题，例如触发，分类。Blei是这样评价lda的：it can easily be used as a module in more complicated models for more complicated goals。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;为什么topic model不适合作为一种独立的方法去解决某种特定的问题(例如分类，触发等)。&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;个人总结，主要原因是lda模型可控性可解释性相对比较差：对于每个topic，不能用很明确的语义归纳出这个topic在讲什么；重新训练一遍lda模型，每个topic id所对应的语义可能发生了变化；有些topic的准确性比较好，有些比较差，而对于比较差的topic，没有特别好的针对性的方法去优化它；&lt;/li&gt;
      &lt;li&gt;另外一个就是topic之间的重复，特别是在topic数目比较多的情况，重复几乎是不可避免的，当时益总在开发peacock的时候，deduplicate topic就是一个很重要的任务。如果多个topic描述的意思一致时，用topic id来做检索触发，效果大半是不好的，后来我们也尝试用topic word来做，但依旧不够理想。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;section-6&quot;&gt;主题模型最新进展&lt;/h5&gt;
&lt;p&gt;首先主题模型自PLSA, LDA后，又提出了很多变体，譬如HDP。LDA的topic number是预先设定的，而HDP的topic number是不固定，而是从训练数据中学习得到的，这在很多场景是有用的，具体参考&lt;a href=&quot;http://datascience.stackexchange.com/questions/128/latent-dirichlet-allocation-vs-hierarchical-dirichlet-process&quot;&gt;hdp vs lda&lt;/a&gt;。想了解更多LDA模型的升级，请参考文献[73,74]。&lt;/p&gt;

&lt;p&gt;深度学习方面，Geoff Hinton及其学生用Deep Boltzmann Machine研究出了类似LDA的隐变量文本模型[82]，文章称其抽取的特征在文本检索与文本分类上的结果比LDA好。&lt;a href=&quot;http://weibo.com/u/2387864597&quot;&gt;heavenfireray&lt;/a&gt;在其微博评论道：lda结构是word-hidden topic。类lda结构假设在topic下产生每个word是条件独立而且参数相同。这种假设导致参数更匹配长文而非短文。该文章提出word-hidden topic-hidden word，其实是(word,hidden word)-hidden topic，增加的hidden word平衡了参数对短文的适配，在分类文章数量的度量上更好很自然。&lt;/p&gt;

&lt;p&gt;其次，随着目前互联网的数据规模的逐渐增加，大规模并行PLSA，LDA训练将是主旋律。大规模主题模型训练，除了从系统架构上进行优化外，更关键的，还需要在算法本身上做升级。variational方法不太适合并行化，且速度相对也比较慢，这里我们着重看sampling-base inference。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;collapsed Gibbs sampler[57]：O(K)复杂度，K表示topic的总个数。&lt;/li&gt;
  &lt;li&gt;SparseLDA[66]：算法复杂度为O(Kd + Kw)，Kd表示文档d所包含的topic个数，Kw表示词w所属的topic个数，考虑到一个文档所包含的topic和一个词所属的topic个数是有限的，肯定远小于K，所以相比于collapsed Gibbs，复杂度已有较大的下降。&lt;/li&gt;
  &lt;li&gt;AliasLDA[56]：利用alias table和Metropolis-Hastings，将词这个维度的采样复杂度降至O(1)。所以算法总复杂度为O(Kd)。&lt;/li&gt;
  &lt;li&gt;Metropolis-Hastings sampler[13]：复杂度降至O(1)。这里不做分析了，具体请参考文献[13]&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;section-7&quot;&gt;主题模型并行化&lt;/h5&gt;
&lt;p&gt;在文献[67]中，Newman团队提出了LDA算法的并行化版本Approximate distributed-LDA，如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/ad_lda.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;图11. AD-LDA算法&lt;/p&gt;

&lt;p&gt;在原始gibbs sampling算法里，N(w,t)这个矩阵的更新是串行的，但是研究发现，考虑到N(w,t)矩阵在迭代过程中，相对变化较小，多个worker独立更新N(w,t)，在一轮迭代结束后再根据多个worker的本地更新合并到全局更新N(w,t)，算法依旧可以收敛[67]。&lt;/p&gt;

&lt;p&gt;那么，主题模型的并行化(不仅仅是主题模型，其实是绝大部分机器学习算法)，主要可以从两个角度来说明：数据并行和模型并行。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;数据并行。这个角度相对比较直观，譬如对于LDA模型，可以将训练数据按照worker数目切分为M片(M为worker数)，每个worker保存一份全局的N(w,t)矩阵，在一轮迭代里，各个worker独立计算，迭代结束后，合并各个worker的本地更新。这个思路可以借用目前通用的并行计算框架，譬如Spark，Hadoop，Graphlab等来实现。&lt;/li&gt;
  &lt;li&gt;模型并行。考虑到矩阵N(w,t)在大规模主题模型中相当巨大，单机内存不可能存下。所以直观的想法，可以将N(w,t)也切分成多个分片。N(w,t)可以考虑使用全局的parameter server来存储，也可以考虑存储在不同worker上，利用MPI AllReduce来通信。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;数据与模型并行，可以形象的描述为一个棋盘。棋盘的行按照数据划分，棋盘的列按照模型划分。LDA的并行化，就是通过这样的切分，将原本巨大的，不可能在单机存储的矩阵切分到不同的机器，使每台机器都能够将参数存储在内存。再接着，各个worker相对独立计算，计算的过程中不时按照某些策略同步模型数据。&lt;/p&gt;

&lt;p&gt;最近几年里，关于LDA并行化已有相当多的开源实现，譬如：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;PLDA，&lt;a href=&quot;https://code.google.com/p/plda/&quot;&gt;PLDA+&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/sudar/Yahoo_LDA&quot;&gt;Yahoo LDA&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/mli/parameter_server&quot;&gt;Parameter server&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;最近的并行LDA实现Peacock[70,65]和LigthLda[13]没有开源()，但我们可以从其论文一窥究竟，总体来说，并行化的大体思路是一致的。譬如LightLDA[13]，下图是实现架构框图，它将训练数据切分成多个Block，模型通过parameter server来同步，每个data block，类似于sliding windows，在计算完V1的采样后，才会去计算V2的采样(下图中V1,V2,V3表示word空间的划分，即模型的划分)。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/parallelism_lda.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;图12. LightLda并行结构图&lt;/p&gt;

&lt;h4 id=&quot;section-8&quot;&gt;2.2 词向量，句向量&lt;/h4&gt;

&lt;h5 id=&quot;section-9&quot;&gt;词向量是什么&lt;/h5&gt;
&lt;p&gt;在文本分析的vector space model中，是用向量来描述一个词的，譬如最常见的One-hot representation。One-hot representation方法的一个明显的缺点是，词与词之间没有建立关联。在深度学习中，一般用Distributed Representation来描述一个词，常被称为”Word Representation”或”Word Embedding”，也就是我们俗称的”词向量”。&lt;/p&gt;

&lt;p&gt;词向量起源于hinton在1986年的论文[11]，后来在Bengio的ffnnlm论文[3]中，被发扬光大，但它真正被我们所熟知，应该是word2vec[14]的开源。在ffnnlm中，词向量是训练语言模型的一个副产品，不过在word2vec里，是专门来训练词向量，所以word2vec相比于ffnnlm的区别主要体现在：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;模型更加简单，去掉了ffnnlm中的隐藏层，并去掉了输入层跳过隐藏层直接到输出层的连接。&lt;/li&gt;
  &lt;li&gt;训练语言模型是利用第m个词的前n个词预测第m个词，而训练词向量是用其前后各n个词来预测第m个词，这样做真正利用了上下文来预测，如下图所示。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/word2vec.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;图13. word2vec的训练算法&lt;/p&gt;

&lt;p&gt;上图是word2vec的两种训练算法：CBOW(continuous bag-of-words)和Skip-gram。在cbow方法里，训练目标是给定一个word的context，预测word的概率；在skip-gram方法里，训练目标则是给定一个word，预测word的context的概率。&lt;/p&gt;

&lt;p&gt;关于word2vec，在算法上还有较多可以学习的地方，例如利用huffman编码做层次softmax，negative sampling，工程上也有很多trick，具体请参考文章[16][17]。&lt;/p&gt;

&lt;h5 id=&quot;section-10&quot;&gt;词向量的应用&lt;/h5&gt;
&lt;p&gt;词向量的应用点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;可以挖掘词之间的关系，譬如同义词。&lt;/li&gt;
  &lt;li&gt;可以将词向量作为特征应用到其他机器学习任务中，例如作为文本分类的feature，Ronan collobert在Senna[37]中将词向量用于POS, CHK, NER等任务。&lt;/li&gt;
  &lt;li&gt;用于机器翻译[28]。分别训练两种语言的词向量，再通过词向量空间中的矩阵变换，将一种语言转变成另一种语言。&lt;/li&gt;
  &lt;li&gt;word analogy，即已知a之于b犹如c之于d，现在给出 a、b、c，C(a)-C(b)+C(c)约等于C(d)，C(*)表示词向量。可以利用这个特性，提取词语之间的层次关系。&lt;/li&gt;
  &lt;li&gt;Connecting Images and Sentences，image understanding。例如文献，DeViSE: A deep visual-semantic em-bedding model。&lt;/li&gt;
  &lt;li&gt;Entity completion in Incomplete Knowledge bases or ontologies，即relational extraction。Reasoning with neural tensor net- works for knowledge base completion。&lt;/li&gt;
  &lt;li&gt;more word2vec applications，点击&lt;a href=&quot;http://www.quora.com/Do-you-know-any-interesting-applications-using-distributed-representations-of-words-obtained-from-NNLM-eg-word2vec&quot;&gt;link1&lt;/a&gt;，&lt;a href=&quot;https://www.quora.com/What-are-some-interesting-Word2Vec-results&quot;&gt;link2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;除了产生词向量，word2vec还有很多其他应用领域，对此我们需要把握两个概念：doc和word。在词向量训练中，doc指的是一篇篇文章，word就是文章中的词。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;假设我们将一簇簇相似的用户作为doc（譬如QQ群），将单个用户作为word，我们则可以训练user distributed representation，可以借此挖掘相似用户。&lt;/li&gt;
  &lt;li&gt;假设我们将一个个query session作为doc，将query作为word，我们则可以训练query distributed representation，挖掘相似query。&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;section-11&quot;&gt;句向量&lt;/h5&gt;

&lt;p&gt;分析完word distributed representation，我们也许会问，phrase，sentence是否也有其distributed representation。最直观的思路，对于phrase和sentence，我们将组成它们的所有word对应的词向量加起来，作为短语向量，句向量。在参考文献[34]中，验证了将词向量加起来的确是一个有效的方法，但事实上还有更好的做法。&lt;/p&gt;

&lt;p&gt;Le和Mikolov在文章《Distributed Representations of Sentences and Documents》[20]里介绍了sentence vector，这里我们也做下简要分析。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;先看c-bow方法，相比于word2vec的c-bow模型，区别点有：&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;训练过程中新增了paragraph id，即训练语料中每个句子都有一个唯一的id。paragraph id和普通的word一样，也是先映射成一个向量，即paragraph vector。paragraph vector与word vector的维数虽一样，但是来自于两个不同的向量空间。在之后的计算里，paragraph vector和word vector累加或者连接起来，作为输出层softmax的输入。在一个句子或者文档的训练过程中，paragraph id保持不变，共享着同一个paragraph vector，相当于每次在预测单词的概率时，都利用了整个句子的语义。&lt;/li&gt;
      &lt;li&gt;在预测阶段，给待预测的句子新分配一个paragraph id，词向量和输出层softmax的参数保持训练阶段得到的参数不变，重新利用梯度下降训练待预测的句子。待收敛后，即得到待预测句子的paragraph vector。&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/sentence2vec0.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;图14. sentence2vec cbow算法&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;sentence2vec相比于word2vec的skip-gram模型，区别点为：在sentence2vec里，输入都是paragraph vector，输出是该paragraph中随机抽样的词。&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/sentence2vec1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;图15. sentence2vec skip-gram算法&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面是sentence2vec的结果示例。先利用中文sentence语料训练句向量，然后通过计算句向量之间的cosine值，得到最相似的句子。可以看到句向量在对句子的语义表征上还是相当惊叹的。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/sentence2vec4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;图16. sentence2vec 结果示例&lt;/p&gt;

&lt;h5 id=&quot;section-12&quot;&gt;词向量的改进&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;学习词向量的方法主要分为：Global matrix factorization和Shallow Window-Based。Global matrix factorization方法主要利用了全局词共现，例如LSA；Shallow Window-Based方法则主要基于local context window，即局部词共现，word2vec是其中的代表；Jeffrey Pennington在word2vec之后提出了&lt;a href=&quot;http://nlp.stanford.edu/projects/glove/&quot;&gt;GloVe&lt;/a&gt;，它声称结合了上述两种方法，提升了词向量的学习效果。它与word2vec的更多对比请点击&lt;a href=&quot;http://www.quora.com/How-is-GloVe-different-from-word2vec&quot;&gt;GloVe vs word2vec&lt;/a&gt;，&lt;a href=&quot;http://radimrehurek.com/2014/12/making-sense-of-word2vec/&quot;&gt;GloVe &amp;amp; word2vec评测&lt;/a&gt;。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;目前通过词向量可以充分发掘出”一义多词”的情况，譬如”快递”与”速递”；但对于”一词多义”，束手无策，譬如”苹果”(既可以表示苹果手机、电脑，又可以表示水果)，此时我们需要用多个词向量来表示多义词。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;section-13&quot;&gt;2.3 卷积神经网络&lt;/h4&gt;

&lt;h5 id=&quot;section-14&quot;&gt;卷积&lt;/h5&gt;

&lt;p&gt;介绍卷积神经网络(convolutional neural network，简记cnn)之前，我们先看下卷积。&lt;/p&gt;

&lt;p&gt;在一维信号中，卷积的运算，请参考&lt;a href=&quot;http://zh.wikipedia.org/wiki/卷积&quot;&gt;wiki&lt;/a&gt;，其中的图示很清楚。在图像处理中，对图像用一个卷积核进行卷积运算，实际上是一个滤波的过程。下面是卷积的数学表示：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x,y)*w(x,y) = \sum_{s=-a}^{a} \sum_{t=-b}^{b} w(s,t) f(x-s,y-t)&lt;/script&gt;

&lt;p&gt;f(x,y)是图像上点(x,y)的灰度值，w(x,y)则是卷积核，也叫滤波器。卷积实际上是提供了一个权重模板，这个模板在图像上滑动，并将中心依次与图像中每一个像素对齐，然后对这个模板覆盖的所有像素进行加权，并将结果作为这个卷积核在图像上该点的响应。如下图所示，卷积操作可以用来对图像做边缘检测，锐化，模糊等。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/convolution1.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/convolution2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;图17. 卷积操作示例&lt;/p&gt;

&lt;h5 id=&quot;section-15&quot;&gt;什么是卷积神经网络&lt;/h5&gt;
&lt;p&gt;卷积神经网络是一种特殊的、简化的深层神经网络模型，它的每个卷积层都是由多个卷积滤波器组成。它最先由lecun在LeNet[40]中提出，网络结构如下图所示。在cnn中，图像的一小部分（局部感受区域）作为层级结构的最低层的输入，信息再依次传输到不同的层，每层通过多个卷积滤波器去获得观测数据的最显著的特征。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/lenet5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;图18. Lenet5网络结构图&lt;/p&gt;

&lt;p&gt;卷积神经网络中的每一个特征提取层（卷积层）都紧跟着一个用来求局部平均与二次提取的计算层（pooling层），这种特有的两次特征提取结构使网络在识别时对输入样本有较高的畸变容忍能力。如下图所示，就是一个完整的卷积过程[21]。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/convolution6.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;图19. 一次完整的卷积过程&lt;/p&gt;

&lt;p&gt;它的特殊性体现在两点：(1)局部感受野(receptive field)，cnn的神经元间的连接是非全连接的；(2)同一层中同一个卷积滤波器的权重是共享的（即相同的）。局部感受野和权重共享这两个特点，使cnn网络结构更类似于生物神经网络，降低了网络模型的复杂度，减少了神经网络需要训练的参数的个数。&lt;/p&gt;

&lt;h5 id=&quot;section-16&quot;&gt;卷积神经网络的一些细节&lt;/h5&gt;

&lt;p&gt;接下来结合文献[25]，再讲讲卷积神经网络的一些注意点和问题。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;激励函数，要选择非线性函数，譬如tang，sigmoid，rectified liner。在CNN里，relu用得比较多，原因在于：(1)简化BP计算；(2)使学习更快。(3)避免饱和问题(saturation issues)&lt;/li&gt;
  &lt;li&gt;Pooling：其作用在于(1)对一些小的形态改变保持不变性，Invariance to small transformations；(2)拥有更大的感受域，Larger receptive fields。pooling的方式有sum or max。&lt;/li&gt;
  &lt;li&gt;Normalization：Equalizes the features maps。它的作用有：(1)  Introduces local competition between features；(2)Also helps to scale activations at each layer better for learning；(3)Empirically, seems to help a bit (1-2%) on ImageNet&lt;/li&gt;
  &lt;li&gt;训练CNN：back-propagation；stochastic gradient descent；Momentum；Classification loss，cross-entropy；Gpu实现。&lt;/li&gt;
  &lt;li&gt;预处理：Mean removal；Whitening(ZCA)&lt;/li&gt;
  &lt;li&gt;增强泛化能力：Data augmentation；Weight正则化；在网络里加入噪声，包括DropOut，DropConnect，Stochastic pooling。
    &lt;ul&gt;
      &lt;li&gt;DropOut：只在全连接层使用，随机的将全连接层的某些神经元的输出置为0。&lt;/li&gt;
      &lt;li&gt;DropConnect：也只在全连接层使用，Random binary mask on weights&lt;/li&gt;
      &lt;li&gt;Stochastic Pooling：卷积层使用。Sample location from multinomial。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;模型不work，怎么办？结合我自身的经验，learning rate初始值设置得太大，开始设置为0.01，以为很小了，但实际上0.001更合适。&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;section-17&quot;&gt;卷积神经网络在文本上的应用&lt;/h5&gt;
&lt;p&gt;卷积神经网络在image classify和image detect上得到诸多成功的应用，后文将再详细阐述。但除了图片外，它在文本分析上也取得一些成功的应用。&lt;/p&gt;

&lt;p&gt;基于CNN，可以用来做文本分类，情感分析，本体分类等[36,41,84]。传统文本分类等任务，一般基于bag of words或者基于word的特征提取，此类方法一般需要领域知识和人工特征。利用CNN做，方法也类似，但一般都是基于raw text，CNN模型的输入可以是word series，可以是word vector，还可以是单纯的字符。比起传统方法，CNN不需要过多的人工特征。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;将word series作为输入，利用CNN做文本分类。如下图所示[36]，该CNN很简单，共分四层，第一层是词向量层，doc中的每个词，都将其映射到词向量空间，假设词向量为k维，则n个词映射后，相当于生成一张n*k维的图像；第二层是卷积层，多个滤波器作用于词向量层，不同滤波器生成不同的feature map；第三层是pooling层，取每个feature map的最大值，这样操作可以处理变长文档，因为第三层输出只依赖于滤波器的个数；第四层是一个全连接的softmax层，输出是每个类目的概率。除此之外，输入层可以有两个channel，其中一个channel采用预先利用word2vec训练好的词向量，另一个channel的词向量可以通过backpropagation在训练过程中调整。这样做的结果是：在目前通用的7个分类评测任务中，有4个取得了state-of-the-art的结果，另外3个表现接近最好水平。&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/cnn_text_classify.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;图20.基于CNN的文本分类&lt;/p&gt;

    &lt;p&gt;利用cnn做文本分类，还可以考虑到词的顺序。利用传统的”bag-of-words + maxent/svm”方法，是没有考虑词之间的顺序的。文献[41]中提出两种cnn模型：seq-cnn, bow-cnn，利用这两种cnn模型，均取得state-of-the-art结果。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;将doc character作为输入，利用CNN做文本分类。文献[86]介绍了一种方法，不利用word，也不利用word vector，直接将字符系列作为模型输入，这样输入维度大大下降(相比于word)，有利于训练更复杂的卷积网络。对于中文，可以将汉字的拼音系列作为输入。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;section-18&quot;&gt;2.4 文本分类&lt;/h4&gt;
&lt;p&gt;文本分类应该是最常见的文本语义分析任务了。首先它是简单的，几乎每一个接触过nlp的同学都做过文本分类，但它又是复杂的，对一个类目标签达几百个的文本分类任务，90%以上的准确率召回率依旧是一个很困难的事情。这里说的文本分类，指的是泛文本分类，包括query分类，广告分类，page分类，用户分类等，因为即使是用户分类，实际上也是对用户所属的文本标签，用户访问的文本网页做分类。&lt;/p&gt;

&lt;p&gt;几乎所有的机器学习方法都可以用来做文本分类，常用的主要有：lr，maxent，svm等，下面介绍一下文本分类的pipeline以及注意点。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;建立分类体系。
    &lt;ul&gt;
      &lt;li&gt;分类相比于topic model或者聚类，一个显著的特点是：类目体系是确定的。而不像在聚类和LDA里，一个类被聚出来后，但这个类到底是描述什么的，或者这个类与另外的类是什么关系，这些是不确定的，这样会带来使用和优化上的困难。&lt;/li&gt;
      &lt;li&gt;一般而言，类目体系是由人工设定的。而类目体系的建立往往需要耗费很多人工研究讨论，一方面由于知识面的限制，人工建立的类目体系可能不能覆盖所有情况；另一方面，还可能存在类目之间instance数的不平衡。比较好的方法，是基于目前已有的类目体系再做一些加工，譬如ODP，FreeBase等。&lt;/li&gt;
      &lt;li&gt;还可以先用某种无监督的聚类方法，将训练文本划分到某些clusters，建立这些clusters与ODP类目体系的对应关系，然后人工review这些clusters，切分或者合并cluster，提炼name，再然后根据知识体系，建立层级的taxonomy。&lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;如果类目标签数目很多的话，我们一般会将类目标签按照一定的层次关系，建立类目树，如下图所示。那么接下来就可以利用层次分类器来做分类，先对第一层节点训练一个分类器，再对第二层训练n个分类器(n为第一层的节点个数)，依次类推。利用层次类目树，一方面单个模型更简单也更准确，另一方面可以避免类目标签之间的交叉影响，但如果上层分类有误差，误差将会向下传导。&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/taxonomy.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

        &lt;p&gt;图21. 层次类目体系&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;获取训练数据
    &lt;ul&gt;
      &lt;li&gt;一般需要人工标注训练数据。人工标注，准确率高，但标注工作量大，耗费人力。&lt;/li&gt;
      &lt;li&gt;为了减少标注代价，利用无标记的样本，提出了半监督学习(Semi-supervised Learning)，主要考虑如何利用少量的标注样本和大量的未标注样本进行训练和分类的问题。这里介绍两种常见的半监督算法，希望了解更多请参考文献[49]。
        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;Self-learning：两个样本集合，Labeled，Unlabeled。执行算法如下：&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;用Labeled样本集合，生成分类策略F&lt;/li&gt;
              &lt;li&gt;用F分类Unlabeled样本，计算误差&lt;/li&gt;
              &lt;li&gt;选取Unlabeled中误差小的子集u，加入到Labeled集合。&lt;/li&gt;
            &lt;/ul&gt;

            &lt;p&gt;接着重复上述步骤。&lt;/p&gt;

            &lt;p&gt;举一个例子：以前在做page分类器时，先对每一个类人工筛选一些特征词，然后根据这些特征词对亿级文本网页分类，再然后对每一个明确属于该类的网页提取更多的特征词，加入原有的特征词词表，再去做分类；中间再辅以一定的人工校验，这种方法做下来，效果还是不错的，更关键的是，如果发现那个类有badcase，可以人工根据badcase调整某个特征词的权重，简单粗暴又有效。&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;Co-training：其主要思想是：每次循环，从Labeled数据中训练出两个不同的分类器，然后用这两个分类器对Unlabeled中数据进行分类，把可信度最高的数据加入到Labeled中，继续循环直到U中没有数据或者达到循环最大次数。&lt;/li&gt;
          &lt;li&gt;协同训练，例如Tri-train算法：使用三个分类器.对于一个无标签样本，如果其中两个分类器的判别一致，则将该样本进行标记，并将其纳入另一个分类器的训练样本；如此重复迭代，直至所有训练样本都被标记或者三个分类器不再有变化。&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;半监督学习，随着训练不断进行，自动标记的示例中的噪音会不断积累，其负作用会越来越大。所以如term weighting工作里所述，还可以从其他用户反馈环节提取训练数据，类似于推荐中的隐式反馈。&lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;我们看一个具体的例子，在文献[45]中，twitter利用了三种方法，user-level priors（发布tweet的用户属于的领域），entity-level priors（话题，类似于微博中的#***#），url-level priors（tweet中的url）。利用上面三种数据基于一定规则获取到基本的训练数据，再通过Co-Training获取更多高质量的训练数据。上述获取到的都是正例数据，还需要负例样本。按照常见的方法，从非正例样本里随机抽取作为负例的方法，效果并不是好，文中用到了Pu-learning去获取高质量的负例样本，具体请参考文献[58]。&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/training_data_acquisition.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

        &lt;p&gt;图22.文献[45]训练数据获取流程图&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;特征提取
    &lt;ul&gt;
      &lt;li&gt;对于每条instance，运用多种文本分析方法提取特征。常见特征有：
        &lt;ul&gt;
          &lt;li&gt;分词 or 字的ngram，对词的权重打分，计算词的一些领域特征，又或者计算词向量，词的topic分布。&lt;/li&gt;
          &lt;li&gt;文本串的特征，譬如sentence vector，sentence topic等。&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;提取的特征，从取值类型看，有二值特征，浮点数特征，离线值特征。&lt;/li&gt;
      &lt;li&gt;特征的预处理包括：
        &lt;ul&gt;
          &lt;li&gt;一般来说，我们希望instance各维特征的均值为0，方差为1或者某个有边界的值。如果不是，最好将该维度上的取值做一个变换。&lt;/li&gt;
          &lt;li&gt;特征缺失值和异常值的处理也需要额外注意。&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;特征选择，下面这些指标都可以用作筛选区分度高的特征。
        &lt;ul&gt;
          &lt;li&gt;Gini-index: 一个特征的Gini-index越大，特征区分度越高。&lt;/li&gt;
          &lt;li&gt;信息增益(Information Gain)&lt;/li&gt;
          &lt;li&gt;互信息(Mutual Information)&lt;/li&gt;
          &lt;li&gt;相关系数(Correlation)&lt;/li&gt;
          &lt;li&gt;假设检验(Hypothesis Testing)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;模型训练
    &lt;ul&gt;
      &lt;li&gt;模型选择：通常来说，常用的有监督模型已经足够了，譬如lr, svm, maxent, naive-bayes，决策树等。这些基本模型之间的效果差异不大，选择合适的即可。上一小节讲到cnn时，提到深度神经网络也可以用来做文本分类。深度神经网络相比较于传统方法，特征表示能力更强，还可以自学习特征。&lt;/li&gt;
      &lt;li&gt;语言模型也可用于分类，针对不同的label，训练两个不同的语言模型p+(x|y=+1)和p-(x|y=-1)。对于一个testcase x，求解r= p+(x|y=+1)/p-(x|y=-1)*p(y=+1)/p(y=-1)，如果r&amp;gt;1，则x属于label(+1)，否则x属于label(-1)。&lt;/li&gt;
      &lt;li&gt;模型的正则化：一般来说，L1正则化有特征筛选的作用，用得相对较多，除此外，L2正则化，ElasticNet regularization(L1和L2的组合)也很常用。&lt;/li&gt;
      &lt;li&gt;对于多分类问题，可以选择one-vs-all方法，也可以选择multinomial方法。两种选择各有各的优点，主要考虑有：并行训练multiple class model更复杂；不能重新训练 a subset of topics。&lt;/li&gt;
      &lt;li&gt;model fine-tuning。借鉴文献[72]的思路(训练深度神经网络时，先无监督逐层训练参数，再有监督调优)，对于文本分类也可以采用类似思路，譬如可以先基于自提取的大规模训练数据训练一个分类模型，再利用少量的有标注训练数据对原模型做调优。下面这个式子是新的loss function，w是新模型参数，\(w^0\)是原模型参数，\(l(w,b|x_i,y_i)\)是新模型的likelihood，优化目标就是最小化”新模型参数与原模型参数的差 + 新模型的最大似然函数的负数 + 正则化项”。&lt;/li&gt;
    &lt;/ul&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;min_{w,b} \frac{\delta}{2}||w-w^0||_2^2 - \frac{1-\delta}{n}\sum_{i=1}^nl(w,b|x_i,y_i) + \lambda(\alpha||w||_1+\frac{1-\alpha}{2}||w||_2^2)&lt;/script&gt;

    &lt;ul&gt;
      &lt;li&gt;model ensemble：也称”Multi-Model System”，ensemble是提升机器学习精度的有效手段，各种竞赛的冠军队伍的是必用手段。它的基本思想，充分利用不同模型的优势，取长补短，最后综合多个模型的结果。Ensemble可以设定一个目标函数(组合多个模型)，通过训练得到多个模型的组合参数(而不是简单的累加或者多数)。譬如在做广告分类时，可以利用maxent和决策树，分别基于广告title和描述，基于广告的landing page，基于广告图片训练6个分类模型。预测时可以通过ensemble的方法组合这6个模型的输出结果。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;评测
    &lt;ul&gt;
      &lt;li&gt;评测分类任务一般参考Accuracy，recall, precision，F1-measure，micro-recall/precision，macro-recall/precision等指标。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-19&quot;&gt;3 图片语义分析&lt;/h3&gt;

&lt;h4 id=&quot;section-20&quot;&gt;3.1 图片分类&lt;/h4&gt;
&lt;p&gt;图片分类是一个最基本的图片语义分析方法。&lt;/p&gt;

&lt;h5 id=&quot;section-21&quot;&gt;基于深度学习的图片分类&lt;/h5&gt;
&lt;p&gt;传统的图片分类如下图所示，首先需要先手工提取图片特征，譬如SIFT, GIST，再经由VQ coding和Spatial pooling，最后送入传统的分类模型(例如SVM等)。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/tranditional-imageclassify.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;图23. 传统图片分类流程图&lt;/p&gt;

&lt;p&gt;传统方法里，人工特征提取是一个巨大的消耗性工作。而随着深度学习的进展，不再需要人工特征，通过深度学习自动提取特征成为一种可能。接下来主要讲述卷积神经网络在图片分类上的使用。&lt;/p&gt;

&lt;p&gt;下图是一个经典的卷积神经网络模型图，由Hinton和他的学生Alex Krizhevsky在ILSVRC(Imagenet Large Scale Visual Recognition Competition) 2012中提出。
整个网络结构包括五层卷积层和三层全连接层，网络的最前端是输入图片的原始像素点，最后端是图片的分类结果。一个完整的卷积层可能包括一层convolution，一层Rectified Linear Units，一层max-pooling，一层normalization。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/convolution7.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;图24. 卷积神经网络结构图&lt;/p&gt;

&lt;p&gt;对于每一层网络，具体的网络参数配置如下图所示。InputLayer就是输入图片层，每个输入图片都将被缩放成227*227大小，分rgb三个颜色维度输入。Layer1~ Layer5是卷积层，以Layer1为例，卷积滤波器的大小是11*11，卷积步幅为4，本层共有96个卷积滤波器，本层的输出则是96个55*55大小的图片。在Layer1，卷积滤波后，还接有ReLUs操作和max-pooling操作。Layer6~ Layer8是全连接层，相当于在五层卷积层的基础上再加上一个三层的全连接神经网络分类器。以Layer6为例，本层的神经元个数为4096个。Layer8的神经元个数为1000个，相当于训练目标的1000个图片类别。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/convolution_config.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;图25. CNN网络参数配置图&lt;/p&gt;

&lt;p&gt;基于Alex Krizhevsky提出的cnn模型，在13年末的时候，我们实现了用于广点通的图片分类和图片检索(可用于广告图片作弊判别)，下面是一些示例图。&lt;/p&gt;

&lt;p&gt;图片分类示例：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/image_classify.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;图26. 图片分类示例图&lt;/p&gt;

&lt;p&gt;图片检索示例：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/image_search1.png&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/image_search2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;图27. 图片检索示例图&lt;/p&gt;

&lt;h5 id=&quot;section-22&quot;&gt;图片分类上的最新进展&lt;/h5&gt;

&lt;p&gt;在ILSVRC 2012中，Krizhevsky基于GPU实现了上述介绍的，这个有60million参数的模型，赢得了第一名。这个工作是开创性的，它引领了接下来ILSVRC的风潮。2013年，Clarifai通过cnn模型可视化技术调整网络架构，赢得了ILSVRC。2014年，google也加入进来，它通过增加模型的层数（总共22层），让深度更深[48]，并且利用multi-scale data training，取得第一名。baidu最近通过更加”粗暴”的模型[44]，在GooLeNet的基础上，又提升了10%，top-5错误率降低至6%以下。具体结果如下图所示。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images//imagenet_result.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;图28. ImageNet Classification Result&lt;/p&gt;

&lt;p&gt;先简单分析一下”GoogLeNet”[48,51]所采用的方法：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;大大增加的网络的深度，并且去掉了最顶层的全连接层：因为全连接层（Fully Connected）几乎占据了CNN大概90%的参数，但是同时又可能带来过拟合（overfitting）的效果。&lt;/li&gt;
  &lt;li&gt;模型比以前AlexNet的模型大大缩小，并且减轻了过拟合带来的副作用。Alex模型参数是60M，GoogLeNet只有7M。&lt;/li&gt;
  &lt;li&gt;对于google的模型，目前已有开源的实现，有兴趣请点击&lt;a href=&quot;https://github.com/BVLC/caffe/tree/master/models/bvlc_googlenet&quot;&gt;Caffe+GoogLeNet&lt;/a&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;再分析一下”Deep Image by baidu[44]”所采用的方法：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Hardware/Software Co-design。baidu基于GPU，利用36个服务节点开发了一个专为深度学习运算的supercompter(名叫Minwa，敏娲)。这台supercomputer具备TB级的host memory，超强的数据交换能力，使能训练一个巨大的深层神经网络成为可能。&lt;/p&gt;

    &lt;p&gt;而要训练如此巨大的神经网络，除了硬件强大外，还需要高效的并行计算框架。通常而言，都要从data-parallelism和model-data parallelism两方面考虑。&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;data-parallelism：训练数据被分成N份。每轮迭代里，各个GPU基于各自的训练数据计算梯度，最后累加所有梯度数据并广播到所有GPU。&lt;/li&gt;
      &lt;li&gt;model-data parallelism：考虑到卷积层参数较少但消耗计算量，而全连接层参数相对比较多。所以卷积层参数以local copy的形式被每个GPU所持有，而全连接层的参数则被划分到各个CPU。每轮迭代里，卷积层计算可以由各个GPU独立完成，全连接层计算需要由所有GPU配合完成，具体方法请参考[46]。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Data augmentation。训练一个如此巨大的神经网络(100billion个参数)，如果没有充分的训练数据，模型将很大可能陷入过拟合，所以需要采用众多data augmentation方法增加训练数据，例如：剪裁，不同大小，调亮度，饱和度，对比度，偏色等(color casting, vignetting, lens distortion, rotation, flipping, cropping)。举个例子，一个彩色图片，增减某个颜色通道的intensity值，就可以生成多张图片，但这些图片和原图的类目是一致的，相当于增加了训练数据。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Multi-scale training：训练不同输入图片尺度下(例如512*512，256*256)的多个模型，最后ensemble多个模型的输出结果。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;image2textimage2sentence&quot;&gt;3.2 Image2text，Image2sentence&lt;/h4&gt;
&lt;p&gt;上面讲述的图片分类对图片语义的理解比较粗粒度，那么我们会想，是否可以将图片直接转化为一堆词语或者一段文本来描述。转化到文本后，我们积累相对深的文本处理技术就都可以被利用起来。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Image2text&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;首先介绍一种朴素的基于卷积神经网络的image to text方法。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;首先它利用深度卷积神经网络和深度自动编码器提取图片的多层特征，并据此提取图片的visual word，建立倒排索引，产生一种有效而准确的图片搜索方法。&lt;/li&gt;
  &lt;li&gt;再充分利用大量的互联网资源，预先对大量种子图片做语义分析，然后利用相似图片搜索，根据相似种子图片的语义推导出新图片的语义。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;其中种子图片，就是可以覆盖目前所有待研究图片的行业，但较容易分析语义的图片集。这种方法产生了更加丰富而细粒度的语义表征结果。虽说简单，但效果仍然不错，方法的关键在于种子图片。利用比较好的种子图片(例如paipai数据)，简单的方法也可以work得不错。下图是该方法的效果图。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/image_semantic.png&quot; alt=&quot;&quot; /&gt;  &lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/image_semantic2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;图29. 图片语义tag标注示例图&lt;/p&gt;

&lt;p&gt;上面的baseline方法，在训练数据优质且充分的情况下，可以取得很不错的图片tag提取效果，而且应用也非常广泛。但上面的方法非常依赖于训练数据，且不善于发现训练数据之外的世界。&lt;/p&gt;

&lt;p&gt;另一个直观的想法，是否可以通过word embedding建立image与text的联系[26]。例如，可以先利用CNN训练一个图片分类器。每个类目label可以通过word2vec映射到一个embedding表示。对于一个新图片，先进行分类，然后对top-n类目label所对应的embedding按照权重(这里指这个类目所属的概率)相加，得到这个图片的embedding描述，然后再在word embedding空间里寻找与图片embedding最相关的words。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Image detection&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;接下来再介绍下image detection。下图是一个image detection的示例，相比于图片分类，提取到信息将更加丰富。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/image_detection.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;图30. 图片detection示例&lt;/p&gt;

&lt;p&gt;目前最先进的detection方法应该是Region-based CNN(简称R-CNN)[75]，是由Jeff Donahue和Ross Girshick提出的。R-CNN的具体想法是，将detection分为寻找object和识别object两个过程。在第一步寻找object，可以利用很多region detection算法，譬如selective search[76]，CPMC，objectness等，利用很多底层特征，譬如图像中的色块，图像中的边界信息。第二步识别object，就可以利用”CNN+SVM”来做分类识别。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/r-cnn.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;图31. Image detection系统框图&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;给定一张图片，利用selective search方法[76]来产生2000个候选窗口。&lt;/li&gt;
  &lt;li&gt;然后利用CNN进行对每一个候选窗口提取特征(取全连接层的倒数第一层)，特征长度为4096。&lt;/li&gt;
  &lt;li&gt;最后用SVM分类器对这些特征进行分类（每一个目标类别一个SVM分类器），SVM的分类器的参数个数为：4096*N，其中N为目标的类别个数，所以比较容易扩展目标类别数。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这里有R-CNN的实现，请点击&lt;a href=&quot;https://github.com/rbgirshick/rcnn&quot;&gt;rcnn code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Image2sentence&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;那能否通过深度学习方法，直接根据image产生sentence呢？我们先看一组实际效果，如下图所示(copy from 文献[43])。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/image2sentence_example.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;图32. image2sentence示例图&lt;/p&gt;

&lt;p&gt;关于这个方向，最近一年取得了比较大的突破，工业界(Baidu[77]，Google[43]，Microsoft[80,81]等)和学术界(Stanford[35]，Borkeley[79]，UML[19]，Toronto[78]等)都发表了一系列论文。&lt;/p&gt;

&lt;p&gt;简单归纳一下，对这个问题，主要有两种解决思路：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Pipeline方法。这个思路相对直观一点，先学习到image中visual object对应的word(如上一节image detection所述)，再加上language model，就可以生成sentence。这种方法各个模块可以独立调试，相对来说，更灵活一点。如下图所示，这是microsoft的一个工作[81]，它分为三步：(1)利用上一节提到的思路detect words；(2)基于language model(RNN or LSTM)产生句子；(3)利用相关性模型对句子打分排序。&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/AIC.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;图33. “pipeline” image captioning&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;End-to-end方法，即通过一个模型直接将image转换到sentence。google基于CNN+RNN开发了一个Image Caption Generator[43]。这个工作主要受到了基于RNN的机器翻译[27][42]的启发。在机器翻译中，”encoder” RNN读取源语言的句子，将其变换到一个固定长度的向量表示，然后”decoder” RNN将向量表示作为隐层初始值，产生目标语言的句子。&lt;/p&gt;

    &lt;p&gt;那么一个直观的想法是，能否复用上面的框架，考虑到CNN在图片特征提取方面的成功应用，将encoder RNN替换成CNN，先利用CNN将图片转换到一个向量表示，再利用RNN将其转换到sentence。可以通过图片分类提前训练好CNN模型，将CNN最后一个隐藏层作为encoder RNN的输入，从而产生句子描述。如下图所示。&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/cnn_rnn.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/cnn_lstm.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;图34. “CNN+LSTM” Image Caption Generator&lt;/p&gt;

    &lt;p&gt;Li-Feifei团队在文献[35]也提到一种image2sentence方法，如下图所示。与google的做法类似，图片的CNN特征作为RNN的输入。&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/cnn-rnn.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;图35. “CNN+RNN”生成图片描述&lt;/p&gt;

    &lt;p&gt;此方法有开源实现，有兴趣请参考：&lt;a href=&quot;https://github.com/karpathy/neuraltalk&quot;&gt;neuraltalk&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;tricks&quot;&gt;3.3 训练深度神经网络的tricks&lt;/h4&gt;
&lt;p&gt;考虑到图片语义分析的方法大部分都是基于深度学习的，Hinton的学生Ilya Sutskever写了一篇深度学习的综述文章[47]，其中提到了一些训练深度神经网络的tricks，整理如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;保证训练数据的质量&lt;/li&gt;
  &lt;li&gt;使训练数据各维度数值的均值为0，方差为一个比较小的值&lt;/li&gt;
  &lt;li&gt;训练时使用minbatch，但不要设得过大，在合理有效的情况下，越小越好。&lt;/li&gt;
  &lt;li&gt;梯度归一化，将梯度值除于minbatch size。&lt;/li&gt;
  &lt;li&gt;设置一个正常的learning rate，validation无提升后，则将原learning rate除于5继续&lt;/li&gt;
  &lt;li&gt;模型参数随机初始化。如果是深层神经网络，不要设置过小的random weights。&lt;/li&gt;
  &lt;li&gt;如果是在训练RNN or LSTM，对梯度设置一个限值，不能超过15 or 5。&lt;/li&gt;
  &lt;li&gt;注意检查梯度计算的正确性&lt;/li&gt;
  &lt;li&gt;如果是训练LSTM，initialize the biases of the forget gates of the LSTMs to large values&lt;/li&gt;
  &lt;li&gt;Data augmentation很实用。&lt;/li&gt;
  &lt;li&gt;Dropout在训练时很有效，不过记得测试时关掉Dropout。&lt;/li&gt;
  &lt;li&gt;Ensembling。训练多个神经网络，最后计算它们的预测值的平均值。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-23&quot;&gt;4 总结&lt;/h3&gt;

&lt;h4 id=&quot;section-24&quot;&gt;4.1 语义分析方法在实际业务中的使用&lt;/h4&gt;
&lt;p&gt;前面讲述了很多语义分析方法，接下来我们看看如何利用这些方法帮忙我们的实际业务，这里举一个例子，用户广告的语义匹配。&lt;/p&gt;

&lt;p&gt;在广点通系统中，用户与广告的关联是通过定向条件来匹配的，譬如某些广告定向到”北京+男性”，那么当”北京+男性”的用户来到时，所有符合定向的广告就将被检索出，再按照”ecpm*quality”排序，将得分最高的展示给用户。但是凭借一些人口属性，用户与广告之间的匹配并不精确，做不到”广告就是想用户所想”，所以用户和广告的语义分析就将派上用场了，可以从这样两方面来说明：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;特征提取。基于上面介绍的方法，提取用户和广告的语义特征。
    &lt;ul&gt;
      &lt;li&gt;用户语义特征。可以从用户的搜索，购物，点击，阅读记录中发现用户兴趣。考虑到最终的用户描述都是文本，那么文本topic分析，文本分类，文本keyword提取，文本核心term提取都可以运用起来，分析出用户的语义属性，还可以利用矩阵分解和文本分类找到相似用户群。&lt;/li&gt;
      &lt;li&gt;广告语义特征。在广点通里，广告可以从两个维度来描述，一方面是文本，包括广告title和landing page，另一方面是广告展示图片。利用文本和图片的语义分析方法，我们可以提取出广告的topic，类目，keyword，tag描述。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;语义匹配。提取到相应的语义特征之后，怎么用于改善匹配呢？
    &lt;ul&gt;
      &lt;li&gt;用户-广告的语义检索。基于keyword、类目以及topic，对广告建立相应的倒排索引，直接用于广告检索。&lt;/li&gt;
      &lt;li&gt;用户-广告的语义特征。分别提取用户和广告的语义特征，用于计算用户-广告的relevance，pctr，pcvr，达到精确排序。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;future&quot;&gt;4.2 Future&lt;/h4&gt;
&lt;p&gt;对于文本和图片的语义分析，可以看到：最近几年，在某些任务上，基于深度学习的方法逐渐赶上并超过了传统方法的效果。但目前为止，对于深度学习的发掘才刚刚开始，比较惊艳的神经网络方法，也只有有限几种，譬如CNN，RNN，RBM等。&lt;/p&gt;

&lt;p&gt;上文只是介绍了我们在工作中实践过的几个小点，还有更多方法需要我们去挖掘：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Video。Learn about 3D structure from motion。如文献[19]所示，研究将视频也转换到自然语言。&lt;/li&gt;
  &lt;li&gt;Deep Learning + Structured Prediction，用于syntactic representation。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;section-25&quot;&gt;4.3 总结&lt;/h4&gt;

&lt;p&gt;上文主要从文本、图片这两方面讲述了语义分析的一些方法，并结合个人经验做了一点总结。&lt;/p&gt;

&lt;p&gt;原本想写得更全面一些，但写的时候才发现上面所述的只是沧海一粟，后面还有更多语义分析的内容之后再更新。另外为避免看到大篇理论就头痛，文中尽可能不出现复杂的公式和理论推导。如果有兴趣，可以进一步阅读参考文献，获得更深的理解。谢谢。&lt;/p&gt;

&lt;h3 id=&quot;section-26&quot;&gt;5 参考文献&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;http://comminfo.rutgers.edu/~muresan/IR/Docs/Articles/ipmSalton1988.pdf&quot;&gt;Term-weighting approaches in automatic text retrieval，Gerard Salton et.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.sandia.gov/~tgkolda/pubs/pubfiles/ornl-tm-13756.pdf&quot;&gt;New term weighting formulas for the vector space method in information retrieval&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf&quot;&gt;A neural probabilistic language model 2003&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://licstar.net/archives/328&quot;&gt;Deep Learning in NLP-词向量和语言模型&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf&quot;&gt;Recurrent neural network based language models&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Statistical Language Models based on Neural Networks，mikolov博士论文&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.fit.vutbr.cz/~imikolov/rnnlm/&quot;&gt;Rnnlm library&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://brown.cl.uni-heidelberg.de/~sourjiko/NER_Literatur/survey.pdf&quot;&gt;A survey of named entity recognition and classification&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.aclweb.org/anthology/D13-1061&quot;&gt;Deep learning for Chinese word segmentation and POS tagging&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://aclweb.org/anthology/P14-1028&quot;&gt;Max-margin tensor neural network for chinese word segmentation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cogsci.ucsd.edu/~ajyu/Teaching/Cogs202_sp12/Readings/hinton86.pdf&quot;&gt;Learning distributed representations of concepts&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cs.colorado.edu/~jbg/docs/2014_book_chapter_care_and_feeding.pdf&quot;&gt;Care and Feeding of Topic Models: Problems, Diagnostics, and Improvements&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://arxiv.org/abs/1412.1576&quot;&gt;LightLda&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://code.google.com/p/word2vec/&quot;&gt;word2vec&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://arxiv.org/pdf/1301.3781v3.pdf&quot;&gt;Efficient Estimation of Word Representations in Vector Space&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://techblog.youdao.com/?p=915&quot;&gt;Deep Learning实战之word2vec&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://suanfazu.com/t/word2vec-zhong-de-shu-xue-yuan-li-xiang-jie-duo-tu-wifixia-yue-du/178&quot;&gt;word2vec中的数学原理详解&lt;/a&gt; &lt;a href=&quot;http://blog.csdn.net/itplus/article/details/37969519&quot;&gt;出处2&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://52opencourse.com/111/%E6%96%AF%E5%9D%A6%E7%A6%8F%E5%A4%A7%E5%AD%A6%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%AC%E5%9B%9B%E8%AF%BE-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88language-modeling%EF%BC%89&quot;&gt;斯坦福课程-语言模型&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://arxiv.org/abs/1412.4729&quot;&gt;Translating Videos to Natural Language Using Deep Recurrent Neural Networks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://arxiv.org/pdf/1405.4053v2.pdf&quot;&gt;Distributed Representations of Sentences and Documents&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/zouxy09/article/details/8781543&quot;&gt;Convolutional Neural Networks卷积神经网络&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://research.microsoft.com/en-us/news/features/spp-102914.aspx&quot;&gt;A New, Deep-Learning Take on Image Recognition&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://arxiv.org/pdf/1406.4729v1.pdf&quot;&gt;Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.toptal.com/machine-learning/an-introduction-to-deep-learning-from-perceptrons-to-deep-networks&quot;&gt;A Deep Learning Tutorial: From Perceptrons to Deep Networks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://cs.nyu.edu/~fergus/presentations/nips2013_final.pdf&quot;&gt;Deep Learning for Computer Vision&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://arxiv.org/pdf/1312.5650.pdf&quot;&gt;Zero-shot leanring by convex combination of semantic embeddings&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://arxiv.org/pdf/1409.3215v3.pdf&quot;&gt;Sequence to sequence learning with neural network&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://arxiv.org/pdf/1309.4168.pdf&quot;&gt;Exploting similarities among language for machine translation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Grammar as Foreign Language Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, Geoffrey Hinton, arXiv 2014&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://ceur-ws.org/Vol-1204/papers/paper_4.pdf&quot;&gt;Deep Semantic Embedding&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;张家俊. DNN Applications in NLP&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://cl.naist.jp/~kevinduh/notes/cwmt14tutorial.pdf&quot;&gt;Deep learning for natural language processing and machine translation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;&quot;&gt;Distributed Representations for Semantic Matching&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;distributed_representation_nlp&lt;/li&gt;
  &lt;li&gt;Deep Visual-Semantic Alignments for Generating Image Descriptions&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://arxiv.org/pdf/1408.5882v2.pdf&quot;&gt;Convolutional Neural Networks for Sentence Classification&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://ml.nec-labs.com/senna&quot;&gt;Senna&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://arxiv.org/pdf/1409.0575v1.pdf&quot;&gt;ImageNet Large Scale Visual Recognition Challenge&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Krizhevsky A, Sutskever I, Hinton G E. ImageNet Classification with Deep Convolutional Neural Networks&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://turing.iimas.unam.mx/~elena/CompVis/Lecun98.pdf&quot;&gt;Gradient-Based Learning Applied to Document Recognition&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Effetive use of word order for text categorization with convolutional neural network，Rie Johnson&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://arxiv.org/pdf/1406.1078.pdf&quot;&gt;Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://arxiv.org/pdf/1411.4555v1.pdf&quot;&gt;Show and Tell: A Neural Image Caption Generator&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://arxiv.org/ftp/arxiv/papers/1501/1501.02876.pdf&quot;&gt;Deep Image: Scaling up Image Recognition&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Large-Scale High-Precision Topic Modeling on Twitter&lt;/li&gt;
  &lt;li&gt;A. Krizhevsky. One weird trick for parallelizing convolutional neural networks. arXiv:1404.5997, 2014&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://yyue.blogspot.com/2015/01/a-brief-overview-of-deep-learning.html&quot;&gt;A Brief Overview of Deep Learning&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Going deeper with convolutions. Christian Szegedy. Google Inc. &lt;a href=&quot;http://www.gageet.com/2014/09203.php&quot;&gt;阅读笔记&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Long Short-Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://pages.cs.wisc.edu/~jerryzhu/pub/sslicml07.pdf&quot;&gt;Semi-Supervised Learning Tutorial&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;http://www.zhihu.com/question/24904450&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://arxiv.org/pdf/1402.1128.pdf&quot;&gt;LONG SHORT-TERM MEMORY BASED RECURRENT NEURAL NETWORK ARCHITECTURES FOR LARGE VOCABULARY SPEECH RECOGNITION&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.248.4448&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;LSTM Neural Networks for Language Modeling&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf&quot;&gt;LONG SHORT-TERM MEMORY&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Bengio, Y., Simard, P., Frasconi, P., “Learning long-term dependencies with gradient descent is difficult” IEEE Transactions on Neural Networks 5 (1994), pp. 157–166&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.sravi.org/pubs/fastlda-kdd2014.pdf&quot;&gt;AliasLDA&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;www.umiacs.umd.edu/~resnik/pubs/LAMP-TR-153.pdf&quot;&gt;Gibbs sampling for the uninitiated&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.eecs.tufts.edu/~noto/pub/kdd08/elkan.kdd08.poster.pdf&quot;&gt;Learning classifiers from only positive and unlabeled data&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://cda.ornl.gov/publications/ICMLA06.pdf&quot;&gt;TF-ICF: A New Term Weighting Scheme for Clustering Dynamic Data Streams&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.flickering.cn/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1/2014/06/%E3%80%90lda%E6%95%B0%E5%AD%A6%E5%85%AB%E5%8D%A6%E3%80%91%E7%A5%9E%E5%A5%87%E7%9A%84gamma%E5%87%BD%E6%95%B0/&quot;&gt;LDA数学八卦&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.aclweb.org/anthology/W06-0132&quot;&gt;Chinese Word Segmentation and Named Entity Recognition Based on Conditional Random Fields Models&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://repository.upenn.edu/cgi/viewcontent.cgi?article=1162&amp;amp;context=cis_papers&quot;&gt;Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://scholarworks.umass.edu/cgi/viewcontent.cgi?article=1091&amp;amp;context=cs_faculty_pubs&quot;&gt;Chinese Segmentation and New Word Detection using Conditional Random Fields&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.arbylon.net/publications/text-est.pdf&quot;&gt;Gregor Heinrich. Parameter estimation for text analysis&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://km.oa.com/group/14352/articles/show/213192&quot;&gt;Peacock：大规模主题模型及其在腾讯业务中的应用&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;L. Yao, D. Mimno, and A. McCallum. Efficient methods for topic model inference on streaming document collections. In KDD, 2009.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.jmlr.org/papers/volume10/newman09a/newman09a.pdf&quot;&gt;David Newman. Distributed Algorithms for Topic Models&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.flickering.cn/nlp/2014/07/lda工程实践之算法篇-1算法实现正确性验证/&quot;&gt;Xuemin. LDA工程实践之算法篇&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cs.unm.edu/~pdevineni/papers/Lott.pdf&quot;&gt;Brian Lott. Survey of Keyword Extraction Techniques&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Yi Wang, Xuemin Zhao, Zhenlong Sun, Hao Yan, Lifeng Wang, Zhihui Jin, Liubin Wang, Yang Gao, Ching Law, and Jia Zeng. Peacock: Learning Long-Tail Topic Features for Industrial Applications. TIST’2015.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://nlp.csai.tsinghua.edu.cn/~lzy/publications/phd_thesis.pdf&quot;&gt;刘知远. 基于文档主题结构的关键词抽取方法研究&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cs.toronto.edu/~hinton/science.pdf&quot;&gt;Hinton. Reducing the Dimensionality of Data with Neural Networks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://dl.acm.org/citation.cfm?id=2396863&quot;&gt;Samaneh Moghaddam. On the design of LDA models for aspect-based opinion mining&lt;/a&gt;；&lt;/li&gt;
  &lt;li&gt;The FLDA model for aspect-based opinion mining: addressing the cold start problem&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cs.berkeley.edu/~rbg/papers/r-cnn-cvpr.pdf&quot;&gt;Ross Girshick et. Rich feature hierarchies for accurate object detection and semantic segmentation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;J. Uijlings, K. van de Sande, T. Gevers, and A. Smeulders. Selective search for object recognition. IJCV, 2013.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://arxiv.org/abs/1410.1090&quot;&gt;Baidu/UCLA: Explain Images with Multimodal Recurrent Neural Networks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://arxiv.org/abs/1411.2539&quot;&gt;Toronto: Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://arxiv.org/abs/1411.4389&quot;&gt;Berkeley: Long-term Recurrent Convolutional Networks for Visual Recognition and Description&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://arxiv.org/abs/1411.5654&quot;&gt;Xinlei Chen et. Learning a Recurrent Visual Representation for Image Caption Generation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://arxiv.org/pdf/1411.4952v2&quot;&gt;Hao Fang et. From Captions to Visual Concepts and Back&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cs.toronto.edu/~nitish/uai13.pdf&quot;&gt;Modeling Documents with a Deep Boltzmann Machine&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://nikhilbuduma.com/2015/01/11/a-deep-dive-into-recurrent-neural-networks/&quot;&gt;A Deep Dive into Recurrent Neural Nets&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://arxiv.org/abs/1502.01710?utm_source=dlvr.it&amp;amp;utm_medium=tumblr&quot;&gt;Xiang zhang et. Text Understanding from Scratch&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Sun, 01 Feb 2015 00:00:00 +0800</pubDate>
        <link>http://yourdomain.com/2015/02/01/semantic_analysis_method.html</link>
        <guid isPermaLink="true">http://yourdomain.com/2015/02/01/semantic_analysis_method.html</guid>
        
        <category>machine learning</category>
        
        <category>nlp</category>
        
        
      </item>
    
      <item>
        <title>deep learning的坑</title>
        <description>&lt;h2 id=&quot;deep-learning&quot;&gt;deep learning的坑&lt;/h2&gt;

&lt;p&gt;转自：&lt;a href=&quot;http://www.zhihu.com/question/27608272/answer/37318565&quot;&gt;知乎问答&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;section&quot;&gt;深度学习的4个研究点&lt;/h3&gt;
&lt;p&gt;要说深度学习还有什么坑，就要看看目前的深度学习都从哪些方面去研究。个人觉得当前深度学习领域的学术研究可以包含四部分：优化（Optimization），泛化（Generalization），表达（Representation）以及应用（Applications）。除了应用（Applications）之外每个部分又可以分成实践和理论两个方面。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;优化（Optimization）：深度学习的问题最后似乎总能变成优化问题，这个时候数值优化的方法就变得尤其重要。&lt;/p&gt;

    &lt;p&gt;从实践方面来说，现在最为推崇的方法依旧是随机梯度递减，这样一个极其简单的方法以其强悍的稳定性深受广大研究者的喜爱，而不同的人还会结合动量（momentum）、伪牛顿方法（Pseudo-Newton）以及自动步长等各种技巧。此外，深度学习模型优化过程的并行化也是一个非常热的点，近年在分布式系统的会议上相关论文也逐渐增多。&lt;/p&gt;

    &lt;p&gt;在理论方面，目前研究的比较清楚的还是凸优化（Convex Optimization），而对于非凸问题的理论还严重空缺，然而深度学习大多数有效的方法都是非凸的。现在有一些对深度学习常用模型及其目标函数的特性研究，期待能够发现非凸问题中局部最优解的相关规律。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;泛化（Generalization）：一个模型的泛化能力是指它在训练数据集上的误差是否能够接近所有可能测试数据误差的均值。泛化误差大致可以理解成测试数据集误差和训练数据集误差之差。在深度学习领域变流行之前，如何控制泛化误差一直是机器学习领域的主流问题。&lt;/p&gt;

    &lt;p&gt;从实践方面来说，之前许多人担心的深度神经网络泛化能力较差的问题，在现实使用中并没有表现得很明显。这一方面源于大数据时代样本巨大的数量，另一方面近年出现了一些新的在实践上比较有效的控制泛化误差（Regularization）的方法，比如Dropout和DropConnect，以及非常有效的数据扩增（Data Agumentation）技术。是否还有其它实践中会比较有效的泛化误差控制方法一直是研究者们的好奇点，比如是否可以通过博弈法避免过拟合，以及是否可以利用无标记（Unlabeled）样本来辅助泛化误差的控制。&lt;/p&gt;

    &lt;p&gt;从理论方面来说，深度学习的有效性使得PAC学习（Probably Approximately Correct Learning）相关的理论倍受质疑。这些理论无一例外地属于“上界的上界”的一个证明过程，而其本质无外乎各种集中不等式（Concentration Inequality）和复杂性度量（Complexity Measurement）的变种，因此它对深度学习模型有相当不切实际的估计。这不应该是泛函理论已经较为发达的当下出现的状况，因此下一步如何能够从理论上分析深度学习模型的泛化能力也会是一个有趣的问题。而这个研究可能还会牵涉表达（Representation，见下）的一些理论。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;表达（Representation）：这方面主要指的是深度学习模型和它要解决的问题之间的关系，比如给出一个设计好的深度学习模型，它适合表达什么样的问题，以及给定一个问题是否存在一个可以进行表达的深度学习模型。&lt;/p&gt;

    &lt;p&gt;这方面的实践主要是两个主流，一方面那些笃信无监督学习（Unsupervised Learning）可行性的研究者们一直在寻找更好的无监督学习目标及其评价方法，以使得机器能够自主进行表达学习变得可能。这实际上包括了受限波尔兹曼模型（Restricted Boltzmann Machine），稀疏编码（Sparse Coding）和自编码器（Auto-encoder）等。另一方面，面对实际问题的科学家们一直在凭借直觉设计深度学习模型的结构来解决这些问题。这方面出现了许多成功的例子，比如用于视觉和语音识别的卷积神经网络（Convolutional Neural Network），以及能够进行自我演绎的深度回归神经网络（Recurrent Neural Network）和会自主玩游戏的深度强化学习（Reinforcement Learning）模型。绝大多数的深度学习研究者都集中在这方面，而这些也恰恰能够带来最大的学术影响力。&lt;/p&gt;

    &lt;p&gt;然而，有关表达（Representation）的理论，除了从认知心理学和神经科学借用的一些启发之外，几乎是空白。这主要是因为是否能够存在表达的理论实际上依赖于具体的问题，而面对具体问题的时候目前唯一能做的事情就是去类比现实存在的智能体（人类）是如何解决这一问题的，并设计模型来将它归约为学习算法。我直觉上认为，终极的表达理论就像是拉普拉斯幽灵（Laplace’s Demon）一样，如果存在它便无所不知，也因此它的存在会产生矛盾，使得这一理论实际上只能无限逼近。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;应用（Applications）：深度学习的发展伴随着它对其它领域的革命过程。在过去的数年中，深度学习的应用能力几乎是一种“敢想就能成”的状态。这当然得益于现今各行各业丰富的数据集以及计算机计算能力的提升，同时也要归功于过去近三十年的领域经验。未来，深度学习将继续解决各种识别（Recognition）相关的问题，比如视觉（图像分类、分割，计算摄影学），语音（语音识别），自然语言（文本理解）；同时，在能够演绎（Ability to Act）的方面如图像文字描述、语音合成、自动翻译、段落总结等也会逐渐出现突破，更可能协助寻找NP难（NP-Hard）问题在限定输入集之后的可行算法。所有的这些都可能是非常好的研究点，能够带来经济和学术双重的利益。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;nlp&quot;&gt;深度学习与NLP&lt;/h3&gt;

&lt;p&gt;从应用角度来看,NLP是一个重要阵地，Nlp目前还没有像别的领域那样被dl席卷。成效明显的有MT和language model。Speech也很明显但那更偏向信号处理而非语言分析。&lt;/p&gt;

&lt;p&gt;DL的representation很吸引人，但是在constituent parsing上，Dan Klein撰文分析认为很难从word embedding得到好处。目前我听到的一些讨论认为这是由于人类对语言现象的解释比较好（向对于图像跟声波），也在此理解上搭了很丰富的理论框架，neural net这种自动学的feature未必占优。&lt;/p&gt;

&lt;p&gt;学者对此态度也明显份分阵地，比如Noah Smith经常调侃之，比如称之为derp learning。Chris manning 则大力支持，Michael Jordan 认为现在的成果不显著但此方向值得做。Ed Hovy则认为5年内Deep Learning 将全盘取胜，现在必须做。&lt;/p&gt;

&lt;p&gt;我比较倾向于Manning跟Hovy，他们对领域了解很透彻，下的判断是根据长年在领域里面浸淫的经验。两位都是功力深厚的linguistic兼cs专家，“见得多了”，“早已看穿了一切”。Noah则非常执着于神经网络的可解释性，认为其无法理解，但是某种程度的解释在未来应该是有可能的。所以要跳坑请尽早。&lt;/p&gt;
</description>
        <pubDate>Sat, 31 Jan 2015 00:00:00 +0800</pubDate>
        <link>http://yourdomain.com/2015/01/31/deep-learning.html</link>
        <guid isPermaLink="true">http://yourdomain.com/2015/01/31/deep-learning.html</guid>
        
        
      </item>
    
      <item>
        <title>最优化方法</title>
        <description>&lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;h1 id=&quot;section&quot;&gt;最优化方法&lt;/h1&gt;

&lt;p&gt;一般我们接触到的最优化分为两类：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;无约束最优化&lt;/li&gt;
  &lt;li&gt;有约束最优化&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-1&quot;&gt;无约束最优化&lt;/h2&gt;

&lt;p&gt;通常对于无约束最优化，首先要判断是否为凸函数。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/unconstrained-optimization-one&quot;&gt;无约束最优化&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.cnblogs.com/daniel-D/p/3377840.html&quot;&gt;机器学习中导数最优化方法&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://cseweb.ucsd.edu/~elkan/250B/logreg.pdf&quot;&gt;最大似然、逻辑回归和随机梯度训练&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;section-2&quot;&gt;梯度下降法&lt;/h3&gt;

&lt;h3 id=&quot;section-3&quot;&gt;牛顿法&lt;/h3&gt;

&lt;h3 id=&quot;section-4&quot;&gt;拟牛顿法&lt;/h3&gt;

&lt;h3 id=&quot;section-5&quot;&gt;共轭梯度法&lt;/h3&gt;

&lt;h2 id=&quot;section-6&quot;&gt;有约束最优化&lt;/h2&gt;

&lt;p&gt;一般采用拉格朗日方程，kkt，对偶问题求解。&lt;a href=&quot;http://www.moozhi.com/topic/show/54a8a261c555c08b3d59d996&quot;&gt;关于拉格朗日乘子法与KKT条件&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;譬如svm里，最大化几何间隔 max y(wx+b)/||w||&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://blog.csdn.net/v_july_v/article/details/7624837&quot;&gt;支持向量机&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;首先写出cost function：min [ 1/2*w^2 + max(0,1-y(wx+b)) ]&lt;/p&gt;

&lt;p&gt;可以看出，这是一个有约束的问题，那么就可以用到”拉普拉斯+KKT+对偶”来求解了。&lt;/p&gt;

&lt;h2 id=&quot;section-7&quot;&gt;最优化算法的并行化&lt;/h2&gt;

&lt;h3 id=&quot;logistic-regression&quot;&gt;Logistic Regression&lt;/h3&gt;

&lt;p&gt;这里主要以Logistic regression为例，讲一讲最优化算法的并行化实现。&lt;/p&gt;

&lt;p&gt;先看一下Logistic regression的损失函数：&lt;/p&gt;

&lt;p&gt;Logistic函数（或称为Sigmoid函数）为：
&lt;script type=&quot;math/tex&quot;&gt;sigmoid(z)=f(z)=\frac{1}{1+e^{-z}}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;sigmoid函数的导数为：
&lt;script type=&quot;math/tex&quot;&gt;\nabla f(z)=f(z)(1-f(z))&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;对于线性回归来说，其分类函数为：
&lt;script type=&quot;math/tex&quot;&gt;h(x)=w_0+\sum_{i=1}^p{w_ix_i}=w^Tx&lt;/script&gt;
其中x是特征向量，w是特征权重向量，p是特征向量维度。&lt;/p&gt;

&lt;p&gt;逻辑回归本质上是一个被logistic函数归一化后的线性回归，即在特征到结果的映射中加入了一层sigmoid函数映射。相比于线性回归，模型输出取值范围为[0，1]，&lt;/p&gt;

&lt;p&gt;如果y的取值是0或1，定义事件发生的条件概率为：
&lt;script type=&quot;math/tex&quot;&gt;P(y=1|x)=\pi(x)=\frac{1}{1+exp(-h(x))}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;定义事件不发生的条件概率为：
&lt;script type=&quot;math/tex&quot;&gt;P(y=0|x)=1-P(y=1|x)=\frac{1}{1+exp(h(x))}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;假设有n个观测样本，分别为：(\(x_1\),\(y_1\))，(\(x_2\),\(y_2\)) … (\(x_n\),\(y_n\))。&lt;/p&gt;

&lt;p&gt;得到一个观测值(\(x_i\),\(y_i\))的概率为：
&lt;script type=&quot;math/tex&quot;&gt;P(y_i)=p_i^{y_i}(1-p_i)^{1-y_i}&lt;/script&gt;  其中\(p_i=P(y_i=1|x_i)=\pi(x_i)\)&lt;/p&gt;

&lt;p&gt;由于各项观测独立，所以它们的联合分布可以表示为各边际分布的乘积：
&lt;script type=&quot;math/tex&quot;&gt;l(w)=\prod_{i=1}^n{p_i^{y_i}(1-p_i)^{1-y_i}}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;对上述函数取对数，根据最大似然估计，得到最优化目标为：
&lt;script type=&quot;math/tex&quot;&gt;\max{L(w)}=\max{log[l(w)]}=\max{\sum_{i=1}^n{y_i*log[\pi(x_i)] + (1-y_i)*log[1-\pi(x_i)]}}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;对L(w)求导得：
&lt;script type=&quot;math/tex&quot;&gt;\nabla L(w) = \sum_{i=1}^n{(\pi(x_i)-y_i)x_i}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;而如果y的取值是1或-1，则最优化目标为：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\max{L(w)}=\max{\sum_{i=1}^n{-log[1+exp(-y_iw^Tx_i)]}}&lt;/script&gt;

&lt;p&gt;加上正则项后则是：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_w \frac{1}{2}||w||^2+C\sum_{i=1}^{n}log[1+exp(-y_iw^Tx_i)]&lt;/script&gt;

&lt;h3 id=&quot;lrmapreduce&quot;&gt;LR的MapReduce并行&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/lbfgs.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/lbfgs_two_loops.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/lbfgs_two_loops_vf.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/distributed-liblinear/&quot;&gt;Distributed LIBLINEAR&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://nips.cc/Conferences/2014/Program/event.php?ID=4831&quot;&gt;Large scale learning spotlights&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;loss-function&quot;&gt;Loss Function&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://web.mit.edu/lrosasco/www/publications/loss.pdf&quot;&gt;loss.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/JohnLangford/vowpal_wabbit/wiki/Loss-functions&quot;&gt;vowpal_wabbit Loss-functions &lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Loss_function&quot;&gt;Loss function wiki&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://image.diku.dk/shark/sphinx_pages/build/html/rest_sources/tutorials/concepts/library_design/losses.html&quot;&gt;shark loss function&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;adaboost，svm，lr三个算法的关系：&lt;/p&gt;

&lt;p&gt;三种算法的分布对应exponential loss（指数损失函数），hinge loss，log loss（对数损失函数），无本质区别。应用凸上界取代0、1损失，即凸松弛技术。从组合优化到凸集优化问题。凸函数，比较容易计算极值点。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;关于loss function，记录几点：
    &lt;ul&gt;
      &lt;li&gt;SVC &amp;amp; SVR：对于SVC，由于hinge loss: max(0, 1-yw’x)，SVC会特别关注那些难分类的点（classify with least certainty; support vectors, i.e., sv）；对于SVR，由于epsilon loss: max(0, |y-w’x|-e)，SVR类似的会关注那些residual比较大的点（”outlier”; prediction with least certainty too; sv）&lt;/li&gt;
      &lt;li&gt;SVM和ANN 1) SVM的hinge loss func: max(0, 1 - y’x), 当“激活量” y’x 超过阈值1，那么就不计损失即损失为零 2) Deep Sparse Rectifier的rectifier activation func: max(0, x), 看图它在零点处不可导带来了稀疏性，可大量约简神经网络参数规模 3) loss function + penalty regularization&lt;/li&gt;
      &lt;li&gt;SVM &amp;amp; Boosting：SVM用loss func来剔除了那些容易的点，实际相当于给样本加了权重0，使得模型更加关注难预测的点；Boosting每次迭代后调整权重：降低容易点的权重，增加难预测点的权重，也使得模型更关注难预测的点&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.cs.nyu.edu/~yann/talks/lecun-20071207-nonconvex.pdf&quot;&gt;Who is afraid of non-convex loss functions?&lt;/a&gt;&lt;/p&gt;

    &lt;p&gt;Yann LeCun在2007年的讲义。目前大部分loss function都是凸函数，譬如lr，svms，exponential-family graphical models，凸函数一个很优秀的性质，但有些时候却是限制，坚持凸性将增大模型规模或者求解最优化问题方法的复杂程度。&lt;/p&gt;

    &lt;p&gt;Deep architectures势必将带来非凸的loss function。但是(1)采用一个合适的模型架构(即便将带来loss function非凸)比坚持凸性更加重要；(2) 即使是对于浅层模型，例如SVM，使用非凸的loss function实际上可以带来准确率和性能的提升。&lt;/p&gt;

    &lt;p&gt;对于神经网络，conjugate gradient, BFGS, LM-BFGS没有stochastic gradient好用；对于SVM，“batch” quadratic方法没有SMO好用，而SMO又没有on-line methods好用；对于CRF，Iterative scaling没有stochastic gradient好用；虽说stochastic gradient没有很多的理论保证，但经验证明是不错的选择。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-8&quot;&gt;最优化算法参考资料&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.erogol.com/comparison-sgd-vs-momentum-vs-rmsprop-vs-momentumrmsprop/&quot;&gt;几点改进随机梯度下降(SGD)过程的tricks&lt;/a&gt;
  对梯度本身的探究也是特别有价值的，这里的优化方法，就是让梯度的update更平滑和稳定，记录之前在这个方向上的梯度，做平滑，并防止不同batch带来的突变是有价值的&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://stanford.edu/~boyd/cvxbook/&quot;&gt;凸优化，convex optimization&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Thu, 15 Jan 2015 00:00:00 +0800</pubDate>
        <link>http://yourdomain.com/2015/01/15/%E6%9C%80%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95.html</link>
        <guid isPermaLink="true">http://yourdomain.com/2015/01/15/%E6%9C%80%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95.html</guid>
        
        
      </item>
    
      <item>
        <title>gbdt_adaboost_bootstrap</title>
        <description>&lt;h2 id=&quot;gbdtadaboostbootstrap&quot;&gt;gbdt，adaboost，bootstrap&lt;/h2&gt;

&lt;h3 id=&quot;gbdt&quot;&gt;gbdt&lt;/h3&gt;

&lt;p&gt;意为 gradient boost decision tree。又叫MART（Multiple Additive Regression Tree)&lt;/p&gt;

&lt;h4 id=&quot;section&quot;&gt;分类树和回归树&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;分类树：预测分类标签；C4.5；选择划分成两个分支后熵最大的feature；&lt;/li&gt;
  &lt;li&gt;回归树：预测实数值；回归树的结果是可以累加的；最小化均方差；&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;shrinkage&quot;&gt;Shrinkage&lt;/h4&gt;

&lt;p&gt;Shrinkage（缩减）的思想认为，每次走一小步逐渐逼近结果的效果，要比每次迈一大步很快逼近结果的方式更容易避免过拟合。即它不完全信任每一个棵残差树，它认为每棵树只学到了真理的一小部分，累加的时候只累加一小部分，通过多学几棵树弥补不足。&lt;/p&gt;

&lt;h3 id=&quot;boosting&quot;&gt;boosting方法比较&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;gbdt的核心在于：每一棵树学的是之前所有树的结论和残差。每一步的残差计算其实变相地增大了分错instance的权重，而已经分对的instance则都趋向于0。&lt;/li&gt;
  &lt;li&gt;Adaboost：是另一种boost方法，它按分类对错，分配不同的weight，计算cost function时使用这些weight，从而让“错分的样本权重越来越大，使它们更被重视”。&lt;/li&gt;
  &lt;li&gt;Bootstrap也有类似思想，它在每一步迭代时不改变模型本身，也不计算残差，而是从N个instance训练集中按一定概率重新抽取N个instance出来（单个instance可以被重复sample），对着这N个新的instance再训练一轮。由于数据集变了迭代模型训练结果也不一样，而一个instance被前面分错的越厉害，它的概率就被设的越高，这样就能同样达到逐步关注被分错的instance，逐步完善的效果。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-1&quot;&gt;更多学习资料&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;http://suanfazu.com/t/gbdt-die-dai-jue-ce-shu-ru-men-jiao-cheng/135&quot;&gt;Gbdt迭代决策树入门教程&lt;/a&gt;
&lt;a href=&quot;http://www.schonlau.net/publication/05stata_boosting.pdf&quot;&gt;Boosting Decision Tree入门教程&lt;/a&gt;
&lt;a href=&quot;http://research.microsoft.com/pubs/132652/MSR-TR-2010-82.pdf&quot;&gt;LambdaMART用于搜索排序入门教程&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 15 Jan 2015 00:00:00 +0800</pubDate>
        <link>http://yourdomain.com/2015/01/15/gbdt_adaboost_bootstrap.html</link>
        <guid isPermaLink="true">http://yourdomain.com/2015/01/15/gbdt_adaboost_bootstrap.html</guid>
        
        
      </item>
    
  </channel>
</rss>
