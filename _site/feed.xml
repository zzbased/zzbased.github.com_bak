<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>100的技术博客</title>
    <description>机器学习，自然语言处理，计算广告学，工作与生活，总结与温习
</description>
    <link>http://yourdomain.com/</link>
    <atom:link href="http://yourdomain.com/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 23 Aug 2015 13:22:32 +0800</pubDate>
    <lastBuildDate>Sun, 23 Aug 2015 13:22:32 +0800</lastBuildDate>
    <generator>Jekyll v2.5.3</generator>
    
      <item>
        <title>互联网金融大杂烩</title>
        <description>&lt;p&gt;当前格局&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;当前互联网+金融格局，由传统金融机构和非金融机构组成。传统金融机构主要为传统金融业务的互联网创新以及电商化创新、APP软件等；非金融机构则主要是指利用互联网技术进行金融运作的电商企业、（P2P）模式的网络借贷平台，众筹模式的网络投资平台，挖财类（模式）的手机理财APP(理财宝类)，以及第三方支付平台等。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;众筹&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;众筹大意为大众筹资或群众筹资，是指用团购预购的形式，向网友募集项目资金的模式。众筹的本意是利用互联网和SNS传播的特性，让创业企业、艺术家或个人对公众展示他们的创意及项目，争取大家的关注和支持，进而获得所需要的资金援助。众筹平台的运作模式大同小异——需要资金的个人或团队将项目策划交给众筹平台，经过相关审核后，便可以在平台的网站上建立属于自己的页面，用来向公众介绍项目情况。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;P2P网贷&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;P2P(Peer-to-Peerlending)，即点对点信贷。P2P网贷是指通过第三方互联网平台进行资金借、贷双方的匹配，需要借贷的人群可以通过网站平台寻找到有出借能力并且愿意基于一定条件出借的人群，帮助贷款人通过和其他贷款人一起分担一笔借款额度来分散风险，也帮助借款人在充分比较的信息中选择有吸引力的利率条件，比如贷贷巴等。
  两种运营模式，第一是纯线上模式，其特点是资金借贷活动都通过线上进行，不结合线下的审核。通常这些企业采取的审核借款人资质的措施有通过视频认证、查看银行流水账单、身份认证等。第二种是线上线下结合的模式，借款人在线上提交借款申请后，平台通过所在城市的代理商采取入户调查的方式审核借款人的资信、还款能力等情况。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;第三方支付&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;第三方支付（Third-PartyPayment）狭义上是指具备一定实力和信誉保障的非银行机构，借助通信、计算机和信息安全技术，采用与各大银行签约的方式，在用户与银行支付结算系统间建立连接的电子支付模式。
  根据央行2010年在《非金融机构支付服务管理办法》中给出的非金融机构支付服务的定义，从广义上讲第三方支付是指非金融机构作为收、付款人的支付中介所提供的网络支付、预付卡、银行卡收单以及中国人民银行确定的其他支付服务。第三方支付已不仅仅局限于最初的互联网支付，而是成为线上线下全面覆盖，应用场景更为丰富的综合支付工具。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;数字货币&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;除去蓬勃发展的第三方支付、P2P贷款模式、小贷模式、众筹融资、余额宝模式等形式，以比特币为代表的互联网货币也开始露出自己的獠牙。
  以比特币等数字货币为代表的互联网货币爆发，从某种意义上来说，比其他任何互联网金融形式都更具颠覆性。在2013年8月19日，德国政府正式承认比特币的合法“货币”地位，比特币可用于缴税和其他合法用途，德国也成为全球首个认可比特币的国家。这意味着比特币开始逐渐“洗白”，从极客的玩物，走入大众的视线。也许，它能够催生出真正的互联网金融帝国。
  比特币炒得火热，也跌得惨烈。无论怎样，这场似乎曾经离我们很遥远的互联网淘金盛宴已经慢慢走进我们的视线，它让人们看到了互联网金融最终极的形态就是互联网货币。所有的互联网金融只是对现有的商业银行、证券公司提出挑战，将来发展到互联网货币的形态就是对央行的挑战。也许比特币会颠覆传统金融成长为首个全球货币，也许它会最终走向崩盘，不管怎样，可以肯定的是，比特币会给人类留下一笔永恒的遗产。[5]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;大数据金融&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;大数据金融是指集合海量非结构化数据，通过对其进行实时分析，可以为互联网金融机构提供客户全方位信息，通过分析和挖掘客户的交易和消费信息掌握客户的消费习惯，并准确预测客户行为，使金融机构和金融服务平台在营销和风险控制方面有的放矢。
  基于大数据的金融服务平台主要指拥有海量数据的电子商务企业开展的金融服务。大数据的关键是从大量数据中快速获取有用信息的能力，或者是从大数据资产中快速变现利用的能力。因此，大数据的信息处理往往以云计算为基础。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;金融机构&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;所谓信息化金融机构，是指通过采用信息技术，对传统运营流程进行改造或重构，实现经营、管理全面电子化的银行、证券和保险等金融机构。金融信息化是金融业发展趋势之一，而信息化金融机构则是金融创新的产物。
  从金融整个行业来看，银行的信息化建设一直处于业内领先水平，不仅具有国际领先的金融信息技术平台，建成了由自助银行、电话银行、手机银行和网上银行构成的电子银行立体服务体系，而且以信息化的大手笔——数据集中工程在业内独领风骚，其除了基于互联网的创新金融服务之外，还形成了“门户”“网银、金融产品超市、电商”的一拖三的金融电商创新服务模式。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;金融门户&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;互联网金融门户是指利用互联网进行金融产品的销售以及为金融产品销售提供第三方服务的平台。它的核心就是“搜索比价”的模式，采用金融产品垂直比价的方式，将各家金融机构的产品放在平台上，用户通过对比挑选合适的金融产品。
  互联网金融门户多元化创新发展，形成了提供高端理财投资服务和理财产品的第三方理财机构，提供保险产品咨询、比价、购买服务的保险门户网站等。这种模式不存在太多政策风险，因为其平台既不负责金融产品的实际销售，也不承担任何不良的风险，同时资金也完全不通过中间平台。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;重要文献&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.zhihu.com/question/22858430&quot;&gt;如何用大数据软件分析金融数据&lt;/a&gt;，&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.36dsj.com/archives/10041&quot;&gt;大数据如何作用于金融领域并创造价值？&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.zhihu.com/question/27561422&quot;&gt;&lt;strong&gt;大数据技术在金融行业有哪些应用前景&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.zhihu.com/question/21094114&quot;&gt;大数据在金融领域是如何应用的？&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.zhihu.com/question/21642875&quot;&gt;当传统金融模式遇到了大数据后会有哪些转变？&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.zhihu.com/question/31259020&quot;&gt;外国的金融经济数据如何查出来？&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.zhihu.com/question/25074959&quot;&gt;目前国内的高频交易业务对研发人员的需求如何？&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://zhuanlan.zhihu.com/econophysics/19944548&quot;&gt;大数据在金融市场中的应用-利用Twitter用户数据的情绪预测金融市场未来涨跌&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.gu360.com/&quot;&gt;利用文本检测市场情绪的做法-&amp;gt;股票雷达&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.forbes.com/sites/timworstall/2014/08/04/big-data-using-google-searches-to-predict-stock-market-falls/&quot;&gt;Big Data; Using Google Searches To Predict Stock Market Falls&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.sanban18.com/baike/4418.html&quot;&gt;干货：十张图看懂新三板&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.eastmoney.com/&quot;&gt;东方财富网&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.zhihu.com/question/23512883&quot;&gt;什么叫期货&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;股票机器人&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/zzbased/zzbased.github.com/blob/master/_posts/doc/股票机器人.pdf&quot;&gt;股票机器人&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://cs229.stanford.edu/proj2012/ShenJiangZhang-StockMarketForecastingusingMachineLearningAlgorithms.pdf&quot;&gt;Stock Market Forecasting Using Machine Learning Algorithms&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.vatsals.com/Essays/MachineLearningTechniquesforStockPrediction.pdf&quot;&gt;Machine Learning Techniques for Stock Prediction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://arxiv.org/pdf/1010.3003&amp;amp;&quot;&gt;Twitter mood predicts the stock market&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://arxiv.org/ftp/arxiv/papers/1311/1311.4771.pdf&quot;&gt;Stock Market Trend Analysis Using Hidden Markov Models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Sun, 02 Aug 2015 00:00:00 +0800</pubDate>
        <link>http://yourdomain.com/2015/08/02/%E4%BA%92%E8%81%94%E7%BD%91%E9%87%91%E8%9E%8D%E5%A4%A7%E6%9D%82%E7%83%A9.html</link>
        <guid isPermaLink="true">http://yourdomain.com/2015/08/02/%E4%BA%92%E8%81%94%E7%BD%91%E9%87%91%E8%9E%8D%E5%A4%A7%E6%9D%82%E7%83%A9.html</guid>
        
        
      </item>
    
      <item>
        <title>图像视觉相关</title>
        <description>&lt;h1 id=&quot;section&quot;&gt;图像视觉相关&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://blog.csdn.net/morewindows/article/details/8225783&quot;&gt;OpenCV入门指南&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://quantombone.blogspot.com/2015/01/from-feature-descriptors-to-deep.html&quot;&gt;From feature descriptors to deep learning: 20 years of computer vision&lt;/a&gt; 从特征描述子到深度学习——机器视觉20年回顾。通俗易懂，回顾了很多特征描述子的内容，也介绍了很多计算机视觉、机器学习特别是深度学习方面的大牛。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://t.cn/RZkbwoz&quot;&gt;图像处理中的全局优化技术&lt;/a&gt; 最近打算好好学习一下几种图像处理和计算机视觉中常用的 global optimization (或 energy minimization) 方法，这里总结一下学习心得。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://blog.csdn.net/carson2005/article/details/9502053&quot;&gt;Retinex算法详解&lt;/a&gt; - 计算机视觉小菜鸟的专栏 - 博客频道 - CSDN.NET&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;本团队雕琢多年的人脸检测库现以MIT协议发布 &lt;a href=&quot;https://github.com/ShiqiYu/libfacedetection&quot;&gt;github code&lt;/a&gt; 供商业和非商业无限制使用,包含正面和多视角人脸检测两个算法.优点:速度快(OpenCV haar+adaboost的2-3倍), 准确度高 (FDDB非公开类评测排名第二），能估计人脸角度. 例子看下图. 希望能帮助到有需要的个人和公司。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.guokr.com/article/439945/&quot;&gt;计算机视觉：让冰冷的机器看懂多彩的世界&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;[论文]《FaceNet: A Unified Embedding for Face Recognition and Clustering》http://t.cn/Rwg398R Google对Facebook DeepFace的有力回击—— FaceNet，在LFW(Labeled Faces in the Wild)上达到99.63%准确率(新纪录)，FaceNet embeddings可用于人脸识别、鉴别和聚类&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.cnblogs.com/sciencefans/p/4394861.html&quot;&gt;人脸识别技术大总结(1)：Face Detection &amp;amp; Alignment&lt;/a&gt; 介绍人脸识别的四大块：Face detection, alignment, verification and identification(recognization)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://python.jobbole.com/81277/&quot;&gt;利用图片指纹检测高相似度图片&lt;/a&gt; 我们也曾做过图片指纹，利用sift计算图片特征，再利用simhash计算图片指纹。
  &lt;a href=&quot;https://github.com/maccman/dhash&quot;&gt;dhash library&lt;/a&gt; 该库利用了差分哈希判别图片的相似性和sift特征，能判别变形后同源图片。
  &lt;a href=&quot;http://blog.csdn.net/zmazon/article/details/8618775&quot;&gt;相似图片搜索的三种哈希算法&lt;/a&gt; 平均ahash，感知phash，差分dhash。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/nagadomi/waifu2x&quot;&gt;waifu2x: 用深度卷积神经网络（CNN）对动漫放大降噪处理得超清大图的开源工具&lt;/a&gt; 左链接是GitHub托管地址。&lt;a href=&quot;http://waifu2x.udp.jp&quot;&gt;在线Demo&lt;/a&gt; 据说这款用Lua写的软件在日本引起不少话题，的确令人印象深刻&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Thu, 21 May 2015 00:00:00 +0800</pubDate>
        <link>http://yourdomain.com/2015/05/21/%E5%9B%BE%E5%83%8F%E8%A7%86%E8%A7%89.html</link>
        <guid isPermaLink="true">http://yourdomain.com/2015/05/21/%E5%9B%BE%E5%83%8F%E8%A7%86%E8%A7%89.html</guid>
        
        
      </item>
    
      <item>
        <title>leetcode刷题小结</title>
        <description>&lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;h1 id=&quot;leetcode&quot;&gt;Leetcode刷题小结&lt;/h1&gt;

&lt;h2 id=&quot;array&quot;&gt;Array&lt;/h2&gt;

&lt;h3 id=&quot;median-of-two-sorted-array&quot;&gt;Median of two sorted array&lt;/h3&gt;

&lt;p&gt;There are two sorted arrays nums1 and nums2 of size m and n respectively. Find the median of the two sorted arrays. The overall run time complexity should be O(log (m+n)).&lt;/p&gt;

&lt;p&gt;更通用的形式为：给定两个已排序好的数组，找到两者所有元素中第k大的元素。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;解法1：merge两个数组，然后求第k大的元素。O(m+n)复杂度。&lt;/li&gt;
  &lt;li&gt;解法2：利用一个计数器，记录当前已经找到的第m大的元素，从两个数组的第一个元素开始遍历。O(m+n)复杂度。&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;解法3：利用两个数组有序的特性，每次都删除k/2个元素。O(log(m+n))。&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  class Solution {
  public:
    // 寻找第k小的数
    double find_kth(vector&amp;lt;int&amp;gt;::iterator it1, int n1,
                    vector&amp;lt;int&amp;gt;::iterator it2, int n2,
                    int k) {
      // 确保n1 &amp;gt;= n2
      if (n1 &amp;lt; n2) {
        return find_kth(it2, n2, it1, n1, k);
      }
      if (n2 == 0) {
        return *(it1 + k-1);
      }
      if (k == 1) {
        return min(*it1, *it2);
      }
      // 注意这个划分,很重要
      int i2 = min(k/2, n2);
      int i1 = k - i2;
      if (*(it1 + i1-1) &amp;gt; *(it2 + i2-1)) {
        // 删掉数组2的i2个
        return find_kth(it1, n1, it2 + i2, n2 - i2, i1);
      } else if (*(it1 + i1-1) &amp;lt; *(it2 + i2-1)) {
        // 删掉数组1的i1个
        return find_kth(it1 + i1, n1 - i1, it2, n2, i2);
      } else {
        return *(it1 + i1-1);
      }
    }

    // 寻找第k小的数, C语言版本
    double find_kth2(const int* A, int m, const int* B, int n, int k) {
      if (m &amp;lt; n) {
        return find_kth2(B, n, A, m, k);
      }
      if (n == 0) {
        return A[k-1];
      }
      if (k == 1) {
        return min(A[0], B[0]);
      }

      int i2 = min(k/2, n);
      int i1 = k - i2;
      if (A[i1-1] &amp;lt; B[i2-1]) {
        return find_kth2(A+i1, m-i1, B, n, k-i1);
      } else if (A[i1-1] &amp;gt; B[i2-1]) {
        return find_kth2(A, m, B+i2, n-i2, k-i2);
      } else {
        return A[i1-1];
      }
    }
    // 数组从小到大排序
    double findMedianSortedArrays(vector&amp;lt;int&amp;gt;&amp;amp; nums1, vector&amp;lt;int&amp;gt;&amp;amp; nums2) {
      int total = nums1.size() + nums2.size();
      if (total &amp;amp; 0x1) {
        // odd
        // return find_kth(nums1.begin(), nums1.size(), nums2.begin(), nums2.size(), total/2 + 1);
        return find_kth2(nums1.data(), nums1.size(), nums2.data(), nums2.size(), total/2 + 1);
      } else {
        // return ( find_kth(nums1.begin(), nums1.size(), nums2.begin(), nums2.size(), total/2 + 1)
        //     + find_kth(nums1.begin(), nums1.size(), nums2.begin(), nums2.size(), total/2) )/ 2.0;
        return ( find_kth2(nums1.data(), nums1.size(), nums2.data(), nums2.size(), total/2 + 1)
            + find_kth2(nums1.data(), nums1.size(), nums2.data(), nums2.size(), total/2) )/ 2.0;
      }

    }
  };
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;contains-duplicate-iiihttpsleetcodecomproblemscontains-duplicate-iii&quot;&gt;&lt;a href=&quot;https://leetcode.com/problems/contains-duplicate-iii/&quot;&gt;Contains Duplicate III&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;Input: [-1,2147483647], 1, 2147483647&lt;/p&gt;

&lt;p&gt;下面代码中，在计算gap时，首先gap必须是long类型，其次it_temp-&amp;gt;first和last至少也有一个long，不然这个减法会有问题。&lt;/p&gt;

&lt;p&gt;long gap = it_temp-&amp;gt;first - last&lt;/p&gt;

&lt;p&gt;除此外，还有一个容易犯的错误，gap的计算经常会在while循环里被忽视掉了。&lt;/p&gt;

&lt;p&gt;主要可以参考 &lt;a href=&quot;http://www.cppblog.com/suiaiguo/archive/2009/07/16/90228.html&quot;&gt;隐式类型转换&amp;amp;&amp;amp; 负数的补码&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class Solution {
public:
    // from little to large
    static bool SortFunction(const std::pair&amp;lt;int, int&amp;gt;&amp;amp; x, const std::pair&amp;lt;int, int&amp;gt;&amp;amp; y) {
        if (x.first &amp;gt; y.first) {
            return false;
        } else if (x.first &amp;lt; y.first) {
            return true;
        } else {
            return x.second &amp;lt; y.second;
        }
    }
    bool containsNearbyAlmostDuplicate(vector&amp;lt;int&amp;gt;&amp;amp; nums, int k, int t) {
        if (nums.size() &amp;lt; 2) {
            return false;
        }
        std::vector&amp;lt;std::pair&amp;lt;int, int&amp;gt; &amp;gt; middle;  // num -- index
        for (int i = 0; i &amp;lt; nums.size(); ++i) {
            middle.push_back(std::make_pair(nums[i], i));
        }
        std::sort(middle.begin(), middle.end(), SortFunction);
        std::vector&amp;lt;std::pair&amp;lt;int, int&amp;gt; &amp;gt;::const_iterator it = middle.begin();
        long last = it-&amp;gt;first;
        int index = it-&amp;gt;second;
        ++it;
        for (; it != middle.end(); ++it) {
            std::vector&amp;lt;std::pair&amp;lt;int, int&amp;gt; &amp;gt;::const_iterator it_temp = it;
            long gap = it_temp-&amp;gt;first - last;
            while (it_temp != middle.end() &amp;amp;&amp;amp; gap &amp;lt;= (long)t) {
                // at most t &amp;amp;&amp;amp; most k
                if (abs(it_temp-&amp;gt;second - index) &amp;lt;= k) {
                    return true;
                }
                ++it_temp;
                gap = it_temp-&amp;gt;first - last;
            }
            last = it-&amp;gt;first;
            index = it-&amp;gt;second;
        }
        return false;
    }
};
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;kth-largest-element-in-an-arrayhttpsleetcodecomsubmissionsdetail30747333&quot;&gt;&lt;a href=&quot;https://leetcode.com/submissions/detail/30747333/&quot;&gt;Kth Largest Element in an Array&lt;/a&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;// 错误点: sort默认是从小到大排序
void InsertArray(vector&amp;lt;int&amp;gt;&amp;amp; array, int insert) {
    // 二分查找
    int begin = 0;
    int end = array.size() - 1;
    int search_index = 0;
    while (begin &amp;lt;= end) {
        int middle = (long(begin) + long(end)) / 2;
        if (insert &amp;gt; array[middle]) {
            if (middle - 1 &amp;lt; 0 || insert &amp;lt; array[middle - 1]) {
                search_index = middle;
                break;
            }
            // 前半段
            end = middle;
        } else if (insert &amp;lt;= array[middle]) {
            if (middle + 1 &amp;gt; array.size() - 1 || insert &amp;gt;= array[middle + 1]) {
                search_index = middle + 1;
                break;
            }
            // 后半段
            begin = middle;
        }
    }
    // 找到index区间
    for (int i = array.size() - 1; i &amp;gt; search_index; --i) {
        array[i] = array[i-1];
    }
    array[search_index] = insert;
}
struct myclass {
    bool operator() (int i, int j) { return (i&amp;gt;j);}
} myobject;

int findKthLargest(vector&amp;lt;int&amp;gt;&amp;amp; nums, int k) {
    if (nums.size() &amp;lt; k || k &amp;lt; 1) {
        return 0;
    }
    vector&amp;lt;int&amp;gt; array(nums.begin(), nums.begin() + k);
    std::sort(array.begin(), array.end(), myobject);  // 从大到小排序
    for (int i = k; i &amp;lt; nums.size(); ++i) {
        if (nums[i] &amp;gt; array[k-1]) {
            InsertArray(array, nums[i]);
        } else {
            continue;
        }
    }
    return array[k - 1];
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;list&quot;&gt;List&lt;/h2&gt;

&lt;h3 id=&quot;remove-linked-list-elementshttpsleetcodecomproblemsremove-linked-list-elements&quot;&gt;&lt;a href=&quot;https://leetcode.com/problems/remove-linked-list-elements/&quot;&gt;Remove Linked List Elements&lt;/a&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;// 错误点: 未考虑都是val的情况. 也就是在unittest时，还是应该尽可能的考虑周全，要记得必须写unittest。
// Input: [1,1], 1
// 能否换一个思路,不再考虑删除,而是把不是val的node插入.
// 本次错误的点:主要是没有考虑到连续的val存在. 对付这种题,可以先申明一个temp node;另外, 也就是在now-&amp;gt;val == val的判断,对last的赋值要有一个else

ListNode* removeElements(ListNode* head, int val) {
    if (head == NULL) {
        return head;
    }
    ListNode temp(val+1);
    temp.next = head;

    ListNode* last = &amp;amp;temp;
    ListNode* now = head;
    while (now) {
        if (now-&amp;gt;val == val) {
            last-&amp;gt;next = now-&amp;gt;next;
        } else {
            last = now;
        }
        now = now-&amp;gt;next;
    }
    return temp.next;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;string&quot;&gt;String&lt;/h2&gt;

&lt;h3 id=&quot;isomorphic-stringshttpsleetcodecomproblemsisomorphic-strings&quot;&gt;&lt;a href=&quot;https://leetcode.com/problems/isomorphic-strings/&quot;&gt;Isomorphic strings&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;很简单的一个题目。但还是考虑不严谨。只是从s-&amp;gt;t这个方面做了考虑，而没有考虑t-&amp;gt;s这个方面。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bool isIsomorphic(string s, string t) {
    if (s.size() != t.size()) {
        return false;
    }
    map&amp;lt;char, char&amp;gt; container1;
    map&amp;lt;char, char&amp;gt; container2;
    for (int i = 0; i &amp;lt; s.size(); ++i) {
        map&amp;lt;char, char&amp;gt;::const_iterator it1 = container1.find(s[i]);
        map&amp;lt;char, char&amp;gt;::const_iterator it2 = container2.find(t[i]);
        if (it1 == container1.end()) {
            container1[s[i]] = t[i];
        } else {
            if (it1-&amp;gt;second != t[i]) {
                return false;
            }
        }
        if (it2 == container2.end()) {
            container2[t[i]] = s[i];
        } else {
            if (it2-&amp;gt;second != s[i]) {
                return false;
            }
        }
    }
    return true;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section&quot;&gt;数据结构&lt;/h2&gt;

&lt;h3 id=&quot;implement-stack-using-queueshttpsleetcodecomproblemsimplement-stack-using-queues&quot;&gt;&lt;a href=&quot;https://leetcode.com/problems/implement-stack-using-queues/&quot;&gt;Implement Stack using Queues&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;class Stack {
public:
    // Push element x onto stack.
    void push(int x) {
        in&lt;em&gt;.push_back(x);
        top&lt;/em&gt; = x;
    }&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Removes the element on top of the stack.
void pop() {
    if (in_.empty()) {
        return;
    } else if (in_.size() == 1) {
        in_.pop_front();
    } else {
        int in_size = in_.size();
        int i = 0;
        while (i &amp;lt; in_size - 1) {
            out_.push_back(in_.front());
            in_.pop_front();
            ++i;
        }
        in_.pop_front();
        while (!out_.empty()) {
            // in_.push_back(out_.front()); // 这里出错了.top_未赋值
            push(out_.front());
            out_.pop_front();
        }
    }
}

// Get the top element.
int top() {
    return top_;
}

// Return whether the stack is empty.
bool empty() {
    return in_.empty();
} private:
deque&amp;lt;int&amp;gt; in_;
deque&amp;lt;int&amp;gt; out_;
int top_; };
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-1&quot;&gt;动态规划&lt;/h2&gt;

&lt;h3 id=&quot;maximal-squarehttpsleetcodecomproblemsmaximal-square&quot;&gt;&lt;a href=&quot;https://leetcode.com/problems/maximal-square/&quot;&gt;Maximal Square&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;这里主要是利用动态规划来解，其方程为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;     动态规划：dp[x][y] = min(dp[x - 1][y - 1], dp[x][y - 1], dp[x - 1][y]) + 1
     上式中，dp[x][y]表示以坐标(x, y)为右下角元素的全1正方形矩阵的最大长度（宽度）
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;更多请参考 &lt;a href=&quot;http://stackoverflow.com/questions/1726632/dynamic-programming-largest-square-block&quot;&gt;largest-square-block&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;其他&lt;/h2&gt;

&lt;h3 id=&quot;rectangle-areahttpsleetcodecomproblemsrectangle-area&quot;&gt;&lt;a href=&quot;https://leetcode.com/problems/rectangle-area/&quot;&gt;Rectangle Area&lt;/a&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;int computeArea(int A, int B, int C, int D, int E, int F, int G, int H) {
    int area = (C-A)*(D-B) + (G-E)*(H-F);
    if (A &amp;gt;= G || B &amp;gt;= H || C &amp;lt;= E || D &amp;lt;= F)
    {
        return area;
    }

    int top = min(D, H);
    int bottom = max(B, F);
    int left = max(A, E);
    int right = min(C, G);

    return area - (top-bottom)*(right-left);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;largest-numberhttpsleetcodecomproblemslargest-number&quot;&gt;&lt;a href=&quot;https://leetcode.com/problems/largest-number/&quot;&gt;Largest Number&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;该题目的关键就是定义：比较函数。思路是关键。前面绕了很多弯路。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;static bool compare(string &amp;amp;s1, string &amp;amp;s2)
{
    return s1 + s2 &amp;gt; s2 + s1;
}

string largestNumber(vector&amp;lt;int&amp;gt; &amp;amp;num) {
    vector&amp;lt;string&amp;gt; arr;

    //将num转成string存入数组
    for(int i : num)
        arr.push_back(to_string(i));

    //比较排序
    sort(arr.begin(), arr.end(), compare);

    //连接成字符串
    string ret;
    for(string s : arr)
        ret += s;

    //排除特殊情况
    if(ret[0] == &#39;0&#39; &amp;amp;&amp;amp; ret.size() &amp;gt; 0)
        return &quot;0&quot;;

    return ret;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;count-primeshttpsleetcodecomproblemscount-primes&quot;&gt;&lt;a href=&quot;https://leetcode.com/problems/count-primes/&quot;&gt;Count primes&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;思路很巧妙，关键还是算法。&lt;a href=&quot;https://en.wikipedia.org/wiki/Sieve_of_Eratosthenes&quot;&gt;Sieve_of_Eratosthenes&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;int Label(int* array, int n, int p) {
    int multipler = 2;
    while (multipler * p &amp;lt; n) {
        array[multipler * p] = 1;
        multipler++;
    }
    for (int i = p+1; i &amp;lt; n; ++i) {
        if (array[i] == 0) {
            return i;
        }
    }
    return n;
}
int countPrimes(int n) {
    int* array = new int[n + 1];
    memset(array, 0, sizeof(int) * (n+1));
    int count = 0;
    for (int i = 2; i &amp;lt; n; ) {
        i = Label(array, n, i);
        count++;
    }
    /*
    for (int i = 2; i &amp;lt; n; ++i) {
        if (array[i] == 0) {
            count++;
        }
    }*/
    delete[] array;
    return count;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;shell&quot;&gt;Shell&lt;/h2&gt;

&lt;h3 id=&quot;word-frequencyhttpsleetcodecomproblemsword-frequency&quot;&gt;&lt;a href=&quot;https://leetcode.com/problems/word-frequency/&quot;&gt;Word Frequency&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;解答：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;awk -F&quot; &quot; &#39;{for (i = 1; i &amp;lt;= NF; ++i) {num[$i]++;}}END{for (a in num) print a,num[a]|&quot;sort -k2 -r -n&quot;}&#39; words.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意几个细节：(1)在awk的输出中排序，可以在后面直接接sort命令，不过需要用引号。(2)这里是按照map的value排序，需要指定”-k2”。(3)注意是降序排列，所以有”-r”。(4)再注意默认是ascii排序，这里应该是number排序，所以有”-n”。&lt;/p&gt;

&lt;h3 id=&quot;transpose-file-httpsleetcodecomproblemstranspose-file&quot;&gt;&lt;a href=&quot;https://leetcode.com/problems/transpose-file/&quot;&gt;Transpose File &lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;有一个感触：awk内置的map如此强大。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# (NF &amp;gt; p) {p = NF} 可以放到{}里面,如果在里面,则要加if.
awk -F&quot; &quot; &#39;{
    for (i = 1; i &amp;lt;= NF; i++) {
        content[NR,i] = $i
    }

}
(NF &amp;gt; p) {p = NF}
END{
    for (i = 1; i &amp;lt;= p; i++) {
        str = content[1, i]
        for (j = 2; j &amp;lt;= NR; j++) {
            str = str&quot; &quot;content[j, i]
        }
        print str
    }
}&#39; file.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;valid-phone-numbershttpsleetcodecomproblemsvalid-phone-numbers&quot;&gt;&lt;a href=&quot;https://leetcode.com/problems/valid-phone-numbers/&quot;&gt;Valid Phone Numbers&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;这里主要考察正则表达式。具体tool可以使用：grep, egrep, sed, awk。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#cat file.txt | grep -Eo &#39;^(\([0-9]{3}\) ){1}[0-9]{3}-[0-9]{4}$|^([0-9]{3}-){2}[0-9]{4}$&#39;
#grep -Eo &#39;^(\([0-9]{3}\) ){1}[0-9]{3}-[0-9]{4}$|^([0-9]{3}-){2}[0-9]{4}$&#39; file.txt
awk &#39;/^(\([0-9]{3}\) ){1}[0-9]{3}-[0-9]{4}$|^([0-9]{3}-){2}[0-9]{4}$/&#39; file.txt
sed -n &#39;/^(\([0-9]{3}\) ){1}[0-9]{3}-[0-9]{4}$/,/^([0-9]{3}-){2}[0-9]{4}$/p&#39; file.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;更多参考资料：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://coolshell.cn/articles/9104.html&quot;&gt;Sed简明教程-左耳朵耗子&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://coolshell.cn/articles/9070.html&quot;&gt;Awk简明教程-左耳朵耗子&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.funtoo.org/Sed_by_Example,_Part_2&quot;&gt;Sed by Example&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.gnu.org/software/sed/manual/html_node/Regular-Expressions.html&quot;&gt;Regular Expressions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.math.utah.edu/docs/info/gawk_5.html#SEC27&quot;&gt;Awk regex&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Thu, 21 May 2015 00:00:00 +0800</pubDate>
        <link>http://yourdomain.com/2015/05/21/leetcode%E5%88%B7%E9%A2%98%E5%B0%8F%E7%BB%93.html</link>
        <guid isPermaLink="true">http://yourdomain.com/2015/05/21/leetcode%E5%88%B7%E9%A2%98%E5%B0%8F%E7%BB%93.html</guid>
        
        
      </item>
    
      <item>
        <title>docker浅析</title>
        <description>&lt;h1 id=&quot;docker&quot;&gt;docker浅析&lt;/h1&gt;

&lt;p&gt;最近&lt;a href=&quot;https://github.com/docker/docker&quot;&gt;docker&lt;/a&gt;流行起来了。Docker提供了一种在安全、可重复的环境中自动部署软件的方式。&lt;/p&gt;

&lt;p&gt;google 关于borg的论文&lt;/p&gt;

&lt;p&gt;tencent以前soso时代的tborg。&lt;/p&gt;

&lt;p&gt;如今数平又在大力推进的Gaia。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;参考资料&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.infoq.com/cn/articles/tencent-millions-scale-docker-application-practice&quot;&gt;腾讯万台规模的Docker应用实践&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://cxwangyi.github.io/story/docker_revolution_1.md.html&quot;&gt;yiwang关于docker的tech notes&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;【Docker基础技术：Linux Namespace】上篇：http://t.cn/RACqZ2j ，下篇：http://t.cn/RACGQWs 。很多人说Docker是新技术，所以不敢用，其实不是，Docker里用的全是老技术。去年在阿里给同学们做过分享，这里我把分享成文发出来。我会写一系列的文章来阐述Docker的基础技术。&lt;/li&gt;
  &lt;li&gt;【探秘价值10亿美元的Docker公司总部】就在炙手可热的容器管理创业公司Docker融资9500万美元之际，@AlaudaCloud 战略及市场负责人于历濛造访了这家价值10亿美元的Docker公司总部，带我们感受了这家发展迅猛的公司内部文化氛围。http://t.cn/RACFZfQ&lt;/li&gt;
  &lt;li&gt;Borg论文出来了，《Large-scale cluster management at Google with Borg》：http://t.cn/RACOAho&lt;/li&gt;
  &lt;li&gt;【 从闭源的Borg到开源的Mesos】Mesos是Apache下的开源分布式资源管理框架，被称为是分布式系统的内核。数人科技CEO王璞来自Google，对Google内部的Borg平台非常熟悉，从Google离职后，他选择了基于Mesos和Docker的技术创业。本次采访中王璞分享了他的创业历程及对Mesos的理解。http://t.cn/RA8AejI&lt;/li&gt;
  &lt;li&gt;盆盆的两篇长微博《Windows Docker内部原理猜想》http://t.cn/RAyGI4Q《微软私有云和docker管理平台整合之展望》http://t.cn/RwFnhFx 盆盆在MVP OpenDay上有关云计算和自动化的讲座(抱歉不太清晰)http://t.cn/RAvJT6B&lt;/li&gt;
  &lt;li&gt;【Docker和LXC有什么不同?】Docker和LXC有什么不同? 这大概是很多初学者的困惑所在，为什么说Docker不是LXC的一个替代方案呢? Docker基于LXC的基础上做了哪些有想象力的工作呢? 本文作者就此分享了自己的一些独特见解。http://t.cn/RADN7O1&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-1&quot;&gt;其他资料&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;【zookeeper使用和原理探究（一）】 zookeeper介绍 zookeeper是一个为分布式应用提供一致性服务的软件，它是开源的Hadoop项目中的一个子项目，并且根据google发表的论文来实现的，接下来我们首先来安装使… 详见：http://t.cn/zjl1sUn&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Thu, 21 May 2015 00:00:00 +0800</pubDate>
        <link>http://yourdomain.com/2015/05/21/dockter%E6%B5%85%E6%9E%90.html</link>
        <guid isPermaLink="true">http://yourdomain.com/2015/05/21/dockter%E6%B5%85%E6%9E%90.html</guid>
        
        
      </item>
    
      <item>
        <title>基本数据结构</title>
        <description>&lt;h1 id=&quot;section&quot;&gt;基本数据结构&lt;/h1&gt;

&lt;h2 id=&quot;section-1&quot;&gt;距离度量&lt;/h2&gt;
&lt;p&gt;闵可夫斯基距离(Minkowski Distance)，闵氏距离不是一种距离，而是一组距离的定义。
两个n维变量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的闵可夫斯基距离定义为：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/Minkowski_distance.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;其中p是一个变参数。
当p=1时，就是曼哈顿距离
当p=2时，就是欧氏距离
当p→∞时，就是切比雪夫距离
根据变参数的不同，闵氏距离可以表示一类的距离。&lt;/p&gt;

&lt;p&gt;参考自&lt;a href=&quot;http://blog.csdn.net/v_july_v/article/details/8203674&quot;&gt;从K近邻算法、距离度量谈到KD树、SIFT+BBF算法&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;余弦距离&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://t.cn/R2y1Mzu&quot;&gt;论文:兼顾语义/效率的文本空间夹角余弦(TSCS)《Textual Spatial Cosine Similarity》G Crocetti (2015)&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;相似性查询&lt;/h2&gt;

&lt;p&gt;索引结构中相似性查询有两种基本的方式：
范围查询和K最近邻查询。&lt;/p&gt;

&lt;p&gt;常见应用有：对图片计算一个指纹，查找相似图片；寻找K个距离最近的点；&lt;/p&gt;

&lt;p&gt;其方法为：构建数据索引，因为实际数据一般都会呈现簇状的聚类形态，因此我们想到建立数据索引，然后再进行快速匹配。索引树是一种树结构索引方法，其基本思想是对搜索空间进行层次划分。根据划分的空间是否有混叠可以分为Clipping和Overlapping两种。前者划分空间没有重叠，其代表就是k-d树；后者划分空间相互有交叠，其代表为R树。&lt;/p&gt;

&lt;p&gt;R树请参考&lt;a href=&quot;http://blog.sina.com.cn/s/blog_72e1c7550101dsc3.html&quot;&gt;基于R-Tree的最近邻查询&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;k-d&quot;&gt;K-D树&lt;/h2&gt;

&lt;p&gt;Kd-树是K-dimension tree的缩写，是对数据点在k维空间（如二维(x，y)，三维(x，y，z)，k维(x1，y，z..)）中划分的一种数据结构，主要应用于多维空间关键数据的搜索（如：范围搜索和最近邻搜索）。本质上说，Kd-树就是一种平衡二叉树。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://zh.wikipedia.org/wiki/K-d树&quot;&gt;Wiki&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;section-3&quot;&gt;最邻近搜索&lt;/h3&gt;

&lt;p&gt;最邻近搜索用来找出在树中与输入点最接近的点。&lt;/p&gt;

&lt;p&gt;k-d树最邻近搜索的过程如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;从根节点开始，递归的往下移。往左还是往右的决定方法与插入元素的方法一样(如果输入点在分区面的左边则进入左子节点，在右边则进入右子节点)。&lt;/li&gt;
  &lt;li&gt;一旦移动到叶节点，将该节点当作”目前最佳点”。&lt;/li&gt;
  &lt;li&gt;解开递归，并对每个经过的节点运行下列步骤：
    &lt;ul&gt;
      &lt;li&gt;如果目前所在点比目前最佳点更靠近输入点，则将其变为目前最佳点。&lt;/li&gt;
      &lt;li&gt;检查另一边子树有没有更近的点，如果有则从该节点往下找&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;当根节点搜索完毕后完成最邻近搜索&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-4&quot;&gt;蓄水池抽样&lt;/h2&gt;
&lt;p&gt;怎样随机从N个元素中选择一个或K个元素，你依次遍历每个元素，但不知道N多大。
方法有2个：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;先从中选出前K个元素，然后从 i=K+1 开始，以K/i的概率，将该元素替换原来的K个元素中的一个。&lt;/li&gt;
  &lt;li&gt;对每个元素，赋予一个随机值。维护一个最小堆，取随机值最大的K个，所对应的K个元素。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;更多参考 &lt;a href=&quot;http://blog.csdn.net/hackbuteer1/article/details/7971328&quot;&gt;海量数据随机抽样问题（蓄水池问题）&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-5&quot;&gt;蒙提霍尔问题&lt;/h2&gt;
&lt;p&gt;即&lt;a href=&quot;http://zh.wikipedia.org/w/index.php?title=蒙提霍爾問題&quot;&gt;三门问题&lt;/a&gt;。当这个问题不便于回答的时候，可以想象一下如果是1000个门的情况。&lt;/p&gt;

</description>
        <pubDate>Sun, 03 May 2015 00:00:00 +0800</pubDate>
        <link>http://yourdomain.com/2015/05/03/%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84.html</link>
        <guid isPermaLink="true">http://yourdomain.com/2015/05/03/%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84.html</guid>
        
        
      </item>
    
      <item>
        <title>RPC浅析</title>
        <description>&lt;h1 id=&quot;rpc&quot;&gt;RPC浅析&lt;/h1&gt;

&lt;p&gt;根据该文整理的ppt，请参考&lt;a href=&quot;https://github.com/zzbased/zzbased.github.com/blob/master/_posts/doc/RPC浅析.pdf&quot;&gt;rpc浅析.pdf&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;protobuf&quot;&gt;Protobuf简介&lt;/h2&gt;

&lt;h3 id=&quot;section&quot;&gt;简单介绍&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/google/protobuf&quot;&gt;protobuf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;优点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;用来序列化结构化数据，类似于xml，但是smaller, faster, and simpler，适合网络传输&lt;/li&gt;
  &lt;li&gt;支持跨平台多语言(e.g. Python, Java, Go, C++, Ruby, JavaNano)&lt;/li&gt;
  &lt;li&gt;消息格式升级，有较好的兼容性(想想以前用struct定义网络传输协议,解除version的痛楚)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;可读性差(not human-readable or human-editable)&lt;/li&gt;
  &lt;li&gt;不具有自描述性(self-describing)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;reflection&quot;&gt;Reflection&lt;/h3&gt;

&lt;p&gt;Reflection: 常用于pb与xml,json等其他格式的转换。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/reflection_protobuf.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;更多请参考：
&lt;a href=&quot;http://blog.csdn.net/solstice/article/details/6300108&quot;&gt;一种自动反射消息类型的 Google Protobuf 网络传输方案&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;section-1&quot;&gt;自描述消息&lt;/h3&gt;

&lt;p&gt;生产者：产生消息，填充内容，并序列化保存&lt;/p&gt;

&lt;p&gt;消费者：读取数据，反序列化得到消息，使用消息&lt;/p&gt;

&lt;p&gt;目的：解除这种耦合，让消费者能动态的适应消息格式的变换。&lt;/p&gt;

&lt;p&gt;生产者把定义消息格式的.proto文件和消息作为一个完整的消息序列化保存，完整保存的消息我称之为Wrapper message，原来的消息称之为payload message。&lt;/p&gt;

&lt;p&gt;消费者把wrapper message反序列化，先得到payload message的消息类型，然后根据类型信息得到payload message，最后通过反射机制来使用该消息。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;message SelfDescribingMessage {
	// Set of .proto files which define the type.
	required FileDescriptorSet proto_files = 1;

	// Name of the message type.  Must be defined by one of the files in
	// proto_files.
	required string type_name = 2;

	// The message data.
	required bytes message_data = 3;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Self-describing Messages 生产者&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;使用 protoc生成代码时加上参数–descriptor_set_out，输出类型信息(即SelfDescribingMessage的第一个字段内容)到一个文件，这里假设文件名为desc.set，protoc –cpp_out=. –descriptor_set_out=desc.set addressbook.proto&lt;/li&gt;
  &lt;li&gt;payload message使用方式不需要修改tutorial::AddressBook address_book;PromptForAddress(address_book.add_person());&lt;/li&gt;
  &lt;li&gt;在保存时使用文件desc.set内容填充SelfDescribingMessage的第一个字段，使用AddressBookAddressBook的full name填充SelfDescribingMessage的第二个字段，AddressBook序列化后的数据填充第三个字段。最后序列化SelfDescribingMessage保存到文件中。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Self-describing Messages 消费者&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;消费者编译时需要知道SelfDescribingMessage，不需要知道AddressBook，运行时可以正常操作AddressBook消息。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/self-describing_message_consume.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;section-2&quot;&gt;动态自描述消息&lt;/h3&gt;
&lt;p&gt;@TODO&lt;/p&gt;

&lt;h3 id=&quot;section-3&quot;&gt;工程实践&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;一般对日志数据只加不删不改, 所以其字段设计要极慎重。&lt;/li&gt;
  &lt;li&gt;千万不要随便修改tag number。&lt;/li&gt;
  &lt;li&gt;不要随便添加或者删除required field。&lt;/li&gt;
  &lt;li&gt;Clear并不会清除message memory（clear操作适合于清理那些数据量变化不大的数据，对于大小变化较大的数据是不适合的，需要定期（或每次）进行delete操作。建议swap或者delete）&lt;/li&gt;
  &lt;li&gt;repeated message域，size不要太大。&lt;/li&gt;
  &lt;li&gt;如果一个数据太大，不要使用protobuf。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-4&quot;&gt;参考资料&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/google/protobuf&quot;&gt;Protobuf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.searchtb.com/2012/09/protocol-buffers.html&quot;&gt;玩转Protobuf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://developers.google.com/protocol-buffers/docs/techniques?hl=zh-CN#self-description&quot;&gt;Self-describing Messages&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Protobuf memory内存的使用。 &lt;a href=&quot;http://qa.baidu.com/blog/?p=1179&quot;&gt;Protobuf使用不当导致的程序内存上涨问题&lt;/a&gt;
protobuf的clear操作适合于清理那些数据量变化不大的数据，对于大小变化较大的数据是不适合的，需要定期（或每次）进行delete操作。建议swap或者delete。&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.chinaunix.net/uid-26922071-id-3723751.html&quot;&gt;protobuf中会严重影响时间和空间损耗的地方 &lt;/a&gt;
repeated的性能问题。对于普通数据类型，在2^n+1时重新分配内存空间，而对于message数据，在2^n+1是分配对象地址空间，但每次都是new一个对象，这样就很损耗性能了。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;rpc-1&quot;&gt;RPC&lt;/h2&gt;

&lt;h3 id=&quot;rpc-2&quot;&gt;业界的RPC&lt;/h3&gt;

&lt;p&gt;基于protobuf的rpc最简单实现 两个优点：简化client-server交互，就像在调用一个本地方法；通过Protobuf实现多种编程语言之间的交互。
get all the advantages of working with protocol buffers, including efficient serialization, a simple IDL, and easy interface updating.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.codedump.info/?p=169&quot;&gt;使用google protobuf RPC实现echo service&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://codemacro.com/2014/08/31/protobuf-rpc/&quot;&gt;基于protobuf的RPC实现&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://jeoygin.org/2011/09/rpc-framework-protocol-buffers.html&quot;&gt;RPC框架系列——Protocol Buffers&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://djt.qq.com/article/view/327&quot;&gt;Poppy&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;gdt-rpc&quot;&gt;GDT RPC代码解析&lt;/h3&gt;

&lt;h4 id=&quot;section-5&quot;&gt;公共代码&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;echo_service&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;echo_service.proto:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;service EchoService {
	option (gdt.qzone_protocol_version) = 1;
	rpc Echo(EchoRequest) returns (EchoResponse) {
		option (gdt.qzone_protocol_cmd) = 10;
	}
	rpc FormTest(FormTestMessage) returns(FormTestMessage);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;protoc编译后：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class EchoService : public ::google::protobuf::Service {
	protected:
	// This class should be treated as an abstract interface.
	inline EchoService() {};
	public:
	virtual ~EchoService();

	typedef EchoService_Stub Stub;

	static const ::google::protobuf::ServiceDescriptor* descriptor();
	// 下面两个是虚函数,需要在子类实现
	virtual void Echo(::google::protobuf::RpcController* controller,
						const ::gdt::rpc_examples::EchoRequest* request,
						::gdt::rpc_examples::EchoResponse* response,
						::google::protobuf::Closure* done);
	virtual void FormTest(::google::protobuf::RpcController* controller,
						const ::gdt::rpc_examples::FormTestMessage* request,
						::gdt::rpc_examples::FormTestMessage* response,
						::google::protobuf::Closure* done);

	// implements Service ----------------------------------------------

	const ::google::protobuf::ServiceDescriptor* GetDescriptor();
	void CallMethod(const ::google::protobuf::MethodDescriptor* method,
					::google::protobuf::RpcController* controller,
					const ::google::protobuf::Message* request,
					::google::protobuf::Message* response,
					::google::protobuf::Closure* done);
	const ::google::protobuf::Message&amp;amp; GetRequestPrototype(
		const ::google::protobuf::MethodDescriptor* method) const;
	const ::google::protobuf::Message&amp;amp; GetResponsePrototype(
		const ::google::protobuf::MethodDescriptor* method) const;

	private:
	GOOGLE_DISALLOW_EVIL_CONSTRUCTORS(EchoService);
};

class EchoService_Stub : public EchoService {
	public:
	EchoService_Stub(::google::protobuf::RpcChannel* channel);
	EchoService_Stub(::google::protobuf::RpcChannel* channel,
					::google::protobuf::Service::ChannelOwnership ownership);
	~EchoService_Stub();

	inline ::google::protobuf::RpcChannel* channel() { return channel_; }

	// implements EchoService ------------------------------------------

	void Echo(::google::protobuf::RpcController* controller,
						const ::gdt::rpc_examples::EchoRequest* request,
						::gdt::rpc_examples::EchoResponse* response,
						::google::protobuf::Closure* done);
	void FormTest(::google::protobuf::RpcController* controller,
						const ::gdt::rpc_examples::FormTestMessage* request,
						::gdt::rpc_examples::FormTestMessage* response,
						::google::protobuf::Closure* done);
	private:
	::google::protobuf::RpcChannel* channel_;
	bool owns_channel_;
	GOOGLE_DISALLOW_EVIL_CONSTRUCTORS(EchoService_Stub);
};

// 客户端实际调用的是RpcChannel的CallMethod
void EchoService_Stub::Echo(::google::protobuf::RpcController* controller,
							const ::gdt::rpc_examples::EchoRequest* request,
							::gdt::rpc_examples::EchoResponse* response,
							::google::protobuf::Closure* done) {
	channel_-&amp;gt;CallMethod(descriptor()-&amp;gt;method(0),
						controller, request, response, done);
}
void EchoService_Stub::FormTest(::google::protobuf::RpcController* controller,
							const ::gdt::rpc_examples::FormTestMessage* request,
							::gdt::rpc_examples::FormTestMessage* response,
							::google::protobuf::Closure* done) {
	channel_-&amp;gt;CallMethod(descriptor()-&amp;gt;method(1),
						controller, request, response, done);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;system/io_frame/net_options&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;非阻塞IO: O_NONBLOCK&lt;/li&gt;
  &lt;li&gt;CloseOnExec: FD_CLOEXEC (该句柄在fork子进程后执行exec时就关闭)&lt;/li&gt;
  &lt;li&gt;SO_SNDBUF&lt;/li&gt;
  &lt;li&gt;SO_RCVBUF&lt;/li&gt;
  &lt;li&gt;SO_LINGER 设置套接口关闭后的行为&lt;/li&gt;
  &lt;li&gt;TCP_NODELAY：禁用Nagle‘s Algorithm(积累数据量到TCP Segment Size后发送)&lt;/li&gt;
  &lt;li&gt;SO_REUSEADDR：让端口释放后立即可以被再次使用&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;更多参考资料：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://stackoverflow.com/questions/4257410/what-are-so-sndbuf-and-so-recvbuf&quot;&gt;What are SO_SNDBUF and SO_RECVBUF&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/houlaizhe221/article/details/6580775&quot;&gt;非阻塞IO&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/chrisniu1984/article/details/7050663&quot;&gt;FD_CLOEXEC解析&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.chinaunix.net/uid-29075379-id-3905006.html&quot;&gt;SO_RCVBUF and SO_SNDBUF&lt;/a&gt;。
接收缓冲区被TCP和UDP用来缓存网络上来的数据，一直保存到应用进程读走为止。一个发送缓冲区和一个接收缓冲区，TCP的全双工的工作模式以及TCP的滑动窗口便是依赖于这两个独立的buffer以及此buffer的填充状态。&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/factor2000/article/details/3929816&quot;&gt;setsockopt ：SO_LINGER 选项设置&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://jerrypeng.me/2013/08/mythical-40ms-delay-and-tcp-nodelay/&quot;&gt;神秘的40毫秒延迟与 TCP_NODELAY&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cnblogs.com/mydomain/archive/2011/08/23/2150567.html&quot;&gt;SO_REUSEADDR的意义&lt;/a&gt;。一个端口释放后会等待两分钟之后才能再被使用，SO_REUSEADDR是让端口释放后立即就可以被再次使用。&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://man7.org/linux/man-pages/man7/socket.7.html&quot;&gt;socket option&lt;/a&gt; socketoptions.h/cc里面的实现也看看&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;section-6&quot;&gt;客户端代码&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;RpcClient:
负责所有RpcChannel对象的管理和对服务器端应答的处理&lt;/li&gt;
  &lt;li&gt;RpcChannel:
代表通讯通道，每个服务器地址对应于一个RpcChannel对象，客户端通过它向服务器端发送方法调用请求并接收结果。&lt;/li&gt;
  &lt;li&gt;RpcController:
存储一次rpc方法调用的上下文，包括对应的连接标识，方法执行结果等。&lt;/li&gt;
  &lt;li&gt;RpcServer:
服务器端的具体业务服务对象的容器，负责监听和接收客户端的请求，分发并调用实际的服务对象方法。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;rpc/client_connection&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;connection列表：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./system/io_frame/base_connection.h 这个是基类
./net/http/client/connection.h
./rpc/client_connection.h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;客户端connection:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./client_connection.h:95:class ClientConnection : public BaseConnection
./ckv_client_channel.h:23:class CkvClientConnection : public ClientConnection
./http_rpc_channel.h:24:class HttpRpcConnection : public ClientConnection
./qzone_client_channel.h:22:class QzoneClientConnection : public ClientConnection
./rpc_channel_impl.h:42:  virtual ClientConnection* NewConnection() = 0;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;用来在客户端建立连接，读取数据，发送数据等。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;rpc/rpc_channel&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;RpcChannelInterface : public ::google::protobuf::RpcChannel

void CallMethod(
	const google::protobuf::MethodDescriptor* method,
	google::protobuf::RpcController* controller,
	const google::protobuf::Message* request,
	google::protobuf::Message* response,
	google::protobuf::Closure* done);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;发送请求的背后,最后调用的其实是RpcChannel的CallMethod函数.所以,要实现RpcChannel类,最关键的就是要实现这个函数,在这个函数中完成发送请求的事务。&lt;/p&gt;

&lt;p&gt;客户端channel这边主要还是基于 BaseConnection这个在做。还是那两个入口函数，read和write。
ClientConnection里面会调用RpcClientCallContext。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./rpc_channel_impl.h:26:class RpcChannelImpl : public RpcChannelInterface {
./qzone_client_channel.h:35:class QzoneClientChannel : public RpcChannelImpl {
./http_rpc_channel.h:42:class HttpRpcChannel : public RpcChannelImpl {
./ckv_client_channel.h:33:class CkvClientChannel: public RpcChannelImpl {
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;rpc/rpc_controller&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;rpc_controller是一个rpc请求过程中的信息。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class RpcController : public google::protobuf::RpcController
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;主要保存下面这些信息：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;int error_code_;
std::string reason_;
int timeout_;
SocketAddressStorage remote_address_;
int64_t timestamp_;
bool in_use_;
kDefaultTimeout = 2000ms;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;rpc/load_balance&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;LoadBalancer是一个单例。实现了4种load_balancer。
客户端balancer列表，主要来做负载均衡。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./rpc/load_balancer.h 基类
./rpc/domain_load_balancer.h
./rpc/l5_load_balancer.h
./rpc/list_load_balancer.h
./rpc/single_load_balancer.h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;rpc/RpcClient&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;RpcClient是客户端的主类。一般情况下，一个客户端只需要有一个RpcClient。在初始化的时候，也可以设置线程个数，此个数等于PollThread的个数(多路器的个数)。&lt;/p&gt;

&lt;p&gt;利用RpcClient::OpenChannel创建RpcChannel。先根据scheme(目前有qzone,ckv,http三种)创建对应的Factory：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;RpcChannelFactory* factory = GDT_RPC_GET_CHANNEL_FACTORY(scheme)。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再利用factory创建channel:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;shared_ptr&amp;lt;RpcChannelInterface&amp;gt; channel_impl(factory-&amp;gt;CreateChannel(multiplexers_, server, NetOptions()))。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建channel时，先调用RpcChannel::Open。&lt;/p&gt;

&lt;p&gt;这里注册了三个Channel以及Factory&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./rpc/ckv_client_channel.cc:22:GDT_RPC_REGISTER_CHANNEL(&quot;ckv&quot;, CkvClientChannel);
./rpc/http_rpc_channel.cc:209:GDT_RPC_REGISTER_CHANNEL(&quot;http&quot;, HttpRpcChannel);
./rpc/qzone_client_channel.cc:269:GDT_RPC_REGISTER_CHANNEL(&quot;qzone&quot;, QzoneClientChannel);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Client代码流程&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;rpc client里发起请求，内部调用的都是RpcChannel。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Closure* done = ::NewCallback(this, &amp;amp;TestClient::AsyncCallDone, i,
							  controller, request, response);
EchoService::Stub stub(channels_[i % channels_.size()].get());
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果是http请求，则Stub调用的是 HttpRpcChannel::CallMethod。根据是否有done回调函数，分为同步和异步。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class HttpRpcChannel : public RpcChannelImpl
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;CallMethod实际调用的是RpcChannelImpl::Call(context)。Call函数里，先获取到connection。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;shared_ptr&amp;lt;ClientConnection&amp;gt; connection = GetRoute(call_context);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;GetRoute里首先判断是否已有connected_，如果没有新需要新建立连接。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;result.reset(NewConnection())
(!result-&amp;gt;Open((*multiplexers_)[index].get(), address, options_))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;http_rpc_channel里，实现的是&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class HttpRpcConnection : public ClientConnection。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在ClientConnection::Open函数里，调用了NonblockingConnect。&lt;/p&gt;

&lt;p&gt;OK，这下请求就算发送过去了。&lt;/p&gt;

&lt;p&gt;channel_number 这个设置主要是为什么？多一点有什么好处？channel是socket connect的个数。&lt;/p&gt;

&lt;p&gt;clinet 选取multiplexer的时候，所用的策略。实现在rpc_channel_impl.cc里。
如果只有一个connection的话，其实一直就用了一个channel。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;int index = connect_count_ % multiplexers_-&amp;gt;size();  // Round-robbin
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在客户端和服务端设置的threads number是PollThread的个数。channel number可以大于thread number。根据epoll机制，一个thread都可以支撑多个channel。&lt;/p&gt;

&lt;p&gt;一个channel里持有很多连接的map，可以共享连接。持有的是长连接通路。看这个函数就知道了：shared_ptr&lt;clientconnection&gt; RpcChannelImpl::GetRoute。&lt;/clientconnection&gt;&lt;/p&gt;

&lt;p&gt;只要channel没有新建，则连接一直保留，所以这时是长连接。&lt;/p&gt;

&lt;p&gt;创建连接的backtrace：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/client_build_connection.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;client异步调用的backtrace：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/client_async_call.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;section-7&quot;&gt;服务端代码&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;system/io_frame/Multiplexer&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;常见的多路复用有：PPC(Process Per Connection)，TPC(Thread PerConnection)，这些模型的缺陷是： resource usage and context-switching time influence the ability to handle many clients at a time。&lt;/p&gt;

&lt;p&gt;select的缺点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;最大并发数限制。一个进程所打开的FD（文件描述符）是有限制的，由FD_SETSIZE设置。&lt;/li&gt;
  &lt;li&gt;效率问题，select每次调用都会线性扫描全部的FD集合。O(n)复杂度。&lt;/li&gt;
  &lt;li&gt;内核/用户空间的内存拷贝问题。通过内存拷贝让内核把FD消息通知给用户空间。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;poll解决了第一个缺点，但第二，三个缺点依然存在。&lt;/p&gt;

&lt;p&gt;epoll是一个相对完美的解决方案。(1)最大FD个数很大(由/proc/sys/fs/file-max给出)；(2)epoll不仅会告诉应用程序有I/0事件到来，还会告诉应用程序相关的信息，不用遍历；(3)内核与用户态传递消息使用共享内存；&lt;/p&gt;

&lt;p&gt;epoll里还有一个level triggered和edge triggered的区分，level triggered vs edge triggered：edge-trigger模式中，epoll_wait仅当状态发生变化的时候才获得通知(即便缓冲区中还有未处理的数据)；而level-triggered模式下，epoll_wait只要有数据，将不断被触发。具体请参考&lt;a href=&quot;http://stackoverflow.com/questions/9162712/what-is-the-purpose-of-epolls-edge-triggered-option&quot;&gt;the purpose of epoll’s edge triggered option&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;分发器/多路器Multiplexer其中主要是通过epoll实现分发。&lt;a href=&quot;http://ssdr.github.io/2015/01/epoll-manual/&quot;&gt;epoll manual&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/sparkliang/article/details/4770655&quot;&gt;Linux Epoll介绍和程序实例&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://banu.com/blog/2/how-to-use-epoll-a-complete-example-in-c/&quot;&gt;How to use epoll? A complete example in C&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Multiplexer类的主要函数有：Create，Poll，AddDescriptor，RemoveDescriptor，ModifyEvent，RegisterTimer等。RegisterTimer可以用来注册一个定时任务，这在某些场景还是蛮有用的。&lt;/p&gt;

&lt;p&gt;调用过AddDescriptor的文件有：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./net/http/client/connection.cc:214:  multiplexer()-&amp;gt;AddDescriptor(this, Multiplexer::kIoEventReadWrite);
./net/http/server/http_server.cc:246:  if (!multiplexer-&amp;gt;AddDescriptor(connection.get())) {
./net/http/server/listener.cc:123:  multiplexer-&amp;gt;AddDescriptor(listener.get());
./rpc/client_connection.cc:208:    multiplexer()-&amp;gt;AddDescriptor(this, events);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Descriptor是FD描述类，其中持有成员变量fd以及close_callback_list，以及两个重要方法：OnWritable，OnReadable。这两个方法在连接时会被回调。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// MultiplexerNotifier is used to wake up epoll_wait
class MultiplexerNotifier : public Descriptor
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Multiplexer持有成员变量MultiplexerNotifier。即便每个multiplexer不监听socket，但都会create一个fd来用notify。&lt;/p&gt;

&lt;p&gt;更多Multiplexer的用法请参考：multiplexer_test。还解释了一个疑问：Poll函数的参数，是epoll_wait的timeout时间，也就是最多等待多久epoll_wait就返回。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;system/io_frame/poll_thread&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;PollThread类是结合Multiplexer一起使用的，即Thread + Multiplexer。也就是每个PollThread，都在loop multiplexer，如果有事件，就处理。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;system/io_frame/base_connection&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;base_connection继承自Descriptor。是connection基类：负责单次网络io。&lt;/p&gt;

&lt;p&gt;rpc server端，两个connection类，主要是用来处理服务端的socket连接。OnReadable, OnWritable。
RpcServerConnection::OnReadable主要做了对http和qzone协议的区分，然后如果是http协议，则主要调用HttpServerConnection，如果是qzone协议，则主要调用QzoneServiceHandler里的方法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class RpcServerConnection : public HttpServerConnection
./net/http/server/connection.h
./rpc/rpc_server_connection.h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;net/http_server&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;uri: 是用来解析url的一个工具类。&lt;/p&gt;

&lt;p&gt;譬如对于url:http://www.baidu.com/s?tn=monline_dg&amp;amp;bs=DVLOG&amp;amp;f=8&amp;amp;wd=glog+DVLOG#fragment，利用Uri类，可以解析出Host,port,Scheme,Fragment,Query等参数。&lt;/p&gt;

&lt;p&gt;对于Query参数：tn=monline_dg&amp;amp;bs=DVLOG&amp;amp;f=8&amp;amp;wd=glog+DVLOG，可以利用QueryParams类快速解析出每个name所对应的value。&lt;/p&gt;

&lt;p&gt;HttpServer常用的方法有：RegisterHttpHandler，RegisterPrefixHandler，Forward，RegisterStaticResource，AddLinkOnIndexPage等函数。&lt;/p&gt;

&lt;p&gt;看一下http_server_test，里面有一些不错的例子，我可以写一个test_server，实际试一下。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;rpc/rpc_server&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;class RpcServer : public HttpServer&lt;/p&gt;

&lt;p&gt;在HttpServer的基础上，又新增了几个方法：RegisterService，RegisterJceProtoService(注册jce服务)。这些注册服务无非就是把service插入到成员变量vector中。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;rpc/rpc_service_register&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;其中主要有三个方法，这个会被server端查找对应service的时候被用到。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 用于qzone协议，qzone+pb
const QzoneMethodContext* FindQzoneMethodContext(int qzone_version, int qzone_cmd) const;
// 用于qzone+jce协议
const JceMethodContext* FindJceMethodContext(int qzone_version, int qzone_cmd) const;
// 用于protobuf
bool FindMethodByName(const std::string&amp;amp; full_name,
	google::protobuf::Service** service,
	const google::protobuf::MethodDescriptor** method_descriptor,
	RpcErrorCode* error_code) const;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;service &amp;amp;&amp;amp; protocol&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;service列表：
这个里面用途不大&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./rpc/ckv_service.h
./rpc/jce_proto_service.h
./rpc/jce_service.h
./rpc/rpc_builtin_service.h
./rpc/udp_rpc_service.h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;protocol列表：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./rpc/http_rpc_protocol.h
./rpc/rpc_qzone_protocol.h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;目前就支持两种协议：qzone，http。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;GDT如果用http协议的话，则它的uri.Path类似为：”/rpc/gdt.rpc_examples.EchoService.Echo”。先发一个http包头，再发多个body(poppy的实现)，这里gdt rpc未做。&lt;/li&gt;
  &lt;li&gt;qzone协议的代码在：base_class_old/include/qzone_protocol.h&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;net/http/http_handler&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;handler列表如下，rpc目录下的两个handler，一个处理qzone协议，一个处理http协议。主要是在server角度，接受数据，解包，调用实际service，封包等操作。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./net/http/server/forward_handler.h
./net/http/server/gflags_handler.h
./net/http/server/http_handler.h
./net/http/server/static_resource_handler.h
./rpc/qzone_service_handler.h
./rpc/rpc_http_handler.h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里主要看下http_handler。HttpHandlerRegistry里持有两个成员变量：handler_map&lt;em&gt;，prefix_map&lt;/em&gt;。它的功能与rpc_service_register类似。
常用的方法有：两个Register，一个Find。Find访问的是HttpHandler。&lt;/p&gt;

&lt;p&gt;HttpHandler。它最重要的函数是：HandleRequest。具体实现为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;if (callback_) {
	HttpRequest* request_copy = new HttpRequest();
	request_copy-&amp;gt;Swap(request);
	HttpResponse* response = new HttpResponse();
	shared_ptr&amp;lt;HttpServerConnection&amp;gt; shared_connection = connection-&amp;gt;shared();
	Closure* done = NewCallback(OnResponseDone, shared_connection,
								request_copy, response, response_modifier);
	callback_-&amp;gt;Run(connection, request_copy, response, done);
} else {
	HttpResponse response;
	simple_callback_-&amp;gt;Run(request, &amp;amp;response);
	if (response_modifier)
		response_modifier-&amp;gt;Run(request, &amp;amp;response);
	PreprocessResponse(request, &amp;amp;response);
	connection-&amp;gt;SendResponse(response);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;HttpServerConnection继承自BaseConnection(Descriptor)。初始化的时候必须传入的参数有：fd, multiplexer, handler_registry。&lt;/p&gt;

&lt;p&gt;HttpServerConnection什么时候被初始化呢？在HttpServer的OnAccept函数里，每从listen_socket获取一个fd，则初始化一个connection。handler_registry来自于HttpServer持有的成员变量HttpHandlerRegistry，multiplexer则是根据fd从multiplexers_按规则取出一个。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;int index = fd % num_threads_;
Multiplexer* multiplexer = multiplexers_[index].get();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;执行流程&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;生成RpcServer::Options参数，这里可以指定服务端的poll_thread(即multiplexer)的线程数。&lt;/li&gt;
  &lt;li&gt;new RpcServer，因为RpcServer继承自HttpServer，HttpServer根据指定的threadnum(n)创建了n个multiplexer，并保存在multiplexers_。再接着RegisterDefaultPaths，背后调用的是handler_registry_-&amp;gt;Register。&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;给RpcServer注册服务。Register Proto/jce service是RpcServer的独有方法，RegisterHttpHander是HttpServer的方法。这里的注册只是将service存储在类成员变量vector里。&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; server.RegisterService(echo.get());
 server.RegisterHttpHandler(&quot;/test&quot;, NewPermanentCallback(TestPage));
&lt;/code&gt;&lt;/pre&gt;

    &lt;p&gt;protobuf的相关处理由rpc_http_handler.cc RpcHttpHandler完成。&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; shared_ptr&amp;lt;HttpHandler&amp;gt; handler(
     new RpcHttpHandler(rpc_service_registry_.get(),
                     rpc_service_stats_.get()));
 RegisterPrefixHandler(kHttpRpcPrefix, handler);
&lt;/code&gt;&lt;/pre&gt;

    &lt;p&gt;其他http处理由http_hander完成。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;执行server.Listen(FLAGS_ip, FLAGS_port)。在此之前，先执行RpcServer.BeforeListen()。这里会创建一个RpcServiceRegistry。把builtin服务和上一步注册的服务都写到RpcServiceRegistry。
接着再调用HttpServerListener::Listen，将HttpServerListener添加到multiplexer[0]，即multiplexer-&amp;gt;AddDescriptor(listener.get())。并且把HttpServer::OnAccept注册为Listen的回调函数accept_callback_。&lt;/p&gt;

    &lt;p&gt;HttpServerListener继承于Descriptor，当multiplexer[0]发现有listen_fs有IoEvent时，会调用HttpServerListener::OnReadable()。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;server.Start()，启动多个PollThread。每个PollThread的Loop函数，multiplexer都执行Poll()。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;最后是while (!server.IsStopped()){}。如果有client请求接入，最先会调用HttpServerListener::OnReadable()函数，这里会先调用accept函数，得到新连接的fd，再调用accept_callback_-&amp;gt;Run(fd)。&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; void HttpServer::OnAccept(int fd) {
     int index = fd % num_threads_;
     Multiplexer* multiplexer = multiplexers_[index].get();
     shared_ptr&amp;lt;Descriptor&amp;gt; connection = MakeConnection(fd, multiplexer);
     if (!connection) {
         close(fd);
         return;
     }
     if (!multiplexer-&amp;gt;AddDescriptor(connection.get())) {
         PLOG(WARNING) &amp;lt;&amp;lt; &quot;Add socket to poll failed, fd=&quot; &amp;lt;&amp;lt; fd;
         return;
     }
     connection_manager_-&amp;gt;Add(connection);
     connection-&amp;gt;PushCloseCallback(
         NewCallback(connection_manager_.get(), &amp;amp;ServerConnectionManager::Remove,
                     connection));
 }
&lt;/code&gt;&lt;/pre&gt;

    &lt;p&gt;上面代码把新连接的fd也加入到multiplexer的监听中。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;如果有数据可以读入，则将调用RpcServerConnection::OnReadable函数，首先判断是qzone协议，还是http协议。如果是qzone协议(protobuf or jce)，先注册RpcServerConnection::OnRequestDone()为done的回调函数，并创建QzoneServiceHandler。
再调用HttpServerConnection::OnReadable函数，即BaseConnection::OnReadable()函数。
在OnReadable()中，再调用BaseConnection::ReadPackets。
GetNextSendingBuffer,GetPacketSize,OnPacketReceived,OnEofReceived是BaseConnection的纯虚函数。RpcServerConnection对这些纯虚函数进行了重写。&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; int RpcServerConnection::GetPacketSize(const StringPiece&amp;amp; buffer) {
 return handler_ ? GetQzonePacketSize(buffer) :
                     HttpServerConnection::GetPacketSize(buffer);
 }
&lt;/code&gt;&lt;/pre&gt;

    &lt;p&gt;譬如上面，如果发现是qzone协议的话，则调用的是GetQzonePacketSize。&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/rpc_method_backtrace.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/rpc_method_write.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/rpc_method_requestdone.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;实际read packet和write packet的逻辑如下：&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; ./system/io_frame/base_connection.cc:147:  int n = read(fd(), buffer, buffer_size);
 ./system/io_frame/base_connection.cc:108:  int n = send(fd(), buffer, buffer_size, flags);
&lt;/code&gt;&lt;/pre&gt;

    &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/read_packet.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/write_packet.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;客户端和服务端都是这个脉路。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;把这两个图熟悉一下。一个是http协议请求时，server端的调用逻辑：&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/http_echo_bt.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;另一个是qzone协议请求时，server端的调用逻辑：&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/qzone_echo_bt.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Content-Type设置kContentTypeProtobuf，kContentTypeJson，kContentTypeProtobufText时不同的处理方式。&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;static const char* const kContentTypeJson = &quot;application/json&quot;;
static const char* const kContentTypeProtobuf = &quot;application/x-protobuf&quot;;
static const char* const kContentTypeProtobufText = &quot;text/x-protobuf&quot;;
&lt;/code&gt;&lt;/pre&gt;

    &lt;p&gt;需要在客户端那里设置相应的值，就可以得到处理逻辑。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;jce是怎么处理的&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;首先有jce格式，再定义一个proto格式。&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;实现下面三个的代码&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; bool JceStructToPbMessage(const QZAP_NEW::wup_qzap_search_display_req* jce,
                         MixerRequest* request);
 bool PbMessageToJceStruct(const MixerResponse* response,
                         QZAP_NEW::wup_qzap_search_display_rsp* jce);

 GDT_RPC_DEFINE_JCE_PROTO_SERVICE(0, JceMixerService, MixerService,
 GDT_RPC_JCE_METHOD(0, SearchAd, QZAP_NEW::wup_qzap_search_display_req,
                     QZAP_NEW::wup_qzap_search_display_rsp)
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;也就是说，如果是jce服务，rpc client会先将jce转成proto，再发起rpc调用，最后回报的时候再从proto解析为jce。&lt;/li&gt;
  &lt;li&gt;如果是qzone服务，其实理论上它也是header+proto，那么在接受包的时候，判断是qzone协议的话，先把包头去掉。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;CKV这里的搞法&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;复用了客户端的multiplexer，继承自RpcClientCallContext。
客户端主要负责写两个函数：EncodeRequest，OnPacketReceived。
EncodeRequest这个函数是对消息做打包。OnPacketReceived是对消息做解包。&lt;/p&gt;

&lt;p&gt;还要注意，同步调用和异步调用的区别。
异步调用的话，纯粹就是复用了multiplexer的功能，做回调。
而同步的话，其实就是调用后，做一个响应的等待，利用的是AutoResetEvent, “./system/concurrency/event.h”。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;poppy&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;除了二进制协议外，Poppy支持还以普通的HTTP协议，传输以JSON/protobuf文本格式定义的消息。很方便用各种脚本语言调用，甚至用 bash，调 wget/curl 都能发起 RPC 调用。
Poppy的二进制协议与一般的设计不一样的是，它是以HTTP协议头为基础建立起来的，只是建立连接后的最初的采用HTTP协议，后续的消息往来直接用二进制协议，所以效率还是比较高的。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;more&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;feeds的rpc服务可以多学习一下。adx的action, state与feeds的action, state以后也可以多学习学习！&lt;/p&gt;

&lt;p&gt;刚才又看了一遍 jeff的rpc，再看起来，感觉非常清晰了。
还是有帮助的。至少这下搞了，这些rpc的实现我基本都理解了。再也不会觉得玄乎了。&lt;/p&gt;

&lt;p&gt;还有支持json格式。支持默认页面请求。这个到时候分享的时候再讲一下。&lt;/p&gt;

&lt;p&gt;明天再问一下陈老师，看看现在channel是不是线程安全的。现在这个样子感觉不是呢？
而且默认的echo client，是4个线程，但是1个连接。也就是感觉4个epoll都在监听一个fd。这种感觉有点奇怪啊。&lt;/p&gt;

&lt;p&gt;做一下性能对比：在数据量比较小的时候，用qzone协议比http协议性能要好一倍。主要是包的大小影响比较大。&lt;/p&gt;

&lt;h3 id=&quot;section-8&quot;&gt;参考资料&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://stackoverflow.com/questions/121162/what-does-the-explicit-keyword-in-c-mean&quot;&gt;What does the explicit keyword in C++ mean?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://en.cppreference.com/w/cpp/language/final&quot;&gt;final specifier - C++ Reference&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;google rpc &lt;a href=&quot;http://www.grpc.io&quot;&gt;grpc&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/google/protobuf/wiki/Third-Party-Add-ons&quot;&gt;protobuf addons&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://developers.google.com/protocol-buffers/docs/reference/cpp-generated&quot;&gt;rpc definition&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://djt.qq.com/article/view/327&quot;&gt;poppy的官方资料&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Sun, 03 May 2015 00:00:00 +0800</pubDate>
        <link>http://yourdomain.com/2015/05/03/RPC%E6%B5%85%E6%9E%90.html</link>
        <guid isPermaLink="true">http://yourdomain.com/2015/05/03/RPC%E6%B5%85%E6%9E%90.html</guid>
        
        
      </item>
    
      <item>
        <title>Aggregation模型</title>
        <description>&lt;h1 id=&quot;aggregation&quot;&gt;Aggregation模型(集成学习)&lt;/h1&gt;

&lt;h4 id=&quot;author-vincentyaotencentcom&quot;&gt;author: vincentyao@tencent.com&lt;/h4&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;p&gt;写下这个主题文章，主要受到两个事情的启发：(1)同事kimmyzhang对&lt;a href=&quot;http://pan.baidu.com/s/1jGjAvhO&quot;&gt;GBDT的分享&lt;/a&gt;；(2)陈天奇的&lt;a href=&quot;https://github.com/dmlc/xgboost&quot;&gt;xgboost&lt;/a&gt; 开始被我们在实际工作中使用。以前对GBDT为代表的aggregation模型或多或少也有一些理解，但知识体系感不强，所以下面的文章主要是从体系角度梳理一下aggregation模型相关的内容。在梳理的过程中，参考了很多现有的资料，譬如kimmyzhang的分享ppt，陈天奇的ppt，林轩田老师的课程等，具体请见文末的参考文献，在此对这些作者表示感谢。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;开篇&lt;/h2&gt;

&lt;p&gt;Aggregation模型，即融合式的模型，也叫Ensemble Learning。那什么是Aggregation模型呢？通俗的讲，就是多算法融合。它的思想用一句俗语概括：三个臭皮匠，顶个诸葛亮。实际操作中，Aggregation模型把大大小小的多种算法融合在一起，共同协作来解决一个问题。这些算法可以是不同的算法，也可以是相同的算法。&lt;/p&gt;

&lt;p&gt;根据融合的方式，我们可以将Aggregation模型分为三种：(1)Uniform，将多个模型平均的合并在一起；(2)Linear组合，将多个模型利用linear model融合起来；(3)Conditional，不同的情形使用不同的模型，即将多个模型利用non-linear model融合起来。&lt;/p&gt;

&lt;p&gt;在下文中，我们用g_t 表示第t个单模型，Aggregation model所要做的就是把多个g_t 融合起来。而融合的过程，我们又可以分为两类：(1)Blending，已知多个g_t，再将多个g_t 融合起来，即aggregation after getting g_t；(2)Learning: 一边学习g_t，一边合并多个g_t，即aggregation as well as getting g_t。&lt;/p&gt;

&lt;p&gt;所以，对Aggregation模型基本的划分，则如下表所示。其中，对每一种融合类型，都列举了一种典型的Aggregation模型。&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Aggregation Type&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Blending(已知g，再融合多个g)&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Learning(一边学习g，一边融合多个g)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;uniform&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;voting/averaging&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;Bagging&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;non-uniform&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;linear&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;AdaBoost，GradientBoost&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;conditional&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;stacking(non-linear)&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;Decision Tree&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;hr /&gt;

&lt;p&gt;有了多种Aggregation模型后，还可以将Aggregation模型再融合。如果将bagging配上decision tree，则是random forest。如果将AdaBoost配上Decision Tree，则是AdaBoost-DTree。如果将GradientBoost配上Decision Tree，则是大名鼎鼎的GBDT(Gradient Boost Decision Tree)。&lt;/p&gt;

&lt;p&gt;OK，对Aggregation模型有了大体的认识后，下文将来讲述一些具有代表性的Aggregation模型。本文大致分为六个部分：第一部分简要介绍有监督学习；第二部分介绍Decision Tree；第三部分介绍Random forest；第四部分介绍AdaBoost；第五部分介绍Gradient Boost Decision Tree；最后对Aggregation模型做一下对比与总结。&lt;/p&gt;

&lt;h2 id=&quot;supervised-learning&quot;&gt;Supervised Learning基础&lt;/h2&gt;
&lt;p&gt;先介绍一些Supervised Learning的基础知识。&lt;/p&gt;

&lt;p&gt;首先是模型。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/model_description.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;其次是loss function和regularization。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/loss_regularization.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;将loss function和regularization合到一起，就是一些常见的有监督模型：Logistic regression，lasso等。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/ridge_and_lasso.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;decision-tree&quot;&gt;Decision Tree(决策树)&lt;/h2&gt;

&lt;p&gt;按照Aggregation的方式，可以将决策树的表达式写为：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;G(x)=\sum_{t=1}^T{q_t(x).g_t(x)}&lt;/script&gt;

&lt;p&gt;g_t表示一个base hypothesis，在决策树里，也就是每条路径的叶子节点。q_t表示条件，表示输入x 是不是在path t上。下图是一个决策树的例子，图中有5个叶子节点，则有5个g_t。通常情况下，我们都用递归形式来表示一个决策树。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/decision_tree_recursive_view.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;根据决策树的输出y的类型，可以将decision tree分为：分类树和回归树。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;分类树：预测分类标签；&lt;/li&gt;
  &lt;li&gt;回归树：预测实数值；回归树的结果是可以累加的；即regression tree is a function that maps the attributes to the score。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;另一种decision tree的表示方法如下所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/another_decision_tree.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;其中，该树有J个叶子节点，Rj 表示x的一个分离区域，1(.) 是indicator function。
b_j 是base learner的参数，如果是classification tree，则是该叶子节点的类目；如果是regression tree，则有 \(b_j = ave_{x_i \in R_j} {y_i}\)。决策树可以简单表述为：if \(x \in R_j\)，then \(h(x)=b_j\)。&lt;/p&gt;

&lt;p&gt;一棵树的训练过程为：根据一个指标，分裂训练集为几个子集。这个过程不断的在产生的子集里重复递归进行，即递归分割。当一个训练子集的类标都相同时递归停止。这种决策树的自顶向下归纳(TDITD) 是贪心算法的一种，也是目前为止最为常用的一种训练方法，但不是唯一的方法。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/decision_tree_train_algorithm.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;从decision tree的训练过程可以看到，训练的关键点是branching(即在每一步选择一个最好的属性来分裂)。那如何branching呢，通常的做法是：Split training set at “the best value” of “the best feature”。”最好”的定义是使得子节点中的训练集尽量的纯，不同的算法使用不同的指标来定义”最好”。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Information gain (ratio)：信息增益是用来衡量样本集S下属性A分裂时的信息熵减少量。信息增益是信息熵的有效减少量，值越高，说明失去的不确定性越多，那么它就应该越早作为决策的依据属性。例如ID3, C4.5 和 C5.0算法。&lt;/li&gt;
  &lt;li&gt;Gini index：基尼不纯度表示一个随机选中的样本在子集中被分错的可能性。基尼不纯度为这个样本被选中的概率乘以它被分错的概率。当一个节点中所有样本都是一个类时，基尼不纯度为零。例如CART算法。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;cart&quot;&gt;CART&lt;/h3&gt;

&lt;p&gt;CART全称”Classification and Regression Tree”，是一种较常用的决策树。为了简化决策过程，它有两个基本选择：(1)二叉树；(2)g_t(x)输出是一个常数。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/cart_two_choices.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;CART的branch利用Impurity function来衡量。如果目标是回归，利用regression error作为Impurity function。如果目标是分类，利用Gini index作为impurity function。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/cart_impurity_function.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;利用&lt;a href=&quot;http://mydisk.com/yzlv/webpage/datamining/xiti.html&quot;&gt;利用信息增益，决策树算法的计算过程演示&lt;/a&gt;的例子，如果采用Gini系数的话，计算过程为：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;按性别属性变量进行分裂。9/20&lt;em&gt;(1- (6/9)^2 - (3/9)^2) + 11/20&lt;/em&gt;(1- (7/11)^2 - (4/11)^2) = 0.4545。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;按车型变量进行分裂(运动 vs 豪华+家用)。9/20&lt;em&gt;(1- (1/9)^2 - (8/9)^2) + 11/20&lt;/em&gt;(1 - (11/11)^2) = 0.088。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;CART的termination条件是：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/CART_termination.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;所以，总结下来，CART是：fully-grown tree with constant leaves that come from bi-branching by purifying。&lt;/p&gt;

&lt;p&gt;关于CART算法的演算过程，可以参考：&lt;a href=&quot;http://www.academia.edu/7032069/An_example_of_calculating_gini_gain_in_CART&quot;&gt;An example of calculating gini gain in CART&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;一个fully-grown的CART，在训练数据上可以做到无差错(即Ein=0)，但这样往往是过拟合的。所以需要regularizer，通常的方法是：限制叶子节点的个数。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/CART_regularizer.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;decision-tree-1&quot;&gt;Decision tree小结&lt;/h3&gt;

&lt;p&gt;Regularization方法：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1)Number of nodes in the tree, depth；(2)L2 norm of the leaf weights&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;决策树的流程(From heuristics view)：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1)Split by information gain；(2)Prune the tree；(3)Maximum depth；(4)Smooth the leaf values&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;决策树的流程(From objective optimization view)：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1)Information gain -&amp;gt; training loss；(2)Pruning -&amp;gt; regularization defined by #nodes；(3)Max depth -&amp;gt; constraint on the function space；(4)Smoothing leaf values -&amp;gt; L2 regularization on leaf weights&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Decision tree优点：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1)易于理解和解释；(2)即可以处理数值型数据也可以处理类别型数据；(3)生成的模式简单，对噪声数据有很好的健壮性。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Decision tree缺点：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1)启发式的规则(前人的巧思)，缺乏理论基础，并且启发式规则很多，需要selection；(2)容易过拟合；(3)对那些有类别型属性的数据, 信息增益会有一定的偏置；(4)训练一棵最优的决策树是一个完全NP问题。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;几种决策树算法的区别：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;ID3算法使用信息增益。C4.5算法是在ID3算法的基础上采用&lt;strong&gt;信息增益率&lt;/strong&gt;的方法选择测试属性。ID3算法和C4.5算法虽然在对训练样本集的学习中可以尽可能多地挖掘信息，但其生成的决策树分支较大，规模较大。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;为了简化决策树的规模，提高生成决策树的效率，所以有了根据GINI系数来选择测试属性的决策树算法CART。
CART算法采用一种二分递归分割的技术，与基于信息熵的算法不同，CART算法对每次样本集的划分计算GINI系数，GINI系数，GINI系数越小则划分越合理。CART算法总是将当前样本集分割为两个子样本集，使得生成的决策树的每个非叶结点都只有两个分枝。因此CART算法生成的决策树是结构简洁的二叉树。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;更多参考资料&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.raychase.net/1275&quot;&gt;使用ID3算法构造决策树&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.raychase.net/1951&quot;&gt;C4.5&amp;amp; CART&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.autonlab.org/tutorials/dtree.html&quot;&gt;Decision Trees Tutorial by Andrew Moore&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Decision_tree_learning&quot;&gt;Wiki: Decision tree learning&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://pages.cs.wisc.edu/~jerryzhu/cs540/handouts/dt.pdf&quot;&gt;Machine Learning: Decision Trees.CS540&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;random-forest&quot;&gt;Random forest&lt;/h2&gt;

&lt;h3 id=&quot;bagging&quot;&gt;Bagging&lt;/h3&gt;

&lt;p&gt;开篇里已经简要介绍过uniform aggregation。在uniform融合过程中，diversity非常重要，可以利用多个不同的模型，可以用一个模型但不同的参数，可以采用不同的随机值初始化模型，可以随机抽样数据来训练多个模型。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/bagging_diversity.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Bagging(也称bootstrap aggregation)是一种基于data randomness的uniform的融合方式。
bootstrapping指从给定训练集中有放回的均匀抽样。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/bootstarpping_aggregation.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Uniform blending有一个好的特性，它可以降低模型的variance。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/uniform-blending-reduces-variance.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;random-forest-1&quot;&gt;Random forest&lt;/h3&gt;

&lt;p&gt;Bagging方法通过voting可以减小variance，而decision tree具有良好的bias表现，但有large variance(特别是fully-grown DTree)。所以一个直观的想法，能否将Bagging和Decision Tree这两者融合在一起，这样得到的新模型则具备了相对良好的bias和variance表现，这就是Random forest。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/Bagging-and-Decision-Tree.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;random forest(RF)的算法描述如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/random_forest_algorithm.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Bagging是data randomness，而为了增强diversity(即得到更多不同的g_t)，还可以对建立decision tree的features做抽样，即random subspace。该思想也可以套用在其他模型上(譬如svm，lr)。&lt;/p&gt;

&lt;p&gt;在build decision tree时，每一次做branch的时候，都可以做一次random re-sample feature，这样可以让g_t 更不一样。&lt;/p&gt;

&lt;p&gt;除此外，还可以利用random combination，也就是在branching时，不仅仅只是随机选择一个feature做切分，还可以random多个feature，将feature做linear combination后，再来做切分。random combination理论上就是一个perceptron过程。&lt;/p&gt;

&lt;p&gt;所以，Random forest是bagging + random-subspace &amp;amp; random-combination CART，可以看到randomness思想在random forest里无处不在。&lt;/p&gt;

&lt;p&gt;回顾一下bagging的过程，每次随机抽样一些数据，这样下去，总会有一些样本是一直未被抽中的，这些样本我们称之为out-of-bag examples，它们可以被当作validation set来使用。所以，random forest的另一个重要特性是：相比于通常的validation过程，RF可以做self-validation，也就是在训练的过程中，把model选择顺便也做了。&lt;/p&gt;

&lt;h2 id=&quot;adaboost&quot;&gt;AdaBoost&lt;/h2&gt;

&lt;h3 id=&quot;boosting&quot;&gt;Boosting&lt;/h3&gt;

&lt;p&gt;Boosting的思想相当的简单，对一份数据，建立M个模型（比如分类），一般这种模型比较简单，称为弱分类器(weak learner)。每次分类都将上一次分错的数据权重提高一点再进行分类，这样最终得到的分类器在测试数据与训练数据上都可以得到比较好的成绩。Boosting也就是开篇所述的linear blending模型。&lt;/p&gt;

&lt;p&gt;boosting可以用下面公式来表示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/boosting_formula1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;其中alpha是权重，y_m是弱分类器，整体就是一个linear模型。&lt;/p&gt;

&lt;p&gt;从Function Space里的Numerical Optimization角度看Boosting。boosting也叫forward stagewise additive modeling，因为在迭代的过程中，我们不能再回退去修改以前的参数，一切只能向前看了。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/Additive_Training_process.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/Function-Space-optimizaition1.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/Function-Space-optimizaition2.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/Function-Space-optimizaition3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;不同的损失函数和极小化损失函数方法决定了boosting的最终效果，先说几个常见的boosting：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/boosting_category.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;上图出自 Machine Learning A Probabilistic Perspective Table 16.1(P556)。不过其中注释的algorithm和个人理解有些不一致，Absolute error应该是叫Least Absolute Deviation (LAD) Regression。Gradient boosting的常见示例是squared loss。&lt;/p&gt;

&lt;p&gt;Boosting方法共性：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Train one base learner at a time&lt;/li&gt;
  &lt;li&gt;Focus it on the mistakes of its predecessors&lt;/li&gt;
  &lt;li&gt;Weight it based on how ‘useful’ it is in the ensemble (not on its training error)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;更多请参考：&lt;a href=&quot;http://www.cs.man.ac.uk/~stapenr5/boosting.pdf&quot;&gt;Introduction to Boosting&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;adaboost-1&quot;&gt;AdaBoost&lt;/h3&gt;
&lt;p&gt;AdaBoost(Adaptive Boosting)由Yoav Freund和Robert Schapire提出。AdaBoost方法的自适应在于：前一个分类器分错的样本会被用来训练下一个分类器。AdaBoost方法对于噪声数据和异常数据很敏感。但在一些问题中，AdaBoost方法相对于大多数其它学习算法而言，不会很容易出现过拟合现象。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/adaboost_algorithm1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;AdaBoost方法是一种迭代算法，在每一轮中加入一个新的弱分类器，直到达到某个预定的足够小的错误率。每一个训练样本都被赋予一个权重，表明它被某个分类器选入训练集的概率。如果某个样本点已经被准确地分类，那么在构造下一个训练集中，它被选中的概率就被降低；相反，如果某个样本点没有被准确地分类，那么它的权重就得到提高。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/adaboost_pseudo_code.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;通过这样的方式，AdaBoost方法能“聚焦于”那些较难分（更富信息）的样本上。在具体实现上，最初令每个样本的权重都相等，对于第k次迭代操作，我们就根据这些权重来选取样本点，进而训练分类器g_k。然后就根据这个分类器，来提高被它分错的的样本的权重，并降低被正确分类的样本权重。然后，权重更新过的样本集被用于训练下一个分类器g_k。整个训练过程如此迭代地进行下去。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/Theoretical-Guarantee-of-AdaBoost.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;AdaBoost方法中使用的分类器可能很弱（比如出现很大错误率），但只要它的分类效果比随机好一点（比如两类问题分类错误率略小于0.5），就能够改善最终得到的模型。而错误率高于随机分类器的弱分类器也是有用的，因为在最终得到的多个分类器的线性组合中，可以给它们赋予负系数，同样也能提升分类效果。&lt;/p&gt;

&lt;h3 id=&quot;adaboost-dtree&quot;&gt;AdaBoost-DTree&lt;/h3&gt;
&lt;p&gt;将AdaBoost和decision tree融合起来，就是AdaBoost-DTree，如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/adaboost_decision_tree.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在AdaBoost模型中，需要给予不同样本不同的权重。如果模型是svm或lr的话，很容易把weight加到每个instance上，只需要在计算loss function的时候，乘上相应的weight系数即可。&lt;/p&gt;

&lt;p&gt;但在AdaBoost-DTree模型中，对于DTree，不方便赋予weight给不同样本。这时可以利用bootstrap的思想。&lt;/p&gt;

&lt;p&gt;Bootstrap，它在每一步迭代时不改变模型本身，而是从N个instance训练集中按随机抽取N个instance出来（单个instance可以被重复sample），对着这N个新的instance再训练一轮，由于数据集变了迭代模型训练结果也不一样。&lt;/p&gt;

&lt;p&gt;在AdaBoost-DTree模型做样本抽样时，并不是Uniform抽样，而是根据一定概率来抽样。如果一个instance在前面分错的越厉害，它的概率就被设的越高，这样就能同样达到逐步关注被分错的instance，逐步完善的效果。&lt;/p&gt;

&lt;h3 id=&quot;optimizationadaboost&quot;&gt;Optimization视角看AdaBoost&lt;/h3&gt;

&lt;p&gt;从前述AdaBoost的训练过程，可以得到其训练目标为：让正确instance的weight越来越小，正确的instance个数越多越好。&lt;/p&gt;

&lt;p&gt;那么其最终目标为：第T次训练时，所有instance的weight之和最小。写出其Error function为：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/AdaBoost-Error-Function1.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/AdaBoost-Error-Function2.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/AdaBoost-Error-Function3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;从上面的优化目标，可以得到AdaBoost的loss function是exponential loss。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/adaboost_lost_function.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;为什么选择Exponential Loss？&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Loss is higher when a prediction is wrong.&lt;/li&gt;
  &lt;li&gt;Loss is steeper when a prediction is wrong.&lt;/li&gt;
  &lt;li&gt;Precise reasons later&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;求解AdaBoost的优化目标，得到下一个h(x_n)即为A(base algorithm)，h上的权重即为a_t。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/Gradient-Descent-on-AdaBoost.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;具体的推导过程参考”机器学习技法”课程，这里不赘述了。总结下来，AdaBoost：steepest decent with approximate functional gradient。&lt;/p&gt;

&lt;h2 id=&quot;gbdt&quot;&gt;GBDT&lt;/h2&gt;

&lt;h3 id=&quot;gradientboost&quot;&gt;GradientBoost&lt;/h3&gt;

&lt;p&gt;前面已经对boost方法做了一些介绍，这里再针对GradientBoost从公式推导角度再做更细致的介绍。&lt;/p&gt;

&lt;p&gt;首先GradientBoost如所有boost方法一样，可以将最终模型表达式写为：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/gbdt_formula1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;对于有限的训练样本 \({[y_i,x_i]}_1^N\)，下式是我们要优化的目标：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/gbdt_formula2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;因为boost是一种stagewise additive方法，对于其每一次迭代，m=1,2 … M，优化目标为：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/gbdt_formula3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;直接求解上面的目标函数会比较复杂。所以，我们换个思路，考虑到通常情况下，梯度下降方向是一个合理的优化方向，那么我们可以先求出m-1时的负梯度方向 -g，然后尽可能把h(x)往 -g 方向上拟合。所以有：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/gbdt_formula5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/gbdt_formula6.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/gbdt_formula7.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;那么第m次迭代计算后，得到的模型为：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/gbdt_formula4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;将上面的计算过程整体串起来，则有：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/gradient_boost_process.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Gradient Boosting是一种Boosting的方法。与传统的Boost的区别是，每一次的计算是为了减少上一次的残差(residual)，而为了消除残差，我们可以在残差减少的梯度(Gradient)方向上建立一个新的模型。所以说，在Gradient Boost中，每个新的模型的建立是为了使得之前模型的残差往梯度方向减少，与传统Boost对正确、错误的样本进行加权有着很大的区别。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/gradient_boost1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;GradientBoost: allows extension to different err for regression/soft classification/etc。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/gradientBoost_for_regression.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/residuals_for_gbdt.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;如果整体loss function取squared error，即L(y,F) = (y - F)^2 / 2。此时，我们得到Least-squares regression。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/ls_boost_algorithm.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;如果整体loss function取absolute error，即L(y,F) =&lt;/td&gt;
      &lt;td&gt;y - F&lt;/td&gt;
      &lt;td&gt;。此时有：&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/least_absolute_deviation_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/least_absolute_deviation_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;更多请参考&lt;a href=&quot;http://docs.salford-systems.com/GreedyFuncApproxSS.pdf&quot;&gt;Greedy Function Approximation: A Gradient Boosting Machine&lt;/a&gt;，&lt;a href=&quot;http://www.cnblogs.com/LeftNotEasy/archive/2011/01/02/machine-learning-boosting-and-gradient-boosting.html&quot;&gt;模型组合(Model Combining)之Boosting与Gradient Boosting&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;gradient-boost-decision-tree&quot;&gt;Gradient boost decision tree&lt;/h3&gt;
&lt;p&gt;GBDT(Gradient boost decision tree)，又叫MART(Multiple Additive Regression Tree)。目前GBDT有两个不同的描述版本。&lt;a href=&quot;http://hi.baidu.com/hehehehello/item/96cc42e45c16e7265a2d64ee&quot;&gt;残差版本&lt;/a&gt;把GBDT当做一个残差迭代树，认为每一棵回归树都在学习前N-1棵树的残差。&lt;a href=&quot;http://blog.csdn.net/dark_scope/article/details/24863289&quot;&gt;Gradient版本&lt;/a&gt;把GBDT说成一个梯度迭代树，使用梯度下降法求解，认为每一棵回归树在学习前N-1棵树的梯度下降值。这两种描述版本我认为是一致的，因为损失函数的梯度下降方向，就是残差方向。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/gbdt_algorithm1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Gradient Boosting Machine：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/gradient_boosting_machine.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;GB+DT+squared error loss：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/gbdt_squaredloss.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;更多请参考：&lt;a href=&quot;http://www.360doc.com/content/14/1205/20/11230013_430680346.shtml&quot;&gt;GBDT迭代决策树&lt;/a&gt;，&lt;a href=&quot;&quot;&gt;kimmyzhang-GBDT&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&quot;regularization&quot;&gt;Regularization&lt;/h3&gt;
&lt;p&gt;GBDT的常见regularization方法有：控制树的个数(即early stop)，控制每一棵树的复杂度。&lt;/p&gt;

&lt;p&gt;而控制一棵树的复杂度，可以控制树的深度，叶子节点个数，以及叶子节点的weight。如下式所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/Regularization_formula.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;除此外，还可以在每次训练树时，对data和feature做subsampling。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/sampling_shrinkage.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;另一个常见的正则方法是Shrinkage。Shrinkage（缩减）的思想认为，每次走一小步逐渐逼近结果的效果，要比每次迈一大步很快逼近结果的方式更容易避免过拟合。即它不完全信任每一个棵残差树，它认为每棵树只学到了真理的一小部分，累加的时候只累加一小部分，通过多学几棵树弥补不足。This means we do not do full optimization in each step and reserve chance for future rounds, it helps prevent overfitting。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/shrinkage_algorithm.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;gbdt-1&quot;&gt;GBDT应用&lt;/h3&gt;
&lt;p&gt;如果想通过代码学习GBDT，可以参考code：&lt;a href=&quot;https://github.com/zzbased/kaggle-2014-criteo&quot;&gt;kaggle-2014-criteo my notes&lt;/a&gt;，&lt;a href=&quot;https://github.com/dmlc/xgboost&quot;&gt;陈天奇的xgboost&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;在xgboost中，GBDT的编码实现步骤为：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/xgboost1.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/xgboost2.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/xgboost3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;相比于gbdt的常见算法，为什么要推导出上面优化目标，主要原因为Engineering benefit。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/xgboost4.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/xgboost5.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/xgboost6.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/xgboost7.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/xgboost8.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/xgboost10.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;最近，gbdt模型在搜索排序里得到大量应用。除此外，GBDT还可以用来做特征选择和特征组合。&lt;/p&gt;

&lt;p&gt;特征选择，参考&lt;a href=&quot;http://fr.slideshare.net/MichaelBENESTY/feature-importance-analysis-with-xgboost-in-tax-audit&quot;&gt;Feature Importance Analysis with XGBoost in Tax audit&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;特征组合里，比较有代表性的是facebook的文章
&lt;a href=&quot;http://quinonero.net/Publications/predicting-clicks-facebook.pdf&quot;&gt;Practical Lessons from Predicting Clicks on Ads at Facebook&lt;/a&gt;提到的方法，它利用GBDT+LR做CTR预估，取得不错的效果。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/facebook_gdbt_lr.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;总结&lt;/h2&gt;

&lt;h3 id=&quot;aggregation-1&quot;&gt;Aggregation方法总结&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Blending Models&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;blending: aggregate after getting diverse g_t&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/blending_models.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Aggregation-Learning Models&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;learning: aggregate as well as getting diverse g_t&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/Aggregation-Learningmodels.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Aggregation of Aggregation Models&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/Aggregation-of-Aggregation-Models.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;为什么Aggregation方法是有效的？&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;可以从两方面来看，其一通过Aggregation可以生成复杂的hypotheses，相当于做了feature transform；其二，生成的G(x)更加moderate，例如下图中PLA的uniform mix就是large-margin，相当于做了regularization。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/aggregation_works.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;boosting-1&quot;&gt;Boosting方法比较&lt;/h3&gt;

&lt;p&gt;关于boosting方法的比较，上文中mlapp的图已经表达得比较明确了。这里再在公式上做一下细化。&lt;/p&gt;

&lt;p&gt;Square and Absolute Error：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/Square-and-Absolute-Error.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Logistic Loss and LogitBoost：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/Logistic-Loss-and-LogitBoost.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Exponential Loss and Adaboost：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/Exponential-Loss-and-Adaboost.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;下面把一些常见方法的特点再加强阐述下。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Adaboost：一种boost方法，它按分类对错，分配不同的weight，计算cost function时使用这些weight，从而让“错分的样本权重越来越大，使它们更被重视”。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;GBDT的核心在于：每一棵树学的是之前所有树的结论和残差。每一步的残差计算其实变相地增大了分错instance的权重，而已经分对的instance则都趋向于0。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Bootstrap，它在每一步迭代时不改变模型本身，也不计算残差，而是从N个instance训练集中按一定概率重新抽取N个instance出来（单个instance可以被重复sample），对着这N个新的instance再训练一轮，由于数据集变了迭代模型训练结果也不一样。&lt;/p&gt;

    &lt;p&gt;如果一个instance被前面分错的越厉害，它的概率就被设的越高，这样就能同样达到逐步关注被分错的instance，逐步完善的效果。这里是决策树给予不同样本不同权重的方法。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;httpwww52csorgp383&quot;&gt;一篇不错的综述性文章：&lt;a href=&quot;http://www.52cs.org/?p=383&quot;&gt;集成学习：机器学习刀光剑影之屠龙刀&lt;/a&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Bagging和boosting也是当今两大杀器RF（Random Forests）和GBDT（Gradient Boosting Decision Tree）之所以成功的主要秘诀。&lt;/li&gt;
  &lt;li&gt;Bagging主要减小了variance，而Boosting主要减小了bias，而这种差异直接推动结合Bagging和Boosting的MultiBoosting的诞生。参考:Geoffrey I. Webb (2000). MultiBoosting: A Technique for Combining Boosting and Wagging. Machine Learning. Vol.40(No.2)&lt;/li&gt;
  &lt;li&gt;LMT(Logistic Model Tree ) 应运而生，它把LR和DT嫁接在一起，实现了两者的优势互补。对比GBDT和DT会发现GBDT较DT有两点好处：1）GBDT本身是集成学习的一种算法，效果可能较DT好；2）GBDT中的DT一般是Regression Tree，所以预测出来的绝对值本身就有比较意义，而LR能很好利用这个值。这是个非常大的优势，尤其是用到广告竞价排序的场景上。&lt;/li&gt;
  &lt;li&gt;关于Facebook的GBDT+LR方法，它出发点简单直接，效果也好。但这个朴素的做法之后，有很多可以从多个角度来分析的亮点：可以是简单的stacking，也可以认为LR实际上对GBDT的所有树做了选择集成，还可以GBDT学习了基，甚至可以认为最后的LR实际对树做了稀疏求解，做了平滑。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-2&quot;&gt;更多学习资料&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://suanfazu.com/t/gbdt-die-dai-jue-ce-shu-ru-men-jiao-cheng/135&quot;&gt;Gbdt迭代决策树入门教程&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.schonlau.net/publication/05stata_boosting.pdf&quot;&gt;Boosting Decision Tree入门教程&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://research.microsoft.com/pubs/132652/MSR-TR-2010-82.pdf&quot;&gt;LambdaMART用于搜索排序入门教程&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://insidebigdata.com/2014/12/18/ask-data-scientist-ensemble-methods/&quot;&gt;文章 Ask a Data Scientist: Ensemble Methods&lt;/a&gt; - &lt;a href=&quot;http://cvchina.net/post/107.html&quot;&gt;决策树模型组合之随机森林与GBDT&lt;/a&gt;
  &lt;a href=&quot;http://www.cnblogs.com/LeftNotEasy/archive/2011/03/07/random-forest-and-gbdt.html&quot;&gt;机器学习中的算法(1)-决策树模型组合之随机森林与GBDT link2&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/tqchen/xgboost&quot;&gt;xgboost - eXtreme Gradient Boosting (GBDT or GBRT) Library&lt;/a&gt;, also support distributed learning。并行实现推荐 @陈天奇怪 的xgboost，实际例子见@phunter_lau 最近的文章 http://t.cn/RhKAWac&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://machinelearning.wustl.edu/pmwiki.php/Main/Pgbrt&quot;&gt;pGBRT: Parallel Gradient Boosted Regression Trees&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://bigdata.memect.com/?tag=GBDT&quot;&gt;更多GBDT&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.hankcs.com/ml/decision-tree.html&quot;&gt;决策树 用Python实现了决策树的ID3生成算法和C4.5生成算法&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://t.cn/RZBT6Ap&quot;&gt;论文 Understanding Random Forests: From Theory to Practice&lt;/a&gt;
Louppe, Gilles的博士论文，全面了解随机森林的好材料。&lt;a href=&quot;http://t.cn/RZBTobH&quot;&gt;pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.datadive.net/interpreting-random-forests/&quot;&gt;Interpreting random forests&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://toutiao.com/a4055188882/&quot;&gt;计算机视觉：随机森林算法在人体识别中的应用&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zh.coursera.org/course/ntumltwo&quot;&gt;机器学习技法课程&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://statweb.stanford.edu/~jhf/ftp/trebst.pdf&quot;&gt;J. Friedman(1999). Greedy Function Approximation: A Gradient Boosting Machine&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;J. Friedman(1999). Stochastic Gradient Boosting.&lt;/li&gt;
  &lt;li&gt;J. Friedman, T. Hastie, R. Tibshirani(2000). Additive Logistic Regression - A Statistical View of Boosting.&lt;/li&gt;
  &lt;li&gt;T. Hastie, R. Tibshirani, J. Friedman(2008). Chapter 10 of The Elements of Statistical Learning(2e).&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://pan.baidu.com/s/1jGjAvhO&quot;&gt;GBDT的分享-by kimmyzhang&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.52cs.org/?p=429&quot;&gt;Boosted Tree - by 陈天奇&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Fri, 03 Apr 2015 00:00:00 +0800</pubDate>
        <link>http://yourdomain.com/2015/04/03/Aggregation%E6%A8%A1%E5%9E%8B.html</link>
        <guid isPermaLink="true">http://yourdomain.com/2015/04/03/Aggregation%E6%A8%A1%E5%9E%8B.html</guid>
        
        
      </item>
    
      <item>
        <title>Python零碎</title>
        <description>&lt;h1 id=&quot;python&quot;&gt;Python零碎&lt;/h1&gt;

&lt;h2 id=&quot;section&quot;&gt;基础&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://www.cnblogs.com/vivilisa/archive/2009/03/19/1417083.html&quot;&gt;Enumerate&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;enumerate会将数组或列表组成一个索引序列。使我们再获取索引和索引内容的时候更加方便如下：
	for index，text in enumerate(list)):
	   print index ,text&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://stackoverflow.com/questions/3815860/python-how-to-exit-main-function&quot;&gt;Python how to exit main function&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.cnblogs.com/65702708/archive/2010/09/14/1826362.html&quot;&gt;Python sorted函数&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://blog.csdn.net/emaste_r/article/details/8447192&quot;&gt;Python各种类型转换-int,str,char,float,ord,hex,oct等&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;int(x [,base ])         将x转换为一个整数
long(x [,base ])        将x转换为一个长整数
float(x )               将x转换到一个浮点数
complex(real [,imag ])  创建一个复数
str(x )                 将对象 x 转换为字符串
repr(x )                将对象 x 转换为表达式字符串
eval(str )              用来计算在字符串中的有效Python表达式,并返回一个对象
tuple(s )               将序列 s 转换为一个元组
list(s )                将序列 s 转换为一个列表
chr(x )                 将一个整数转换为一个字符
unichr(x )              将一个整数转换为Unicode字符
ord(x )                 将一个字符转换为它的整数值
hex(x )                 将一个整数转换为一个十六进制字符串
oct(x )                 将一个整数转换为一个八进制字符串
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&quot;http://stackoverflow.com/questions/379906/parse-string-to-float-or-int&quot;&gt;Parse Float String to Int&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; a = &quot;545.2222&quot;
&amp;gt;&amp;gt;&amp;gt; float(a)
545.22220000000004
&amp;gt;&amp;gt;&amp;gt; int(float(a))
545
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一些python积累点：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1. from import 与 import的区别。一般推荐用import。
2. 两个list相加：map(operator.add, stat_result.advertiser_map[advertiser_key], stat_value)
3. 判断字符串为空：is_null = （len(str.strip()) == 0)
4. list遍历： for element in list。dict遍历： for (k,v) in dict。
5. 直接写在类里的是：相当于是静态变量，是类公用的。写在__init__里，才是成员变量。
6. 读取文件：for line in open(filename)
7. 字典操作：dict.keys(), dict.values(), dict.items()
8. 按照dict的value做排序：advertiser_list = sorted(yesterday_result.advertiser_map.items(), key=lambda d: d[1][3], reverse=True)
9. string format: &quot;%1.3f%%&quot; % (a*100)
10. [Python中子类怎样调用父类方法](http://blog.csdn.net/caz28/article/details/8270709)。访问父类方法：BaseProcessor.CalcRelativeRatio(self, mapping_dict)，访问父类变量，只需要在子类__init__函数调用：BaseProcessor.__init__(self)。
11. list输出到string：output += &#39; &#39;.join( str(x) for x in v1 )
12. 查看帮助。在python命令行交互模式中，执行help(package/class/function)，譬如help(sys)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-1&quot;&gt;常用库&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;@phunter_lau 推荐：多种非常见数据结构以及NB的各种trie的python包 &lt;a href=&quot;http://kmike.ru/python-data-structures/&quot;&gt;link&lt;/a&gt; 作者是datrie，marisa-trie，DAWG Python这些利器的作者。他列举了python中Bloom Filter， 各种列表和链表，图，Aho-Corasick自动机，前缀树Trie，和其他各种树等高级数据结构。使用它们可能大大加速你的程序&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Wed, 01 Apr 2015 00:00:00 +0800</pubDate>
        <link>http://yourdomain.com/2015/04/01/python%E9%9B%B6%E7%A2%8E.html</link>
        <guid isPermaLink="true">http://yourdomain.com/2015/04/01/python%E9%9B%B6%E7%A2%8E.html</guid>
        
        
      </item>
    
      <item>
        <title>Expectation-Maximization algorithm</title>
        <description>&lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;h1 id=&quot;em&quot;&gt;EM算法随笔&lt;/h1&gt;

&lt;h2 id=&quot;em-overview&quot;&gt;EM overview&lt;/h2&gt;

&lt;p&gt;The EM algorithm belongs to a broader class of &lt;strong&gt;alternating minimization algorithms&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;EM is one such hill-climbing algorithm that converges to a local maximum of the likelihood surface.&lt;/p&gt;

&lt;p&gt;As the name suggests, the EM algorithm alternates between an expectation and a maximization step. The “E step” finds a lower bound that is equal to the log-likelihood function at the current parameter estimate θ_k. The “M step” generates the next estimate θ_k+1 as the parameter that maximizes this greatest lower bound.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/em/em_image1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;x是隐变量。下面变换中用到了著名的Jensen不等式&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/em/em_q_function.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/em/em_formula2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;参考文献：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Zhai chengxiang老师的经典EM note：&lt;a href=&quot;http://www.cs.ust.hk/~qyang/Teaching/537/PPT/em-note.pdf&quot;&gt;A Note on the Expectation-Maximization (EM) Algorithm&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Shane M. Haas的&lt;a href=&quot;http://www.mit.edu/~6.454/www_fall_2002/shaas/summary.pdf&quot;&gt;The Expectation-Maximization and Alternating Minimization Algorithms&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;em-1&quot;&gt;EM算法细节&lt;/h2&gt;

&lt;h3 id=&quot;jensen&quot;&gt;Jensen不等式&lt;/h3&gt;
&lt;p&gt;Jensen不等式表述如下：&lt;/p&gt;

&lt;p&gt;如果f是凸函数，X是随机变量，那么：E[f(X)]&amp;gt;=f(E[X])&lt;/p&gt;

&lt;p&gt;特别地，如果f是严格凸函数，当且仅当X是常量时，上式取等号。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/em/jensen_inequality.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Jensen不等式应用于凹函数时，不等号方向反向。&lt;/p&gt;

&lt;h3 id=&quot;section&quot;&gt;极大似然&lt;/h3&gt;

&lt;p&gt;给定的训练样本是{x(1),…,x(m)}，样本间独立，我们想找到每个样例隐含的类别z，能使得p(x,z)最大。似然函数为：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/em/em_likelihood.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;极大化上面似然函数，需要对函数求导。但里面有”和的对数”，求导后形式会非常复杂（自己可以想象下log(f1(x)+ f2(x)+ f3(x)+…)复合函数的求导）。所以我们要做一个变换，如下图所示，经过这个变换后，”和的对数”变成了”对数的和”，这样计算起来就简单多了。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/em/jensen_transform.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;怎么变换来的呢？其中Q(z)表示隐含变量z的某种分布。由于f(x)=log(x)为凹函数，根据Jensen不等式有：f(E[X]) &amp;gt;= E[f(X)]，即：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/em/jensen_transform2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;OK，通过上面的变换，我们求得了似然函数的下届。我们可以优化这个下届，使其逼近似然函数(incomplete data)。&lt;/p&gt;

&lt;p&gt;按照这个思路，我们要找到等式成立的条件。根据Jensen不等式，要想让等式成立，需要让随机变量变成常数值，这里得到： &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/em/equality_condition.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/em/q_condition.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/em/p_condition.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/em/q_p_z_relation.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;从而，我们推导出：在固定其他参数后，Q(z)的计算公式就是z的后验概率。这一步也就是所谓的E步，求出Q函数，表示的是完全数据对数似然函数相对于隐变量的期望，而得到这个期望，也就是求出z的后验概率P(z&lt;/td&gt;
      &lt;td&gt;x，θ)。&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;M步呢，就是极大化Q函数，也就是优化θ的过程。&lt;/p&gt;

&lt;p&gt;归纳下来，EM算法的基本步骤为：E步固定θ，优化Q；M步固定Q，优化θ。交替将极值推向最大。&lt;/p&gt;

&lt;h3 id=&quot;em-2&quot;&gt;为什么EM是有效的?&lt;/h3&gt;

&lt;p&gt;蓝线代表当前参数下的L函数，也就是目标函数的下界，E步的时候计算L函数，M步的时候通过重新计算θ得到L的最大值。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/em/em_prove1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;emplsa&quot;&gt;EM于PLSA&lt;/h2&gt;

&lt;p&gt;PLSA的图模型：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/em/plsa_graph_model.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;PLSA的生成过程：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/em/plsa_procedure.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(di,wj)的联合分布为：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/em/plsa_formula1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;PLSA的最大似然函数为：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/em/plsa_likelihood.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;注意上式中，第一项的完整形式为：
\(\sum&lt;em&gt;{i=1}^N{\sum&lt;/em&gt;{j=1}^M{n(d_i,w_j) log(p(d_i))}}\)。&lt;/p&gt;

&lt;p&gt;对于这样的包含”隐含变量”或者”缺失数据”的概率模型参数估计问题，我们采用EM算法。这两个概念是互相联系的，当我们的模型中有”隐含变量”时，我们会认为原始数据是”不完全的数据”，因为隐含变量的值无法观察到；反过来，当我们的数据incomplete时，我们可以通过增加隐含变量来对”缺失数据”建模。&lt;/p&gt;

&lt;p&gt;EM算法的步骤是：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;E步骤：Given当前估计的参数条件下，求隐含变量的后验概率。
an expectation (E) step where posterior probabilities are computed for the latent variables, based on the current estimates of the parameters。&lt;/li&gt;
  &lt;li&gt;M步骤：最大化Complete data对数似然函数的期望，此时我们使用E步骤里计算的隐含变量的后验概率，得到新的参数值。
a maximization (M) step, where parameters are updated based on the so-called expected complete data log-likelihood which depends on the posterior probabilities computed in the E-step。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;两步迭代进行直到收敛。&lt;/p&gt;

&lt;p&gt;这里是通过最大化”complete data”似然函数的期望，来最大化”incomplete data”的似然函数，以便得到求似然函数最大值更为简单的计算途径。&lt;/p&gt;

&lt;p&gt;PLSA的E-Step：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/em/plsa_e_step.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;PLSA的M-step，M-step的推导过程请参考下面的文献。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/em/plsa_m_step.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;参考文献：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.cs.bham.ac.uk/~pxt/IDA/plsa.pdf&quot;&gt;Unsupervised Learning by Probabilistic Latent Semantic Analysis&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://blog.csdn.net/yangliuy/article/details/8330640&quot;&gt;概率语言模型及其变形系列(1)-PLSA及EM算法&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;emgmm&quot;&gt;EM于GMM&lt;/h2&gt;

&lt;p&gt;PRML第9章。&lt;/p&gt;

&lt;h2 id=&quot;emhmm&quot;&gt;EM于HMM&lt;/h2&gt;

&lt;p&gt;统计机器学习-HMM那一章节&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;更多参考文献&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/zzbased/zzbased.github.com/blob/master/_posts/doc/我所理解的EM算法.docx&quot;&gt;你所不知道的EM - by erikhu&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/lansatiankongxxc/article/details/45646677&quot;&gt;EM算法原理详解与高斯混合模型&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/zouxy09/article/details/8537620&quot;&gt;从最大似然到EM算法浅解&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Fri, 27 Mar 2015 00:00:00 +0800</pubDate>
        <link>http://yourdomain.com/2015/03/27/EM%E7%AE%97%E6%B3%95%E9%9A%8F%E7%AC%94.html</link>
        <guid isPermaLink="true">http://yourdomain.com/2015/03/27/EM%E7%AE%97%E6%B3%95%E9%9A%8F%E7%AC%94.html</guid>
        
        
      </item>
    
      <item>
        <title>机器学习技法学习笔记</title>
        <description>&lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;h1 id=&quot;section&quot;&gt;机器学习技巧 学习笔记&lt;/h1&gt;

&lt;p&gt;有用链接：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.coursera.org/course/ntumlone&quot;&gt;机器学习基石&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://class.coursera.org/ntumltwo-001/lecture&quot;&gt;机器学习技法&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://beader.me/mlnotebook/&quot;&gt;beader.me笔记&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.douban.com/doulist/3440234/&quot;&gt;听课笔记douban&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://mooc.guokr.com/course/610/機器學習基石--Machine-Learning-Foundations-/&quot;&gt;mooc学院&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;linear-support-vector-machines&quot;&gt;第1讲 Linear Support Vector Machines&lt;/h2&gt;

&lt;p&gt;我们的目标是：最大间隔&lt;/p&gt;

&lt;p&gt;求一个点x距离一个平面的距离：&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;点x到平面上的点x’的向量 x-x’，在平面的法向量上的投影：w*(x-x’)/&lt;/td&gt;
      &lt;td&gt;w&lt;/td&gt;
      &lt;td&gt;，即&lt;/td&gt;
      &lt;td&gt;w^T*x+b&lt;/td&gt;
      &lt;td&gt;/&lt;/td&gt;
      &lt;td&gt;w&lt;/td&gt;
      &lt;td&gt;。&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;最大化这个距离，可以假设 min{y*(wx+b)}=1。那么目标变为：&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;max 1/&lt;/td&gt;
      &lt;td&gt;w&lt;/td&gt;
      &lt;td&gt;条件是： min{y*(wx+b)}=1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;进一步推导，得到最终优化的目标：&lt;/p&gt;

&lt;p&gt;min 1/2 w*w^T  subject to y(wx+b)&amp;gt;=1&lt;/p&gt;

&lt;p&gt;这就是支持向量机的优化目标，它的损失函数，等同于： max{0, 1-ywx}&lt;/p&gt;

&lt;p&gt;注意：函数间隔与几何间隔。&lt;/p&gt;

&lt;p&gt;可以将这个优化目标转化到 &lt;a href=&quot;http://cn.mathworks.com/discovery/quadratic-programming.html&quot;&gt;二次规划 quadratic programming&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/quadratic_programming.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;large-margin algorithm的VC维分析。因为large margin的限制，相比于较于PLA，svm的dichotomies会更少。所以从VC维看，相比于PLA，其泛化能力更强。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/vc_dimension_of_large_margin_algorithm.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;large-margin hyperplanes：参数最少，所以boundary最简单。
一般的hyperplanes：参数适中，边界简单。
一般的hyperplanes+feature转换(非线性的)：参数较多，边界复杂。
large-margin hyperplanes+feature transform：则可以得到适中的参数个数，复杂的边界。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Benefits-of-Large-Margin-Hyperplanes.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;扩展阅读&lt;/strong&gt;
&lt;a href=&quot;http://blog.csdn.net/v_july_v/article/details/7624837&quot;&gt;支持向量机通俗导论（理解SVM的三层境界）&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;习题&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter1_question1.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter1_question2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;dual-support-vector-machine&quot;&gt;第2讲 Dual support vector machine&lt;/h2&gt;

&lt;p&gt;讨论： Support Vector Classification，Logistic Regression，Support Vector Regression的区别：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/L2-regularized-L1-and-L2-loss-Support-Vector-Classification.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/L2-regularized-Logistic-Regression.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/L1-regularized-L2-loss-Support-Vector-Classification.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/L1-regularized-Logistic-Regression.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/L2-regularized-L1-and-L2-loss-Support-Vector-Regression.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;复习一下第1讲，直接求解SVM的original问题，利用QP方法，需要求解 d+1个变量(d指代feature转换后的维度)，N个约束条件。如果我们采用一个非线性变换，维度特别高，就不太可解了，所以我们想SVM without d。所以有 ‘Equivalent’ SVM: based on some dual problem of Original SVM。&lt;/p&gt;

&lt;p&gt;这时就要用到lagrange multipliers。这里看下正则化，为什么正则化的表达式是这样的，这是通过lagrange multipliers。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Lagrange-Multipliers-regularization.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;下面是SVM的对偶问题推导过程：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Lagrange-Function1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/lagrange-dual-problem2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这里要提一下KKT条件：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/kkt_11.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/kkt_12.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;经过一通推导，我们得到了svm的对偶问题：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Dual-Formulation-of-svm.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这个对偶问题，就可以用QP来求解了。&lt;/p&gt;

&lt;p&gt;求得a后，primal问题的w和b，可以通过下面式子求得：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/w_b_optim.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;最后说一个解释：当a_n大于0时，此时该点正好处于边界上，这也就是所谓的支撑向量。&lt;/p&gt;

&lt;p&gt;有趣之处在于，对于新点x的预测，只需要计算它与训练数据点的内积即可（表示向量内积），这一点至关重要，是之后使用 Kernel 进行非线性推广的基本前提。此外，所谓 Supporting Vector 也在这里显示出来——事实上，所有非Supporting Vector 所对应的系数都是等于零的，因此对于新点的内积计算实际上只要针对少量的“支持向量”而不是所有的训练数据即可。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;习题&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter2_question1.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter2_question2.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter2_question3.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter2_question4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;第3讲&lt;/h2&gt;
&lt;p&gt;为什么要把SVM转换到对偶问题，原因有这样几个：1.对偶问题的变量为N个，有时候N远远小于d。2.解释了support vector。 3.比较直观的引入了核函数。&lt;/p&gt;

&lt;p&gt;在线性不可分的情况下，支持向量机首先在低维空间中完成计算，然后通过核函数将输入空间映射到高维特征空间，最终在高维特征空间中构造出最优分离超平面，从而把平面上本身不好分的非线性数据分开。&lt;/p&gt;

&lt;p&gt;建立非线性学习器分为两步：
首先使用一个非线性映射将数据变换到一个特征空间F，
然后在特征空间使用线性学习器分类。&lt;/p&gt;

&lt;p&gt;核函数的优势在于：
一个是映射到高维空间中，然后再根据内积的公式进行计算；
而另一个则直接在原来的低维空间中进行计算，而不需要显式地写出映射后的结果。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Kernel-SVM-with-QP.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;多项式核：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Poly-2-Kernel.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;SVM + Polynomial Kernel: Polynomial SVM&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Poly-Kernel.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;高斯核：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/gaussian_kernel.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;看一下高斯核参数改变带来的变化：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/gaussian_kernel2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;下面对比一下常用的几种核函数：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/compare_linear_kernel.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/compare_poly_kernel.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/compare_gaussian_kernel.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;当然，除了上面三种常用的核函数外，还可以自己构造一些核，只需要这些核满足mercer’s condition。不过需要说明的，很难。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/valid_kernel_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;习题&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter3_question1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter3_question2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;第4讲&lt;/h2&gt;

&lt;p&gt;使用松弛变量处理 outliers 方法，本讲的内容。&lt;/p&gt;

&lt;h2 id=&quot;blending-and-bagging&quot;&gt;第6讲  Blending and Bagging&lt;/h2&gt;

&lt;p&gt;Aggregation的方法包括：select, mix uniformly, mix non-uniformly, combine;
&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/aggregation_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;为什么Aggregation方法是有效的？可以从两方面来看，其一通过Aggregation可以生成复杂的hypotheses，相当于做了feature transform；其二，生成的G(x)更加moderate，例如下图中PLA的uniform mix就是large-margin，相当于做了regularization。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/aggregation_works.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;uniform blending&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;如果是classification，则有：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/uniform_blending_for_classification.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;如果是regression，则有：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/uniform_blending_for_regression.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;从上图还可以看出：任意g的Eout平均大于等于G的Eout。&lt;/p&gt;

&lt;p&gt;从上图的公式还可以得出，expected performance of A = expected deviation to consensus +performance of consensus。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/uniform_blending_reduce_variance.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Linear Blending&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;linear blending就像two-level learning。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/linear_blending.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;like selection, blending practically done with (Eval instead of Ein) + (gt− from minimum Etrain)&lt;/p&gt;

&lt;p&gt;Any blending也叫Stacking。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/any_blending.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;bagging&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;aggregation里最重要的一个点就是：diversity。diversity的方法有很多种。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/diversity_important.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;下面介绍一种通过data randomness的方法，也叫bootstrapping，即bagging。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/bootstarpping1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;习题&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter7_question1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;adaptive-boosting&quot;&gt;第8讲 Adaptive Boosting&lt;/h2&gt;

&lt;p&gt;课程的最开始有一个分辨苹果的例子。以后AdaBoost的时候可以借鉴那个例子。其基本思路是：给予上次分错的样本更高的权重。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/re_weighting_bootstrapping.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;给每个example不同的weight，类似于给予不同的class的样本不同的weight。回忆一下，有时候我们false reject尽可能低，那对于这一类，我们在error measure给予更高的权重。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/re_weighting_bootstrapping2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/false-accept-and-false-reject.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;具体怎么更新下一次训练的样本权重呢，参考下面的图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/scaling_factor.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;有了样本权重更新公式后，则有一个Preliminary算法：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/adaboost_preliminary.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;得到这么多的g后，怎么得到G，也就是aggregation的方法，我们希望在计算g的时候把aggregation的权重也得到。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/adaboost1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;那么完整算法为：
&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/adaboost2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;下面是一些理论：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Theoretical-Guarantee-of-AdaBoost.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Decision Stump&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Decision-Stump.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;AdaBoost与Decision Stump的结合 – &amp;gt; AdaBoost-Stump:
efficient feature selection and aggregation&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;习题&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter8_question1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;decision-tree&quot;&gt;第9讲 Decision Tree&lt;/h2&gt;

&lt;p&gt;decision tree的位置，模仿人脑决策过程。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/decision-tree1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;decision tree缺点：(1)启发式的规则(前人的巧思)，缺乏理论基础；(2)启发式规则很多，需要selection；(3)没有代表性的算法。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Disclaimers-about-Decision-Tree.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;一个基本的decision tree算法：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/basic-decision-tree.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;CART: classification and regression tree。
有两个简单的选择：binary tree；叶子节点是常数。&lt;/p&gt;

&lt;p&gt;怎么选择branching，切完后两个子树的纯度最高。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Purifying-in-CART.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;怎么考量”不纯度”&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Impurity-Functions.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;最终CART算法如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/cart_algorithm.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;关于CART算法的演算过程，具体请参考 &lt;a href=&quot;http://mydisk.com/yzlv/webpage/datamining/xiti.html&quot;&gt;决策树算法的计算过程演示&lt;/a&gt;，&lt;a href=&quot;http://en.wikipedia.org/wiki/Decision_tree_learning&quot;&gt;Decision tree learning&lt;/a&gt;，&lt;a href=&quot;http://www.academia.edu/7032069/An_example_of_calculating_gini_gain_in_CART&quot;&gt;An example of calculating gini gain in CART&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;几种决策树算法的区别：&lt;/p&gt;

&lt;p&gt;C4.5算法是在ID3算法的基础上采用信息增益率的方法选择测试属性。 ID3算法和C4.5算法虽然在对训练样本集的学习中可以尽可能多地挖掘信息，但其生成的决策树分支较大，规模较大。为了简化决策树的规模，提高生成决策树的效率，又出现了根据GINI系数来选择测试属性的决策树算法CART。
CART算法采用一种二分递归分割的技术，与基于信息熵的算法不同，CART算法对每次样本集的划分计算GINI系数，GINI系数，GINI系数越小则划分越合理。CART算法总是将当前样本集分割为两个子样本集，使得生成的决策树的每个非叶结点都只有两个分枝。因此CART算法生成的决策树是结构简洁的二叉树。&lt;/p&gt;

&lt;p&gt;Regularization&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Regularization-by-Pruning.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;当有categorical features时，CART也可以灵活处理。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Branching-on-Categorical-Features.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;如果有缺失特征的话，怎么办？可以利用surrogate feature。&lt;/p&gt;

&lt;p&gt;看一个CART的例子：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/cart_example.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;习题&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter9_question1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;random-forest&quot;&gt;第10讲 random forest&lt;/h2&gt;
&lt;p&gt;Bagging and Decision Tree，将这两者合在一起，就是Random forest。&lt;/p&gt;

&lt;p&gt;random forest (RF) = bagging + fully-grown C&amp;amp;RT decision tree&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Bagging-and-Decision-Tree.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;三个优点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;highly parallel/efficient to learn&lt;/li&gt;
  &lt;li&gt;inherit pros of C&amp;amp;RT&lt;/li&gt;
  &lt;li&gt;eliminate cons of fully-grown tree&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;因为是random forest，除了在bootstrapping时利用data randomness，还可以randomly sample d’ feature from x。即original RF re-sample new subspace for each b(x) in C&amp;amp;RT。&lt;/p&gt;

&lt;p&gt;那么更进一步了，RF = bagging + random-subspace C&amp;amp;RT&lt;/p&gt;

&lt;p&gt;random-combination的意思是：随机抽样一些features后，line combination，作为一个新的feature切分点。那么original RF consider d′ random low-dimensional projections for each b(x) in C&amp;amp;RT。&lt;/p&gt;

&lt;p&gt;所以，再进一步：RF = bagging + random-combination C&amp;amp;RT&lt;/p&gt;

&lt;p&gt;从上面可以看出，randomness是随处不在的。&lt;/p&gt;

&lt;p&gt;回顾一下bagging的过程，每次随机抽样一些数据，这样下去，总会有一些样本是一直未被抽中的。未被抽中的概率计算为：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/numbers_of_oob.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;有了这些out-of-bag (OOB) examples后，可以将其作为validation set来使用。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/oob_vs_validation.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;那么，相比于原来的validation过程，RF可以做self-validation，也就是在训练的过程中，把model选择顺便也做了。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/model_selection_by_oob.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;接着看下Feature selection，decision tree正好是一个内建的feature selection过程。&lt;/p&gt;

&lt;p&gt;先看下利用linear model做feature importance判别，训练完的模型，weight越大，表示feature越重要。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Feature_Selection_by_Importance.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;而RF可以采用permutation test来做特征选择。所谓permutation test，也就是对某一个特征，对所有样本上该维度的特征值做随机排列，然后在这个样本集上计算RF performance。用原来的performance减去这个新的performance后，就得到该特征的重要性。如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/feature_selection_by_permutation_test.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;但是在RF上，因为OOB的存在，可以利用Eoob(G)-Eoob^p(G)。Eoob^p(G)是通过在OOB上permute某一维特征值。&lt;strong&gt;这里后续可以再深挖&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/feature_importance_by_rf.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;习题&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter10_question1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter10_question2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;gradient-boosted-decision-treegbdt&quot;&gt;第11讲 Gradient Boosted Decision Tree(GBDT)&lt;/h2&gt;

&lt;p&gt;random forest用一句话来总结，则是：bagging of randomized C&amp;amp;RT trees with automatic validation and feature selection。&lt;/p&gt;

&lt;p&gt;比较一下Random forest和AdaBoost Tree。
&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/compare_rd_adaboost-tree.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;但是要做AdaBoost Tree的话，首先需要weighted DTree。这个在LR,SVM等模型上容易做到，但是在DT上很难。所以我们换个思路，如果我们想给某个样本加个weight，可以在sample样本的时候，增大或者减小它的概率即可。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Randomized-Base-Algorithm.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;所以AdaBoost-DTree的组成由下图所示：
&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/AdaBoost-DTree1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在adaboost算法中，如果一个g的错误率为0的话，那么这个g的权重将是无限大的。而在决策树的世界里，如果是full-grown的话，在训练数据上，错误率为0是很容易办到的。&lt;/p&gt;

&lt;p&gt;那么为了避免这种过拟合的情况存在，我们需要对DT做剪枝。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/AdaBoost-DTree2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;当我们extremely剪枝时，譬如限制树的高度小于等于1，那此时DT就变成了decision stump。所以有了adaboost-stump算法，它是AdaBoost-DTree的一种特例。&lt;/p&gt;

&lt;p&gt;2，3，4节 未完待续。&lt;/p&gt;

&lt;p&gt;更多具体的内容，请参考单独的文章： &lt;a href=&quot;http://zzbased.github.io/2015/04/03/Aggregation模型.html&quot;&gt;Aggregation模型&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;习题&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter11_question1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;第12讲 神经网络&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Motivation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;通过”Linear Aggregation of Perceptrons”，可以完成AND，OR，NOT等操作，可以完成 convex set等操作，但是不能完成XOR操作。怎么办？只能multi-layer perceptron。&lt;/p&gt;

&lt;p&gt;XOR(g1, g2) = OR(AND(−g1, g2), AND(g1, −g2))&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/perceptron_powerful_limitation.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;perceptron (simple)
=⇒ aggregation of perceptrons (powerful)
=⇒ multi-layer perceptrons (more powerful)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter12_question1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Neural Network Hypothesis&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;output：any linear model can be used；
transformation function of score (signal) s：不用linear，因为多层线性=&amp;gt;whole network linear。也不用阶梯函数(0-1)，因为它不可微。通常的选择有tanh(x)，sigmoid(s)。&lt;/p&gt;

&lt;p&gt;tanh(x) = [exp(s)-exp(-s)] / [exp(s)+exp(-s)] = 2sigmoid(2x)-1&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter12_question2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Backpropagation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Backpropagation_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Backpropagation_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter12_question3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Optimization&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;当multiple hidden layers，一般都是non-convex。对于最优化来说，不容易求得全局最优解。GD/SGD可能只能求出局部最优解。&lt;/p&gt;

&lt;p&gt;对Wij做不同的初始化，可能有不同的局部最优解。所以对初始化值比较敏感。&lt;/p&gt;

&lt;p&gt;有效的建议是：不要初始化太大的weights，因为large weight，加上tanh后，将saturate。如果做梯度下降的话，那段区域里有small gradient。所以建议要try some random&amp;amp;small ones。&lt;/p&gt;

&lt;p&gt;神经网络的dVC=O(VD)，V表示神经元的个数，D表示weight的个数，也就是edge的数目。&lt;/p&gt;

&lt;p&gt;VC维太大，容易overfit。可以加一个L2 regularizer。但是加L2后，带来的只是shrink weights。我们希望可以得到sparse解，那么就可以用L1 regularizer，但L1不可微分。
所以另外一个选择是：weight-elimination（scaled L2），即large weight → median shrink; small weight → median shrink&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/weight-elimination-regularizer.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Early Stopping，随着t 增长，VC维越大。所以合适的t 就够了。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/BP-Early-Stopping.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter12_question4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;deep-learning&quot;&gt;第13讲 Deep Learning&lt;/h2&gt;

&lt;p&gt;structural decisions: key issue for applying NNet。模型结构很关键。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Challenges-and-Key-Techniques-for-Deep-Learning.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;hinton 2006提出的：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/A-Two-Step-Deep-Learning-Framework.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Information-Preserving-Neural-Network.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Auto-encoder的作用：监督学习的话，给予做特征；无监督学习的话，用来做密度预测，也可以用来做异常点检测。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Deep-Learning-with-Autoencoders.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Regularization in Deep Learning的方法：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;structural decisions/constraints，譬如卷积神经网络，循环神经网络&lt;/li&gt;
  &lt;li&gt;weight decay or weight elimination regularizers&lt;/li&gt;
  &lt;li&gt;Early stopping&lt;/li&gt;
  &lt;li&gt;dropout，dropconnect等&lt;/li&gt;
  &lt;li&gt;denosing&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/denosing_auto-encoder.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Linear Autoencoder Hypothesis
&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/linear-autoencoder-hypothesis.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;简单点看就是：h(x) = WW^T x&lt;/p&gt;

&lt;p&gt;复习一下特征值和特征向量。&lt;a href=&quot;http://zh.wikipedia.org/wiki/特征向量&quot;&gt;特征向量wiki&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/optimal_v_linear_autoencoder.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/pca-for-autoencoder.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;习题&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter13_question1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;radial-basis-function-network&quot;&gt;第14讲 Radial Basis Function Network&lt;/h2&gt;

&lt;p&gt;以前在讲SVM时，有提到RBF kernel(gaussian kernel)，这里回顾一下。高斯核是将x空间变换到z空间的无限维。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/gaussian_svm.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;基于高斯核的SVM如下所示，相当于是support vector上的radial hypotheses的线性组合。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/gaussian_svm2.png&quot; alt=&quot;&quot; /&gt;
So，Radial Basis Function (RBF) Network: linear aggregation of radial hypotheses。&lt;/p&gt;

&lt;p&gt;将RBF network类比于neural network，output layer是一样的，都是线性组合，不一样是隐藏层(在RBF network里，是distance + gaussian)。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/rbf_network.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;基于RBF network来解释SVM：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/RBF-Network-Hypothesis.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;此时，需要学习的参数是 u_m(是centers)，b_m(是不同rbf线性组合的系数)。&lt;/p&gt;

&lt;p&gt;kernel是Z空间的相关性度量，而RBF是X空间的相关性度量。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/rbf_vs_kernel.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;所以RBF network： distance similairty-to-centers as feature transform。&lt;/p&gt;

&lt;p&gt;Full RBF Network：是说将所有样本点都参与到运算里(M=N)。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/full_rbf_network.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;full rbf network是一个lazy way to decide u_m。&lt;/p&gt;

&lt;p&gt;Nearest-Neighbor：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Nearest-Neighbor.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;如果是利用RBF network做regression呢？如下所示。但是这样做了后，Ein(g)=0，这样势必会overfit。
所以需要做正则化。正则化的思路有：(1)类似于kernel ridge regression，加正则项。(2)fewer centers，譬如support vector。constraining number of centers and voting weights。&lt;/p&gt;

&lt;p&gt;那怎样才能做到fewer centers呢？通常的方法就是：寻找prototypes。那how to extract prototypes?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Regularized-Full-RBF-Network.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这里提到一种算法：k-means cluster。k-means的优化思路为：alternating minimization。说到这，EM也属于alternating minimization。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/k-Means-Algorithm.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;OK，现在prototypes提取到了，接下来把基于k-means的rbf network写出来。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/RBF-Network-Using-k-Means.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;下面是实战，先看一个k-means的例子：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/k-means-examples.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;可以看到，k和初始化，在k-means算法里非常关键。&lt;/p&gt;

&lt;p&gt;通常随机地从样本中挑k个出来作为k个初始的聚类中心。但这不是个明智的选择。它有可能会导致图像趋于稠密聚集某些区域，因为如果训练样本本身就在某个区域分布非常密，那么我们随机去选择聚类中心的时候，就会出现就在这个数据分布密集的地方被选出了很多的聚类中心。&lt;/p&gt;

&lt;p&gt;那么怎么做k-means的初始化呢？&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;多次运行数据集合，选择最小的SSE的分簇结果作为最终结果。该方法依赖于数据集合和簇数量，对分簇结果有比较大影响，所以在某些场景下效果也不是很好。&lt;/li&gt;
  &lt;li&gt;抽取数据集合样本，对样本进行Hierarchical Clustering技术，从中抽取K个Clustering作为初始中心点。该方法工作良好，知识有两点限制条件：抽样数据不能太大，因为Hierarchical Clustering比较耗时间；K值相对于抽样数据比较小才行。&lt;/li&gt;
  &lt;li&gt;kmeans++算法。&lt;a href=&quot;http://en.wikipedia.org/wiki/K-means%2B%2B&quot;&gt;kmeans++ wiki&lt;/a&gt;，&lt;a href=&quot;http://www.cnblogs.com/shelocks/archive/2012/12/20/2826787.html&quot;&gt;kmenas++中文&lt;/a&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;K-means++的步骤为：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;从输入的数据点集合中随机选择一个点作为第一个聚类中心&lt;/li&gt;
  &lt;li&gt;对于数据集中的每一个点x，计算它与最近聚类中心(指已选择的聚类中心)的距离D(x)&lt;/li&gt;
  &lt;li&gt;选择一个新的数据点作为新的聚类中心，选择的原则是：D(x)较大的点，被选取作为聚类中心的概率较大&lt;/li&gt;
  &lt;li&gt;重复2和3直到k个聚类中心被选出来&lt;/li&gt;
  &lt;li&gt;利用这k个初始的聚类中心来运行标准的k-means算法&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;更多关于k-means初始化的方法，请参考 &lt;a href=&quot;http://www.mecs-press.org/ijisa/ijisa-v4-n1/IJISA-V4-N1-3.pdf&quot;&gt;Efficient and Fast Initialization Algorithm for K- means Clustering&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;扩展阅读&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;发表在Science的论文：基于密度的快速无监督聚类方法 &lt;a href=&quot;http://t.cn/RAASZ4q&quot;&gt;Clustering by fast search and find of density peaks. A Rodriguez, A Laio (2014) &lt;/a&gt; 很棒，推荐给没看过的朋友，另有相关中文两篇：http://t.cn/RPoKmOi http://t.cn/RPOs6uK 供参考理解 云:http://t.cn/RAACowz&lt;/p&gt;

    &lt;p&gt;对所有坐标点，基于相互距离，提出了两个新的属性，一是局部密度rho，即与该点距离在一定范围内的点的总数，二是到更高密度点的最短距离delta。作者提出，类簇的中心是这样的一类点：它们被很多点围绕（导致局部密度大），且与局部密度比自己大的点之间的距离也很远。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Canopy 聚类算法的基本原则是：首先应用成本低的近似的距离计算方法高效的将数据分为多个组，这里称为一个Canopy。Canopy 之间可以有重叠的部分。然后采用严格的距离计算方式准确的计算在同一 Canopy 中的点，将他们分配与最合适的簇。Canopy 聚类算法经常用于 K 均值聚类算法的预处理，用来找合适的 k 值和簇中心。&lt;a href=&quot;http://blog.pureisle.net/archives/2045.html&quot;&gt;Clustering Algorithm/聚类算法&lt;/a&gt;，&lt;a href=&quot;http://en.wikipedia.org/wiki/Canopy_clustering_algorithm&quot;&gt;Canopy clustering algorithm&lt;/a&gt;。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://t.cn/RAwxOJx&quot;&gt;文章 K-means Clustering with scikit-learn&lt;/a&gt; PyData SV 2014上Sarah Guido的报告，Python下用Scikit-Learn做K-means聚类分析的深入介绍，涉及k值选取、参数调优等问题，很实用 GitHub:http://t.cn/RAwxsFS 云(视频+讲义):http://t.cn/RAwJJkG&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://t.cn/RwlDlgq&quot;&gt;文章 Divining the ‘K’ in K-means Clustering&lt;/a&gt; 用G-means算法确定K-means聚类最佳K值，G-means能很好地处理stretched out clusters(非球面伸展型类簇)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.cc.gatech.edu/~isbell/tutorials/rbf-intro.pdf&quot;&gt;RBF的核心论文 Introduction to Radial Basis Function Networks&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;习题&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2_chapter14_question1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2_chapter14_question2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;matrix-factorization&quot;&gt;第15讲 Matrix Factorization&lt;/h2&gt;

&lt;p&gt;从”Linear Network” Hypothesis说起，用来做推荐，也就是根据feature x，预测得分y。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Linear-Network-Hypothesis.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;求解上面的linear network，采用squared error。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/linear_model_for_recommendation.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;从上面的求解过程，得到Matrix factorization：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Matrix-Factorization1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;具体下来，应该怎么求解呢？考虑到这里面有两个变量W和V，这时可以采用alternating minimization。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Matrix-Factorization-Learning.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;所以，得到Alternating Least Squares方法。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Alternating-Least-Squares.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;比较一下 linear autoencoder 和 matrix factorization。linear autoencoder
≡ special matrix factorization of complete X&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Linear-Autoencoder-versus-Matrix-Factorization.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;上面讲述了 alternating解法，matrix factorization还可以利用Stochastic gradient descent求解。SGD：most popular large-scale matrix factorization algorithm，比alternating速度更快。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/SGD-for-Matrix-Factorization.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;举一个SGD的例子。在KDDCup 2011 Track1中，因为该推荐任务与时间系列有关，所以在优化时，没有用stochastic GD算法，而是采用了time-deterministic GD算法，也就是最近的样本最后参与计算，这样可以保证最近的样本拟合得更好。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/KDDCup-2011-Track1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;extraction-models&quot;&gt;Extraction Models总结&lt;/h2&gt;

&lt;p&gt;将特征转换纳入到我们的学习过程。&lt;/p&gt;

&lt;p&gt;Extraction Models： neural network，RBF network，Matrix Factorization。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Map-of-Extraction-Models.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Extraction Techniques：function gradient descnet，SGD。&lt;/p&gt;

&lt;p&gt;无监督学习用于预训练，例如autoencoder，k-means clustering。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Map-of-Extraction-Techniques.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;regularization：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Pros-and-Cons-of-Extraction-Models.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;finale-&quot;&gt;第16讲 Finale 大总结&lt;/h2&gt;

&lt;p&gt;Exploiting Numerous Features via Kernel：Polynomial Kernel，Gaussian Kernel等。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Exploiting-Numerous-Features-via-Kernel.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Exploiting Predictive Features via Aggregation：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Exploiting-Predictive-Features-via-Aggregation.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Exploiting Hidden Features via Extraction：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Exploiting-Hidden-Features-via-Extraction.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Exploiting Low-Dim. Features via Compression：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Exploiting-Low-Dim.Features-via-Compression.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;习题&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter16_question1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 26 Mar 2015 00:00:00 +0800</pubDate>
        <link>http://yourdomain.com/2015/03/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.html</link>
        <guid isPermaLink="true">http://yourdomain.com/2015/03/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.html</guid>
        
        
      </item>
    
  </channel>
</rss>
