<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>机器学习技法学习笔记</title>
  <meta name="description" content="">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://yourdomain.com/2015/03/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.html">
  <link rel="alternate" type="application/rss+xml" title="100的技术博客" href="http://yourdomain.com/feed.xml" />
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">100的技术博客</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
        
          
          <a class="page-link" href="/about/">About</a>
          
        
          
        
          
        
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">机器学习技法学习笔记</h1>
    <p class="post-meta">Mar 26, 2015</p>
  </header>

  <article class="post-content">
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<h1 id="section">机器学习技巧 学习笔记</h1>

<p>有用链接：</p>

<ul>
  <li><a href="https://www.coursera.org/course/ntumlone">机器学习基石</a></li>
  <li><a href="https://class.coursera.org/ntumltwo-001/lecture">机器学习技法</a></li>
  <li><a href="http://beader.me/mlnotebook/">beader.me笔记</a></li>
  <li><a href="http://www.douban.com/doulist/3440234/">听课笔记douban</a></li>
  <li><a href="http://mooc.guokr.com/course/610/機器學習基石--Machine-Learning-Foundations-/">mooc学院</a></li>
</ul>

<h2 id="linear-support-vector-machines">第1讲 Linear Support Vector Machines</h2>

<p>我们的目标是：最大间隔</p>

<p>求一个点x距离一个平面的距离：</p>

<table>
  <tbody>
    <tr>
      <td>点x到平面上的点x’的向量 x-x’，在平面的法向量上的投影：w*(x-x’)/</td>
      <td>w</td>
      <td>，即</td>
      <td>w^T*x+b</td>
      <td>/</td>
      <td>w</td>
      <td>。</td>
    </tr>
  </tbody>
</table>

<p>最大化这个距离，可以假设 min{y*(wx+b)}=1。那么目标变为：</p>

<table>
  <tbody>
    <tr>
      <td>max 1/</td>
      <td>w</td>
      <td>条件是： min{y*(wx+b)}=1</td>
    </tr>
  </tbody>
</table>

<p>进一步推导，得到最终优化的目标：</p>

<p>min 1/2 w*w^T  subject to y(wx+b)&gt;=1</p>

<p>这就是支持向量机的优化目标，它的损失函数，等同于： max{0, 1-ywx}</p>

<p>注意：函数间隔与几何间隔。</p>

<p>可以将这个优化目标转化到 <a href="http://cn.mathworks.com/discovery/quadratic-programming.html">二次规划 quadratic programming</a></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/quadratic_programming.png" alt="" /></p>

<p>large-margin algorithm的VC维分析。因为large margin的限制，相比于较于PLA，svm的dichotomies会更少。所以从VC维看，相比于PLA，其泛化能力更强。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/vc_dimension_of_large_margin_algorithm.png" alt="" /></p>

<p>large-margin hyperplanes：参数最少，所以boundary最简单。
一般的hyperplanes：参数适中，边界简单。
一般的hyperplanes+feature转换(非线性的)：参数较多，边界复杂。
large-margin hyperplanes+feature transform：则可以得到适中的参数个数，复杂的边界。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Benefits-of-Large-Margin-Hyperplanes.png" alt="" /></p>

<p><strong>扩展阅读</strong>
<a href="http://blog.csdn.net/v_july_v/article/details/7624837">支持向量机通俗导论（理解SVM的三层境界）</a></p>

<p><strong>习题</strong></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter1_question1.png" alt="" />
<img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter1_question2.png" alt="" /></p>

<h2 id="dual-support-vector-machine">第2讲 Dual support vector machine</h2>

<p>讨论： Support Vector Classification，Logistic Regression，Support Vector Regression的区别：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/L2-regularized-L1-and-L2-loss-Support-Vector-Classification.png" alt="" /></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/L2-regularized-Logistic-Regression.png" alt="" /></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/L1-regularized-L2-loss-Support-Vector-Classification.png" alt="" /></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/L1-regularized-Logistic-Regression.png" alt="" /></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/L2-regularized-L1-and-L2-loss-Support-Vector-Regression.png" alt="" /></p>

<p>复习一下第1讲，直接求解SVM的original问题，利用QP方法，需要求解 d+1个变量(d指代feature转换后的维度)，N个约束条件。如果我们采用一个非线性变换，维度特别高，就不太可解了，所以我们想SVM without d。所以有 ‘Equivalent’ SVM: based on some dual problem of Original SVM。</p>

<p>这时就要用到lagrange multipliers。这里看下正则化，为什么正则化的表达式是这样的，这是通过lagrange multipliers。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Lagrange-Multipliers-regularization.png" alt="" /></p>

<p>下面是SVM的对偶问题推导过程：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Lagrange-Function1.png" alt="" /></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/lagrange-dual-problem2.png" alt="" /></p>

<p>这里要提一下KKT条件：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/kkt_11.png" alt="" /></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/kkt_12.png" alt="" /></p>

<p>经过一通推导，我们得到了svm的对偶问题：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Dual-Formulation-of-svm.png" alt="" /></p>

<p>这个对偶问题，就可以用QP来求解了。</p>

<p>求得a后，primal问题的w和b，可以通过下面式子求得：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/w_b_optim.png" alt="" /></p>

<p>最后说一个解释：当a_n大于0时，此时该点正好处于边界上，这也就是所谓的支撑向量。</p>

<p>有趣之处在于，对于新点x的预测，只需要计算它与训练数据点的内积即可（表示向量内积），这一点至关重要，是之后使用 Kernel 进行非线性推广的基本前提。此外，所谓 Supporting Vector 也在这里显示出来——事实上，所有非Supporting Vector 所对应的系数都是等于零的，因此对于新点的内积计算实际上只要针对少量的“支持向量”而不是所有的训练数据即可。</p>

<p><strong>习题</strong></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter2_question1.png" alt="" />
<img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter2_question2.png" alt="" />
<img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter2_question3.png" alt="" />
<img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter2_question4.png" alt="" /></p>

<h2 id="section-1">第3讲</h2>
<p>为什么要把SVM转换到对偶问题，原因有这样几个：1.对偶问题的变量为N个，有时候N远远小于d。2.解释了support vector。 3.比较直观的引入了核函数。</p>

<p>在线性不可分的情况下，支持向量机首先在低维空间中完成计算，然后通过核函数将输入空间映射到高维特征空间，最终在高维特征空间中构造出最优分离超平面，从而把平面上本身不好分的非线性数据分开。</p>

<p>建立非线性学习器分为两步：
首先使用一个非线性映射将数据变换到一个特征空间F，
然后在特征空间使用线性学习器分类。</p>

<p>核函数的优势在于：
一个是映射到高维空间中，然后再根据内积的公式进行计算；
而另一个则直接在原来的低维空间中进行计算，而不需要显式地写出映射后的结果。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Kernel-SVM-with-QP.png" alt="" /></p>

<p>多项式核：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Poly-2-Kernel.png" alt="" /></p>

<p>SVM + Polynomial Kernel: Polynomial SVM</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Poly-Kernel.png" alt="" /></p>

<p>高斯核：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/gaussian_kernel.png" alt="" /></p>

<p>看一下高斯核参数改变带来的变化：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/gaussian_kernel2.png" alt="" /></p>

<p>下面对比一下常用的几种核函数：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/compare_linear_kernel.png" alt="" />
<img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/compare_poly_kernel.png" alt="" />
<img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/compare_gaussian_kernel.png" alt="" /></p>

<p>当然，除了上面三种常用的核函数外，还可以自己构造一些核，只需要这些核满足mercer’s condition。不过需要说明的，很难。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/valid_kernel_2.png" alt="" /></p>

<p><strong>习题</strong></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter3_question1.png" alt="" /></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter3_question2.png" alt="" /></p>

<h2 id="section-2">第4讲</h2>

<p>使用松弛变量处理 outliers 方法，本讲的内容。</p>

<h2 id="blending-and-bagging">第6讲  Blending and Bagging</h2>

<p>Aggregation的方法包括：select, mix uniformly, mix non-uniformly, combine;
<img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/aggregation_1.png" alt="" /></p>

<p>为什么Aggregation方法是有效的？可以从两方面来看，其一通过Aggregation可以生成复杂的hypotheses，相当于做了feature transform；其二，生成的G(x)更加moderate，例如下图中PLA的uniform mix就是large-margin，相当于做了regularization。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/aggregation_works.png" alt="" /></p>

<p><strong>uniform blending</strong></p>

<p>如果是classification，则有：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/uniform_blending_for_classification.png" alt="" /></p>

<p>如果是regression，则有：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/uniform_blending_for_regression.png" alt="" /></p>

<p>从上图还可以看出：任意g的Eout平均大于等于G的Eout。</p>

<p>从上图的公式还可以得出，expected performance of A = expected deviation to consensus +performance of consensus。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/uniform_blending_reduce_variance.png" alt="" /></p>

<p><strong>Linear Blending</strong></p>

<p>linear blending就像two-level learning。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/linear_blending.png" alt="" /></p>

<p>like selection, blending practically done with (Eval instead of Ein) + (gt− from minimum Etrain)</p>

<p>Any blending也叫Stacking。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/any_blending.png" alt="" /></p>

<p><strong>bagging</strong></p>

<p>aggregation里最重要的一个点就是：diversity。diversity的方法有很多种。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/diversity_important.png" alt="" /></p>

<p>下面介绍一种通过data randomness的方法，也叫bootstrapping，即bagging。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/bootstarpping1.png" alt="" /></p>

<p><strong>习题</strong></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter7_question1.png" alt="" /></p>

<h2 id="adaptive-boosting">第8讲 Adaptive Boosting</h2>

<p>课程的最开始有一个分辨苹果的例子。以后AdaBoost的时候可以借鉴那个例子。其基本思路是：给予上次分错的样本更高的权重。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/re_weighting_bootstrapping.png" alt="" /></p>

<p>给每个example不同的weight，类似于给予不同的class的样本不同的weight。回忆一下，有时候我们false reject尽可能低，那对于这一类，我们在error measure给予更高的权重。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/re_weighting_bootstrapping2.png" alt="" /></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/false-accept-and-false-reject.png" alt="" /></p>

<p>具体怎么更新下一次训练的样本权重呢，参考下面的图：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/scaling_factor.png" alt="" /></p>

<p>有了样本权重更新公式后，则有一个Preliminary算法：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/adaboost_preliminary.png" alt="" /></p>

<p>得到这么多的g后，怎么得到G，也就是aggregation的方法，我们希望在计算g的时候把aggregation的权重也得到。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/adaboost1.png" alt="" /></p>

<p>那么完整算法为：
<img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/adaboost2.png" alt="" /></p>

<p>下面是一些理论：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Theoretical-Guarantee-of-AdaBoost.png" alt="" /></p>

<p>Decision Stump</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Decision-Stump.png" alt="" /></p>

<p>AdaBoost与Decision Stump的结合 – &gt; AdaBoost-Stump:
efficient feature selection and aggregation</p>

<p><strong>习题</strong></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter8_question1.png" alt="" /></p>

<h2 id="decision-tree">第9讲 Decision Tree</h2>

<p>decision tree的位置，模仿人脑决策过程。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/decision-tree1.png" alt="" /></p>

<p>decision tree缺点：(1)启发式的规则(前人的巧思)，缺乏理论基础；(2)启发式规则很多，需要selection；(3)没有代表性的算法。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Disclaimers-about-Decision-Tree.png" alt="" /></p>

<p>一个基本的decision tree算法：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/basic-decision-tree.png" alt="" /></p>

<p>CART: classification and regression tree。
有两个简单的选择：binary tree；叶子节点是常数。</p>

<p>怎么选择branching，切完后两个子树的纯度最高。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Purifying-in-CART.png" alt="" /></p>

<p>怎么考量”不纯度”</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Impurity-Functions.png" alt="" /></p>

<p>最终CART算法如下：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/cart_algorithm.png" alt="" /></p>

<p>关于CART算法的演算过程，具体请参考 <a href="http://mydisk.com/yzlv/webpage/datamining/xiti.html">决策树算法的计算过程演示</a>，<a href="http://en.wikipedia.org/wiki/Decision_tree_learning">Decision tree learning</a>，<a href="http://www.academia.edu/7032069/An_example_of_calculating_gini_gain_in_CART">An example of calculating gini gain in CART</a></p>

<p>几种决策树算法的区别：</p>

<p>C4.5算法是在ID3算法的基础上采用信息增益率的方法选择测试属性。 ID3算法和C4.5算法虽然在对训练样本集的学习中可以尽可能多地挖掘信息，但其生成的决策树分支较大，规模较大。为了简化决策树的规模，提高生成决策树的效率，又出现了根据GINI系数来选择测试属性的决策树算法CART。
CART算法采用一种二分递归分割的技术，与基于信息熵的算法不同，CART算法对每次样本集的划分计算GINI系数，GINI系数，GINI系数越小则划分越合理。CART算法总是将当前样本集分割为两个子样本集，使得生成的决策树的每个非叶结点都只有两个分枝。因此CART算法生成的决策树是结构简洁的二叉树。</p>

<p>Regularization</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Regularization-by-Pruning.png" alt="" /></p>

<p>当有categorical features时，CART也可以灵活处理。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Branching-on-Categorical-Features.png" alt="" /></p>

<p>如果有缺失特征的话，怎么办？可以利用surrogate feature。</p>

<p>看一个CART的例子：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/cart_example.png" alt="" /></p>

<p><strong>习题</strong></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter9_question1.png" alt="" /></p>

<h2 id="random-forest">第10讲 random forest</h2>
<p>Bagging and Decision Tree，将这两者合在一起，就是Random forest。</p>

<p>random forest (RF) = bagging + fully-grown C&amp;RT decision tree</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Bagging-and-Decision-Tree.png" alt="" /></p>

<p>三个优点：</p>

<ul>
  <li>highly parallel/efficient to learn</li>
  <li>inherit pros of C&amp;RT</li>
  <li>eliminate cons of fully-grown tree</li>
</ul>

<p>因为是random forest，除了在bootstrapping时利用data randomness，还可以randomly sample d’ feature from x。即original RF re-sample new subspace for each b(x) in C&amp;RT。</p>

<p>那么更进一步了，RF = bagging + random-subspace C&amp;RT</p>

<p>random-combination的意思是：随机抽样一些features后，line combination，作为一个新的feature切分点。那么original RF consider d′ random low-dimensional projections for each b(x) in C&amp;RT。</p>

<p>所以，再进一步：RF = bagging + random-combination C&amp;RT</p>

<p>从上面可以看出，randomness是随处不在的。</p>

<p>回顾一下bagging的过程，每次随机抽样一些数据，这样下去，总会有一些样本是一直未被抽中的。未被抽中的概率计算为：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/numbers_of_oob.png" alt="" /></p>

<p>有了这些out-of-bag (OOB) examples后，可以将其作为validation set来使用。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/oob_vs_validation.png" alt="" /></p>

<p>那么，相比于原来的validation过程，RF可以做self-validation，也就是在训练的过程中，把model选择顺便也做了。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/model_selection_by_oob.png" alt="" /></p>

<p>接着看下Feature selection，decision tree正好是一个内建的feature selection过程。</p>

<p>先看下利用linear model做feature importance判别，训练完的模型，weight越大，表示feature越重要。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Feature_Selection_by_Importance.png" alt="" /></p>

<p>而RF可以采用permutation test来做特征选择。所谓permutation test，也就是对某一个特征，对所有样本上该维度的特征值做随机排列，然后在这个样本集上计算RF performance。用原来的performance减去这个新的performance后，就得到该特征的重要性。如下图所示：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/feature_selection_by_permutation_test.png" alt="" /></p>

<p>但是在RF上，因为OOB的存在，可以利用Eoob(G)-Eoob^p(G)。Eoob^p(G)是通过在OOB上permute某一维特征值。<strong>这里后续可以再深挖</strong></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/feature_importance_by_rf.png" alt="" /></p>

<p><strong>习题</strong></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter10_question1.png" alt="" /></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter10_question2.png" alt="" /></p>

<h2 id="gradient-boosted-decision-treegbdt">第11讲 Gradient Boosted Decision Tree(GBDT)</h2>

<p>random forest用一句话来总结，则是：bagging of randomized C&amp;RT trees with automatic validation and feature selection。</p>

<p>比较一下Random forest和AdaBoost Tree。
<img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/compare_rd_adaboost-tree.png" alt="" /></p>

<p>但是要做AdaBoost Tree的话，首先需要weighted DTree。这个在LR,SVM等模型上容易做到，但是在DT上很难。所以我们换个思路，如果我们想给某个样本加个weight，可以在sample样本的时候，增大或者减小它的概率即可。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Randomized-Base-Algorithm.png" alt="" /></p>

<p>所以AdaBoost-DTree的组成由下图所示：
<img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/AdaBoost-DTree1.png" alt="" /></p>

<p>在adaboost算法中，如果一个g的错误率为0的话，那么这个g的权重将是无限大的。而在决策树的世界里，如果是full-grown的话，在训练数据上，错误率为0是很容易办到的。</p>

<p>那么为了避免这种过拟合的情况存在，我们需要对DT做剪枝。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/AdaBoost-DTree2.png" alt="" /></p>

<p>当我们extremely剪枝时，譬如限制树的高度小于等于1，那此时DT就变成了decision stump。所以有了adaboost-stump算法，它是AdaBoost-DTree的一种特例。</p>

<p>2，3，4节 未完待续。</p>

<p>更多具体的内容，请参考单独的文章： <a href="http://zzbased.github.io/2015/04/03/Aggregation模型.html">Aggregation模型</a></p>

<p><strong>习题</strong></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter11_question1.png" alt="" /></p>

<h2 id="section-3">第12讲 神经网络</h2>

<p><strong>Motivation</strong></p>

<p>通过”Linear Aggregation of Perceptrons”，可以完成AND，OR，NOT等操作，可以完成 convex set等操作，但是不能完成XOR操作。怎么办？只能multi-layer perceptron。</p>

<p>XOR(g1, g2) = OR(AND(−g1, g2), AND(g1, −g2))</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/perceptron_powerful_limitation.png" alt="" /></p>

<p>perceptron (simple)
=⇒ aggregation of perceptrons (powerful)
=⇒ multi-layer perceptrons (more powerful)</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter12_question1.png" alt="" /></p>

<p><strong>Neural Network Hypothesis</strong></p>

<p>output：any linear model can be used；
transformation function of score (signal) s：不用linear，因为多层线性=&gt;whole network linear。也不用阶梯函数(0-1)，因为它不可微。通常的选择有tanh(x)，sigmoid(s)。</p>

<p>tanh(x) = [exp(s)-exp(-s)] / [exp(s)+exp(-s)] = 2sigmoid(2x)-1</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter12_question2.png" alt="" /></p>

<p><strong>Backpropagation</strong></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Backpropagation_1.png" alt="" /></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Backpropagation_2.png" alt="" /></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter12_question3.png" alt="" /></p>

<p><strong>Optimization</strong></p>

<p>当multiple hidden layers，一般都是non-convex。对于最优化来说，不容易求得全局最优解。GD/SGD可能只能求出局部最优解。</p>

<p>对Wij做不同的初始化，可能有不同的局部最优解。所以对初始化值比较敏感。</p>

<p>有效的建议是：不要初始化太大的weights，因为large weight，加上tanh后，将saturate。如果做梯度下降的话，那段区域里有small gradient。所以建议要try some random&amp;small ones。</p>

<p>神经网络的dVC=O(VD)，V表示神经元的个数，D表示weight的个数，也就是edge的数目。</p>

<p>VC维太大，容易overfit。可以加一个L2 regularizer。但是加L2后，带来的只是shrink weights。我们希望可以得到sparse解，那么就可以用L1 regularizer，但L1不可微分。
所以另外一个选择是：weight-elimination（scaled L2），即large weight → median shrink; small weight → median shrink</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/weight-elimination-regularizer.png" alt="" /></p>

<p>Early Stopping，随着t 增长，VC维越大。所以合适的t 就够了。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/BP-Early-Stopping.png" alt="" /></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter12_question4.png" alt="" /></p>

<h2 id="deep-learning">第13讲 Deep Learning</h2>

<p>structural decisions: key issue for applying NNet。模型结构很关键。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Challenges-and-Key-Techniques-for-Deep-Learning.png" alt="" /></p>

<p>hinton 2006提出的：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/A-Two-Step-Deep-Learning-Framework.png" alt="" /></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Information-Preserving-Neural-Network.png" alt="" /></p>

<p>Auto-encoder的作用：监督学习的话，给予做特征；无监督学习的话，用来做密度预测，也可以用来做异常点检测。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Deep-Learning-with-Autoencoders.png" alt="" /></p>

<p>Regularization in Deep Learning的方法：</p>

<ul>
  <li>structural decisions/constraints，譬如卷积神经网络，循环神经网络</li>
  <li>weight decay or weight elimination regularizers</li>
  <li>Early stopping</li>
  <li>dropout，dropconnect等</li>
  <li>denosing</li>
</ul>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/denosing_auto-encoder.png" alt="" /></p>

<p>Linear Autoencoder Hypothesis
<img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/linear-autoencoder-hypothesis.png" alt="" /></p>

<p>简单点看就是：h(x) = WW^T x</p>

<p>复习一下特征值和特征向量。<a href="http://zh.wikipedia.org/wiki/特征向量">特征向量wiki</a></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/optimal_v_linear_autoencoder.png" alt="" /></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/pca-for-autoencoder.png" alt="" /></p>

<p><strong>习题</strong></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter13_question1.png" alt="" /></p>

<h2 id="radial-basis-function-network">第14讲 Radial Basis Function Network</h2>

<p>以前在讲SVM时，有提到RBF kernel(gaussian kernel)，这里回顾一下。高斯核是将x空间变换到z空间的无限维。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/gaussian_svm.png" alt="" /></p>

<p>基于高斯核的SVM如下所示，相当于是support vector上的radial hypotheses的线性组合。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/gaussian_svm2.png" alt="" />
So，Radial Basis Function (RBF) Network: linear aggregation of radial hypotheses。</p>

<p>将RBF network类比于neural network，output layer是一样的，都是线性组合，不一样是隐藏层(在RBF network里，是distance + gaussian)。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/rbf_network.png" alt="" /></p>

<p>基于RBF network来解释SVM：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/RBF-Network-Hypothesis.png" alt="" /></p>

<p>此时，需要学习的参数是 u_m(是centers)，b_m(是不同rbf线性组合的系数)。</p>

<p>kernel是Z空间的相关性度量，而RBF是X空间的相关性度量。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/rbf_vs_kernel.png" alt="" /></p>

<p>所以RBF network： distance similairty-to-centers as feature transform。</p>

<p>Full RBF Network：是说将所有样本点都参与到运算里(M=N)。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/full_rbf_network.png" alt="" /></p>

<p>full rbf network是一个lazy way to decide u_m。</p>

<p>Nearest-Neighbor：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Nearest-Neighbor.png" alt="" /></p>

<p>如果是利用RBF network做regression呢？如下所示。但是这样做了后，Ein(g)=0，这样势必会overfit。
所以需要做正则化。正则化的思路有：(1)类似于kernel ridge regression，加正则项。(2)fewer centers，譬如support vector。constraining number of centers and voting weights。</p>

<p>那怎样才能做到fewer centers呢？通常的方法就是：寻找prototypes。那how to extract prototypes?</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Regularized-Full-RBF-Network.png" alt="" /></p>

<p>这里提到一种算法：k-means cluster。k-means的优化思路为：alternating minimization。说到这，EM也属于alternating minimization。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/k-Means-Algorithm.png" alt="" /></p>

<p>OK，现在prototypes提取到了，接下来把基于k-means的rbf network写出来。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/RBF-Network-Using-k-Means.png" alt="" /></p>

<p>下面是实战，先看一个k-means的例子：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/k-means-examples.png" alt="" /></p>

<p>可以看到，k和初始化，在k-means算法里非常关键。</p>

<p>通常随机地从样本中挑k个出来作为k个初始的聚类中心。但这不是个明智的选择。它有可能会导致图像趋于稠密聚集某些区域，因为如果训练样本本身就在某个区域分布非常密，那么我们随机去选择聚类中心的时候，就会出现就在这个数据分布密集的地方被选出了很多的聚类中心。</p>

<p>那么怎么做k-means的初始化呢？</p>

<ul>
  <li>多次运行数据集合，选择最小的SSE的分簇结果作为最终结果。该方法依赖于数据集合和簇数量，对分簇结果有比较大影响，所以在某些场景下效果也不是很好。</li>
  <li>抽取数据集合样本，对样本进行Hierarchical Clustering技术，从中抽取K个Clustering作为初始中心点。该方法工作良好，知识有两点限制条件：抽样数据不能太大，因为Hierarchical Clustering比较耗时间；K值相对于抽样数据比较小才行。</li>
  <li>kmeans++算法。<a href="http://en.wikipedia.org/wiki/K-means%2B%2B">kmeans++ wiki</a>，<a href="http://www.cnblogs.com/shelocks/archive/2012/12/20/2826787.html">kmenas++中文</a>。</li>
</ul>

<p>K-means++的步骤为：</p>

<ol>
  <li>从输入的数据点集合中随机选择一个点作为第一个聚类中心</li>
  <li>对于数据集中的每一个点x，计算它与最近聚类中心(指已选择的聚类中心)的距离D(x)</li>
  <li>选择一个新的数据点作为新的聚类中心，选择的原则是：D(x)较大的点，被选取作为聚类中心的概率较大</li>
  <li>重复2和3直到k个聚类中心被选出来</li>
  <li>利用这k个初始的聚类中心来运行标准的k-means算法</li>
</ol>

<p>更多关于k-means初始化的方法，请参考 <a href="http://www.mecs-press.org/ijisa/ijisa-v4-n1/IJISA-V4-N1-3.pdf">Efficient and Fast Initialization Algorithm for K- means Clustering</a></p>

<p><strong>扩展阅读</strong></p>

<ul>
  <li>
    <p>发表在Science的论文：基于密度的快速无监督聚类方法 <a href="http://t.cn/RAASZ4q">Clustering by fast search and find of density peaks. A Rodriguez, A Laio (2014) </a> 很棒，推荐给没看过的朋友，另有相关中文两篇：http://t.cn/RPoKmOi http://t.cn/RPOs6uK 供参考理解 云:http://t.cn/RAACowz</p>

    <p>对所有坐标点，基于相互距离，提出了两个新的属性，一是局部密度rho，即与该点距离在一定范围内的点的总数，二是到更高密度点的最短距离delta。作者提出，类簇的中心是这样的一类点：它们被很多点围绕（导致局部密度大），且与局部密度比自己大的点之间的距离也很远。</p>
  </li>
  <li>
    <p>Canopy 聚类算法的基本原则是：首先应用成本低的近似的距离计算方法高效的将数据分为多个组，这里称为一个Canopy。Canopy 之间可以有重叠的部分。然后采用严格的距离计算方式准确的计算在同一 Canopy 中的点，将他们分配与最合适的簇。Canopy 聚类算法经常用于 K 均值聚类算法的预处理，用来找合适的 k 值和簇中心。<a href="http://blog.pureisle.net/archives/2045.html">Clustering Algorithm/聚类算法</a>，<a href="http://en.wikipedia.org/wiki/Canopy_clustering_algorithm">Canopy clustering algorithm</a>。</p>
  </li>
  <li>
    <p><a href="http://t.cn/RAwxOJx">文章 K-means Clustering with scikit-learn</a> PyData SV 2014上Sarah Guido的报告，Python下用Scikit-Learn做K-means聚类分析的深入介绍，涉及k值选取、参数调优等问题，很实用 GitHub:http://t.cn/RAwxsFS 云(视频+讲义):http://t.cn/RAwJJkG</p>
  </li>
  <li>
    <p><a href="http://t.cn/RwlDlgq">文章 Divining the ‘K’ in K-means Clustering</a> 用G-means算法确定K-means聚类最佳K值，G-means能很好地处理stretched out clusters(非球面伸展型类簇)</p>
  </li>
  <li>
    <p><a href="http://www.cc.gatech.edu/~isbell/tutorials/rbf-intro.pdf">RBF的核心论文 Introduction to Radial Basis Function Networks</a></p>
  </li>
</ul>

<p><strong>习题</strong></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2_chapter14_question1.png" alt="" /></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2_chapter14_question2.png" alt="" /></p>

<h2 id="matrix-factorization">第15讲 Matrix Factorization</h2>

<p>从”Linear Network” Hypothesis说起，用来做推荐，也就是根据feature x，预测得分y。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Linear-Network-Hypothesis.png" alt="" /></p>

<p>求解上面的linear network，采用squared error。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/linear_model_for_recommendation.png" alt="" /></p>

<p>从上面的求解过程，得到Matrix factorization：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Matrix-Factorization1.png" alt="" /></p>

<p>具体下来，应该怎么求解呢？考虑到这里面有两个变量W和V，这时可以采用alternating minimization。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Matrix-Factorization-Learning.png" alt="" /></p>

<p>所以，得到Alternating Least Squares方法。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Alternating-Least-Squares.png" alt="" /></p>

<p>比较一下 linear autoencoder 和 matrix factorization。linear autoencoder
≡ special matrix factorization of complete X</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Linear-Autoencoder-versus-Matrix-Factorization.png" alt="" /></p>

<p>上面讲述了 alternating解法，matrix factorization还可以利用Stochastic gradient descent求解。SGD：most popular large-scale matrix factorization algorithm，比alternating速度更快。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/SGD-for-Matrix-Factorization.png" alt="" /></p>

<p>举一个SGD的例子。在KDDCup 2011 Track1中，因为该推荐任务与时间系列有关，所以在优化时，没有用stochastic GD算法，而是采用了time-deterministic GD算法，也就是最近的样本最后参与计算，这样可以保证最近的样本拟合得更好。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/KDDCup-2011-Track1.png" alt="" /></p>

<h2 id="extraction-models">Extraction Models总结</h2>

<p>将特征转换纳入到我们的学习过程。</p>

<p>Extraction Models： neural network，RBF network，Matrix Factorization。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Map-of-Extraction-Models.png" alt="" /></p>

<p>Extraction Techniques：function gradient descnet，SGD。</p>

<p>无监督学习用于预训练，例如autoencoder，k-means clustering。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Map-of-Extraction-Techniques.png" alt="" /></p>

<p>regularization：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Pros-and-Cons-of-Extraction-Models.png" alt="" /></p>

<h2 id="finale-">第16讲 Finale 大总结</h2>

<p>Exploiting Numerous Features via Kernel：Polynomial Kernel，Gaussian Kernel等。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Exploiting-Numerous-Features-via-Kernel.png" alt="" /></p>

<p>Exploiting Predictive Features via Aggregation：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Exploiting-Predictive-Features-via-Aggregation.png" alt="" /></p>

<p>Exploiting Hidden Features via Extraction：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Exploiting-Hidden-Features-via-Extraction.png" alt="" /></p>

<p>Exploiting Low-Dim. Features via Compression：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Exploiting-Low-Dim.Features-via-Compression.png" alt="" /></p>

<p><strong>习题</strong></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter16_question1.png" alt="" /></p>

  </article>

</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">100的技术博客</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col  footer-col-1">
        <ul class="contact-list">
          <li>100的技术博客</li>
          <li><a href="mailto:zero_based@foxmail.com">zero_based@foxmail.com</a></li>
        </ul>
      </div>

      <div class="footer-col  footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/zzbased">
              <span class="icon  icon--github">
                <svg viewBox="0 0 16 16">
                  <path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
                </svg>
              </span>

              <span class="username">zzbased</span>
            </a>
          </li>
          

          
          <li>
            <a href="https://twitter.com/zero_based">
              <span class="icon  icon--twitter">
                <svg viewBox="0 0 16 16">
                  <path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                  c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
                </svg>
              </span>

              <span class="username">zero_based</span>
            </a>
          </li>
          
        </ul>
      </div>

      <div class="footer-col  footer-col-3">
        <p class="text">机器学习，自然语言处理，计算广告学，工作与生活，总结与温习
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
