<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Aggregation模型</title>
  <meta name="description" content="Aggregation模型(集成学习)">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://yourdomain.com/2015/04/03/Aggregation%E6%A8%A1%E5%9E%8B.html">
  <link rel="alternate" type="application/rss+xml" title="100的技术博客" href="http://yourdomain.com/feed.xml" />
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">100的技术博客</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
        
          
          <a class="page-link" href="/about/">About</a>
          
        
          
        
          
        
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Aggregation模型</h1>
    <p class="post-meta">Apr 3, 2015</p>
  </header>

  <article class="post-content">
    <h1 id="aggregation">Aggregation模型(集成学习)</h1>

<h4 id="author-vincentyaotencentcom">author: vincentyao@tencent.com</h4>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<h2 id="section">开篇</h2>

<p>Aggregation模型，即融合式的模型，也叫Ensemble Learning。那什么是Aggregation模型呢？通俗的讲，就是多算法融合。它的思想相当简单直接，以至于用一句俗语就可以完美概括：三个臭皮匠，顶个诸葛亮。实际操作中，Aggregation模型把大大小小的多种算法融合在一起，共同协作来解决一个问题。这些算法可以是不同的算法，也可以是相同的算法。</p>

<p>根据融合的方式，我们可以将Aggregation模型分为三种：(1)Uniform，将多个模型平均的合并在一起；(2)Linear组合，将多个模型利用linear model融合起来；(3)Conditional，不同的情形使用不同的模型，即将多个模型利用non-linear model融合起来。</p>

<p>在下文中，我们用g_t 表示第t个单模型，Aggregation model所要做的就是把多个g_t 融合起来。而融合的过程，我们又可以分为两类：(1)Blending，已知多个g_t，再将多个g_t 融合起来，即aggregation after getting g_t；(2)Learning: 一边学习g_t，一边合并多个g_t，即aggregation as well as getting g_t。</p>

<p>所以，对Aggregation模型基本的划分，则如下表所示。其中，对每一种融合类型，都列举了一种典型的Aggregation模型。</p>

<table>
  <thead>
    <tr>
      <th>Aggregation Type</th>
      <th style="text-align: center">Blending(已知g，再融合多个g)</th>
      <th style="text-align: right">Learning(一边学习g，一边融合多个g)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>uniform</td>
      <td style="text-align: center">voting/averaging</td>
      <td style="text-align: right">Bagging</td>
    </tr>
    <tr>
      <td>non-uniform</td>
      <td style="text-align: center">linear</td>
      <td style="text-align: right">AdaBoost，GradientBoost</td>
    </tr>
    <tr>
      <td>conditional</td>
      <td style="text-align: center">stacking(non-linear)</td>
      <td style="text-align: right">Decision Tree</td>
    </tr>
  </tbody>
</table>

<p>有了多种Aggregation模型后，还可以将Aggregation模型再融合。如果将bagging配上decision tree，则是random forest。如果将AdaBoost配上Decision Tree，则是AdaBoost-DTree。如果将GradientBoost配上Decision Tree，则是大名鼎鼎的GBDT(Gradient Boost Decision Tree)。</p>

<p>OK，对Aggregation模型有了大体的认识后，下文将来讲述比较具有代表性的Aggregation模型。本文大致分为五个部分：第一部分介绍Decision Tree；第二部分介绍Random forest；第三部分介绍AdaBoost；第四部分介绍Gradient Boost Decision Tree；最后对Aggregation模型再做一下对比与总结。</p>

<h2 id="decision-tree">Decision Tree(决策树)</h2>

<p>按照Aggregation的方式，可以将决策树的表达式写为：</p>

<script type="math/tex; mode=display">G(x)=\sum_{t=1}^T{q_t(x).g_t(x)}</script>

<p>g_t表示一个base hypothesis，在决策树里，也就是每条路径的叶子节点。q_t表示条件，表示输入x 是不是在path t上。下图是一个决策树的例子，图中有5个叶子节点，则有5个g_t。通常情况下，我们都用递归形式来表示一个决策树。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/decision_tree_recursive_view.png" alt="" /></p>

<p>一棵树的训练过程为：根据一个指标，分裂训练集为几个子集。这个过程不断的在产生的子集里重复递归进行，即递归分割。当一个训练子集的类标都相同时递归停止。这种决策树的自顶向下归纳 (TDITD)是贪心算法的一种, 也是目前为止最为常用的一种训练方法，但不是唯一的方法。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/decision_tree_train_algorithm.png" alt="" /></p>

<p>从decision tree的训练过程可以看到，训练的关键点是branching(即在每一步选择一个最好的属性来分裂)。那如何branching呢，通常的做法是：Split training set at “the best value” of “the best feature”。”最好”的定义是使得子节点中的训练集尽量的纯，不同的算法使用不同的指标来定义”最好”。</p>

<ul>
  <li>Information gain (ratio)：信息增益是用来衡量样本集S下属性A分裂时的信息熵减少量。信息增益是信息熵的有效减少量，值越高，说明失去的不确定性越多，那么它就应该越早作为决策的依据属性。</li>
  <li>Gini index：基尼不纯度表示一个随机选中的样本在子集中被分错的可能性。基尼不纯度为这个样本被选中的概率乘以它被分错的概率。当一个节点中所有样本都是一个类时，基尼不纯度为零。</li>
</ul>

<p>根据决策树的输出y的类型，可以将decision tree分为：分类树和回归树。</p>

<ul>
  <li>分类树：预测分类标签；例如C4.5，选择划分成两个分支后熵最大的feature；</li>
  <li>回归树：预测实数值；回归树的结果是可以累加的；最小化均方差；</li>
</ul>

<h3 id="cart">CART</h3>

<p>CART全称”Classification and Regression Tree”，是一种较常用的决策树。为了简化决策过程，它有两个基本选择：(1)二叉树；(2)g_t(x)输出是一个常数。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/cart_two_choices.png" alt="" /></p>

<p>CART的branch利用Impurity function来衡量。如果目标是回归，利用regression error作为Impurity function。如果目标是分类，利用Gini index作为impurity function。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/cart_impurity_function.png" alt="" /></p>

<p>利用<a href="http://mydisk.com/yzlv/webpage/datamining/xiti.html">利用信息增益，决策树算法的计算过程演示</a>的例子，如果采用Gini系数的话，计算过程为：</p>

<blockquote>
  <p>按性别属性变量进行分裂。9/20<em>(1- (6/9)^2 - (3/9)^2) + 11/20</em>(1- (7/11)^2 - (4/11)^2) = 0.4545。</p>
</blockquote>

<blockquote>
  <p>按车型变量进行分裂(运动 vs 豪华+家用)。9/20<em>(1- (1/9)^2 - (8/9)^2) + 11/20</em>(1 - (11/11)^2) = 0.088。</p>
</blockquote>

<p>CART的termination条件是：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/CART_termination.png" alt="" /></p>

<p>所以，总结下来，CART是：fully-grown tree with constant leaves that come from bi-branching by purifying。</p>

<p>关于CART算法的演算过程，可以参考：<a href="http://www.academia.edu/7032069/An_example_of_calculating_gini_gain_in_CART">An example of calculating gini gain in CART</a></p>

<p>一个fully-grown的CART，在训练数据上可以做到无差错(即Ein=0)，但这样往往是过拟合的。所以需要regularizer，通常的方法是：限制叶子节点的个数。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/CART_regularizer.png" alt="" /></p>

<h3 id="decision-tree-1">Decision tree小结</h3>
<p>Decision tree优点：</p>

<blockquote>
  <p>(1)易于理解和解释；(2)即可以处理数值型数据也可以处理类别型数据；(3)生成的模式简单，对噪声数据有很好的健壮性。</p>
</blockquote>

<p>Decision tree缺点：</p>

<blockquote>
  <p>(1)启发式的规则(前人的巧思)，缺乏理论基础，并且启发式规则很多，需要selection；(2)容易过拟合；(3)对那些有类别型属性的数据, 信息增益会有一定的偏置；(4)训练一棵最优的决策树是一个完全NP问题。</p>
</blockquote>

<p>几种决策树算法的区别：</p>

<blockquote>
  <p>ID3算法使用信息增益。C4.5算法是在ID3算法的基础上采用<strong>信息增益率</strong>的方法选择测试属性。
ID3算法和C4.5算法虽然在对训练样本集的学习中可以尽可能多地挖掘信息，但其生成的决策树分支较大，规模较大。</p>
</blockquote>

<blockquote>
  <p>为了简化决策树的规模，提高生成决策树的效率，又出现了根据GINI系数来选择测试属性的决策树算法CART。
CART算法采用一种二分递归分割的技术，与基于信息熵的算法不同，CART算法对每次样本集的划分计算GINI系数，GINI系数，GINI系数越小则划分越合理。CART算法总是将当前样本集分割为两个子样本集，使得生成的决策树的每个非叶结点都只有两个分枝。因此CART算法生成的决策树是结构简洁的二叉树。</p>
</blockquote>

<p>更多参考资料</p>

<ul>
  <li><a href="http://www.raychase.net/1275">使用ID3算法构造决策树</a></li>
  <li><a href="http://www.raychase.net/1951">C4.5&amp; CART</a></li>
  <li><a href="http://www.autonlab.org/tutorials/dtree.html">Decision Trees Tutorial by Andrew Moore</a></li>
  <li><a href="http://en.wikipedia.org/wiki/Decision_tree_learning">Wiki: Decision tree learning</a></li>
  <li><a href="http://pages.cs.wisc.edu/~jerryzhu/cs540/handouts/dt.pdf">Machine Learning: Decision Trees.CS540</a></li>
</ul>

<h2 id="random-forest">Random forest</h2>

<h3 id="bagging">Bagging</h3>

<p>开篇里已经简要介绍过uniform aggregation。在uniform融合过程中，diversity非常重要，可以利用多个不同的模型，可以用一个模型但不同的参数，可以采用不同的随机值初始化模型，可以随机抽样数据来训练多个模型。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/bagging_diversity.png" alt="" /></p>

<p>Bagging(也称bootstrap aggregation)是一种基于data randomness的uniform的融合方式。
bootstrapping指从给定训练集中有放回的均匀抽样。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/bootstarpping_aggregation.png" alt="" /></p>

<p>Uniform blending有一个好的特性，它可以降低模型的variance。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/uniform-blending-reduces-variance.png" alt="" /></p>

<h3 id="random-forest-1">Random forest</h3>

<p>Bagging方法通过voting可以减小variance，而decision tree具有良好的bias表现，但有large variance(特别是fully-grown DTree)。所以一个直观的想法，能否将Bagging和Decision Tree这两者融合在一起，这样得到的新模型则具备了相对良好的bias和variance表现，这就是Random forest。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/Bagging-and-Decision-Tree.png" alt="" /></p>

<p>random forest (RF) = bagging + fully-grown C&amp;RT decision tree</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/random_forest_algorithm.png" alt="" /></p>

<p>Bagging是data randomness，而为了增强diversity(即得到更多不同的g_t)，还可以对建立decision tree的features做抽样，即random subspace。该思想也可以套用在其他模型上(譬如svm，lr)。</p>

<p>在build decision tree时，每一次做branch的时候，都可以做一次random re-sample feature，这样可以让g_t 更不一样。</p>

<p>除此外，还可以利用random combination，也就是在branching时，不仅仅只是随机选择一个feature做切分，还可以random多个feature，将feature做linear combination后，再来做切分。random combination理论上就是一个perceptron过程。</p>

<p>所以，Random forest是bagging + random-subspace &amp; random-combination CART，可以看到randomness思想在random forest里无处不在。</p>

<p>回顾一下bagging的过程，每次随机抽样一些数据，这样下去，总会有一些样本是一直未被抽中的，这些样本我们称之为out-of-bag examples，它们可以被当作validation set来使用。所以，random forest的另一个重要特性是：相比于通常的validation过程，RF可以做self-validation，也就是在训练的过程中，把model选择顺便也做了。</p>

<h2 id="adaboost">AdaBoost</h2>

<h3 id="boosting">Boosting</h3>

<p>Boosting的思想相当的简单，对一份数据，建立M个模型（比如分类），一般这种模型比较简单，称为弱分类器(weak learner)。每次分类都将上一次分错的数据权重提高一点再进行分类，这样最终得到的分类器在测试数据与训练数据上都可以得到比较好的成绩。Boosting也就是开篇所述的linear blending模型。</p>

<p>boosting可以用下面公式来表示：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/boosting_formula1.png" alt="" /></p>

<p>其中alpha是权重，y_m是弱分类器，整体就是一个linear模型。</p>

<p>从Function Space里的Numerical Optimization角度看Boosting。boosting也叫forward stagewise additive modeling，因为在迭代的过程中，我们不能再回退去修改以前的参数，一切只能向前看了。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/Function-Space-optimizaition1.png" alt="" />
<img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/Function-Space-optimizaition2.png" alt="" />
<img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/Function-Space-optimizaition3.png" alt="" /></p>

<p>不同的损失函数和极小化损失函数方法决定了boosting的最终效果，先说几个常见的boosting：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/boosting_category.png" alt="" /></p>

<p>上图出自 Machine Learning A Probabilistic Perspective Table 16.1(P556)。不过其中注释的algorithm和个人理解有些不一致，Absolute error应该是叫Least Absolute Deviation (LAD) Regression。Gradient boosting的常见示例是squared loss。</p>

<p>Boosting方法共性：</p>

<ul>
  <li>Train one base learner at a time</li>
  <li>Focus it on the mistakes of its predecessors</li>
  <li>Weight it based on how ‘useful’ it is in the ensemble (not on its training error)</li>
</ul>

<p>更多请参考：<a href="http://www.cs.man.ac.uk/~stapenr5/boosting.pdf">Introduction to Boosting</a></p>

<h3 id="adaboost-1">AdaBoost</h3>
<p>AdaBoost(Adaptive Boosting)由Yoav Freund和Robert Schapire提出。AdaBoost方法的自适应在于：前一个分类器分错的样本会被用来训练下一个分类器。AdaBoost方法对于噪声数据和异常数据很敏感。但在一些问题中，AdaBoost方法相对于大多数其它学习算法而言，不会很容易出现过拟合现象。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/adaboost_algorithm1.png" alt="" /></p>

<p>AdaBoost方法是一种迭代算法，在每一轮中加入一个新的弱分类器，直到达到某个预定的足够小的错误率。每一个训练样本都被赋予一个权重，表明它被某个分类器选入训练集的概率。如果某个样本点已经被准确地分类，那么在构造下一个训练集中，它被选中的概率就被降低；相反，如果某个样本点没有被准确地分类，那么它的权重就得到提高。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/adaboost_pseudo_code.png" alt="" /></p>

<p>通过这样的方式，AdaBoost方法能“聚焦于”那些较难分（更富信息）的样本上。在具体实现上，最初令每个样本的权重都相等，对于第k次迭代操作，我们就根据这些权重来选取样本点，进而训练分类器g_k。然后就根据这个分类器，来提高被它分错的的样本的权重，并降低被正确分类的样本权重。然后，权重更新过的样本集被用于训练下一个分类器g_k。整个训练过程如此迭代地进行下去。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/Theoretical-Guarantee-of-AdaBoost.png" alt="" /></p>

<p><strong>Boosting view of AdaBoost</strong></p>

<p>AdaBoost方法中使用的分类器可能很弱（比如出现很大错误率），但只要它的分类效果比随机好一点（比如两类问题分类错误率略小于0.5），就能够改善最终得到的模型。而错误率高于随机分类器的弱分类器也是有用的，因为在最终得到的多个分类器的线性组合中，可以给它们赋予负系数，同样也能提升分类效果。</p>

<h3 id="adaboost-dtree">AdaBoost-DTree</h3>
<p>将AdaBoost和decision tree融合起来，就是AdaBoost-DTree，如下图所示：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/adaboost_decision_tree.png" alt="" /></p>

<p>在AdaBoost模型中，需要给予不同样本不同的权重。如果模型是svm或lr的话，很容易把weight加到每个instance上，只需要在计算loss function的时候，乘上相应的weight系数即可。</p>

<p>但在AdaBoost-DTree模型中，对于DTree，不方便赋予weight给不同样本。这时可以利用bootstrap的思想。</p>

<p>Bootstrap，它在每一步迭代时不改变模型本身，而是从N个instance训练集中按随机抽取N个instance出来（单个instance可以被重复sample），对着这N个新的instance再训练一轮，由于数据集变了迭代模型训练结果也不一样。</p>

<p>在AdaBoost-DTree模型抽样时，并不是Uniform抽样，而是根据一定概率来抽样。如果一个instance在前面分错的越厉害，它的概率就被设的越高，这样就能同样达到逐步关注被分错的instance，逐步完善的效果。</p>

<h3 id="optimizationadaboost">Optimization视角看AdaBoost</h3>

<p>从前述AdaBoost的训练过程，可以得到其训练目标为：让正确instance的weight越来越小，正确的instance个数越多越好。</p>

<p>那么其最终目标为：第T次训练时，所有instance的weight之和最小。写出其Error function为：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/AdaBoost-Error-Function1.png" alt="" />
<img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/AdaBoost-Error-Function2.png" alt="" />
<img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/AdaBoost-Error-Function3.png" alt="" /></p>

<p>从上面的优化目标，可以得到AdaBoost的loss function是exponential loss。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/adaboost_lost_function.png" alt="" /></p>

<p>为什么选择Exponential Loss？</p>

<ul>
  <li>Loss is higher when a prediction is wrong.</li>
  <li>Loss is steeper when a prediction is wrong.</li>
  <li>Precise reasons later</li>
</ul>

<p>求解AdaBoost的优化目标，得到下一个h(x_n)即为A(base algorithm)，h上的权重即为a_t。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/Gradient-Descent-on-AdaBoost.png" alt="" /></p>

<p>具体的推导过程参考”机器学习技法”课程，这里不赘述了。总结下来，AdaBoost：steepest decent with approximate functional gradient。</p>

<h2 id="gbdt">GBDT</h2>

<h3 id="gradientboost">GradientBoost</h3>

<p>Gradient Boosting是一种Boosting的方法。
与传统的Boost的区别是，每一次的计算是为了减少上一次的残差(residual)，而为了消除残差，我们可以在残差减少的梯度(Gradient)方向上建立一个新的模型。所以说，在Gradient Boost中，每个新的模型的建立是为了使得之前模型的残差往梯度方向减少，与传统Boost对正确、错误的样本进行加权有着很大的区别。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/gradient_boost1.png" alt="" /></p>

<p>GradientBoost: allows extension to different err for regression/soft classification/etc。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/gradientBoost_for_regression.png" alt="" /></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/residuals_for_gbdt.png" alt="" /></p>

<p>更多请参考<a href="http://www.cnblogs.com/LeftNotEasy/archive/2011/01/02/machine-learning-boosting-and-gradient-boosting.html">模型组合(Model Combining)之Boosting与Gradient Boosting</a></p>

<h3 id="gradient-boost-decision-tree">Gradient boost decision tree</h3>
<p>目前GBDT有两个不同的描述版本。<a href="http://hi.baidu.com/hehehehello/item/96cc42e45c16e7265a2d64ee">残差版本</a>把GBDT当做一个残差迭代树，认为每一棵回归树都在学习前N-1棵树的残差。<a href="http://blog.csdn.net/dark_scope/article/details/24863289">Gradient版本</a>把GBDT说成一个梯度迭代树，使用梯度下降法求解，认为每一棵回归树在学习前N-1棵树的梯度下降值。这两种描述版本我认为是一致的，因为损失函数的梯度下降方向，就是残差方向。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/gbdt_algorithm1.png" alt="" /></p>

<p>意为gradient boost decision tree。又叫MART（Multiple Additive Regression Tree)</p>

<p><strong>kimmyzhang的ppt: Gradient Boosted Decision Tree</strong></p>

<p>Gradient Boosting Machine：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/gradient_boosting_machine.png" alt="" /></p>

<p>GB+DT+squared error loss：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/gbdt_squaredloss.png" alt="" /></p>

<p>更多请参考：<a href="http://www.360doc.com/content/14/1205/20/11230013_430680346.shtml">GBDT迭代决策树</a></p>

<h3 id="regularization">Regularization</h3>
<p>GBDT的常见regularization方法有：控制树的个数(即early stop)，对每一棵树控制其深度、叶子节点个数。</p>

<p>除此外，还可以在每次训练树时，对data和feature做subsampling。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/sampling_shrinkage.png" alt="" /></p>

<p>另一个常见的正则方法是Shrinkage。Shrinkage（缩减）的思想认为，每次走一小步逐渐逼近结果的效果，要比每次迈一大步很快逼近结果的方式更容易避免过拟合。即它不完全信任每一个棵残差树，它认为每棵树只学到了真理的一小部分，累加的时候只累加一小部分，通过多学几棵树弥补不足。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/shrinkage_algorithm.png" alt="" /></p>

<h3 id="gbdt-1">GBDT应用</h3>
<p>最近，gbdt模型在搜索排序里得到大量应用。除此外，GBDT还可以用来做特征选择和特征组合。</p>

<p>比较有代表性的是facebook的文章
<a href="http://quinonero.net/Publications/predicting-clicks-facebook.pdf">Practical Lessons from Predicting Clicks on Ads at Facebook</a>提到的方法，它利用GBDT+LR做CTR预估，取得不错的效果。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/facebook_gdbt_lr.png" alt="" /></p>

<p>如果想通过代码学习GBDT，可以参考code：<a href="https://github.com/zzbased/kaggle-2014-criteo">kaggle-2014-criteo my notes</a>，<a href="https://github.com/dmlc/xgboost">陈天奇的xgboost</a>。</p>

<h2 id="section-1">总结</h2>

<h3 id="aggregation-1">Aggregation方法总结</h3>
<p><strong>Blending Models</strong></p>

<p>blending: aggregate after getting diverse g_t</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/blending_models.png" alt="" /></p>

<p><strong>Aggregation-Learning Models</strong></p>

<p>learning: aggregate as well as getting diverse g_t</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/Aggregation-Learningmodels.png" alt="" /></p>

<p><strong>Aggregation of Aggregation Models</strong></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/Aggregation-of-Aggregation-Models.png" alt="" /></p>

<p><strong>为什么Aggregation方法是有效的？</strong></p>

<p>可以从两方面来看，其一通过Aggregation可以生成复杂的hypotheses，相当于做了feature transform；其二，生成的G(x)更加moderate，例如下图中PLA的uniform mix就是large-margin，相当于做了regularization。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/aggregation_works.png" alt="" /></p>

<h3 id="boosting-1">Boosting方法比较</h3>

<p>关于boosting方法的比较，上文中mlapp的图已经表达得比较明确了。这里再在公式上做一下细化。</p>

<p>Square and Absolute Error：
<img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/Square-and-Absolute-Error.png" alt="" /></p>

<p>Logistic Loss and LogitBoost：
<img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/Logistic-Loss-and-LogitBoost.png" alt="" /></p>

<p>Exponential Loss and Adaboost：
<img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/aggregation/Exponential-Loss-and-Adaboost.png" alt="" /></p>

<p>下面把一些常见方法的特点再加强阐述下。</p>

<ul>
  <li>
    <p>Adaboost：一种boost方法，它按分类对错，分配不同的weight，计算cost function时使用这些weight，从而让“错分的样本权重越来越大，使它们更被重视”。</p>
  </li>
  <li>
    <p>GBDT的核心在于：每一棵树学的是之前所有树的结论和残差。每一步的残差计算其实变相地增大了分错instance的权重，而已经分对的instance则都趋向于0。</p>
  </li>
  <li>
    <p>Bootstrap，它在每一步迭代时不改变模型本身，也不计算残差，而是从N个instance训练集中按一定概率重新抽取N个instance出来（单个instance可以被重复sample），对着这N个新的instance再训练一轮，由于数据集变了迭代模型训练结果也不一样。</p>

    <p>如果一个instance被前面分错的越厉害，它的概率就被设的越高，这样就能同样达到逐步关注被分错的instance，逐步完善的效果。这里是决策树给予不同样本不同权重的方法。</p>
  </li>
</ul>

<p>一篇不错的综述性文章：<a href="http://www.52cs.org/?p=383">集成学习：机器学习刀光剑影 之 屠龙刀</a></p>

<ul>
  <li>Bagging和boosting也是当今两大杀器RF（Random Forests）和GBDT（Gradient Boosting Decision Tree）之所以成功的主要秘诀。</li>
  <li>Bagging主要减小了variance，而Boosting主要减小了bias，而这种差异直接推动结合Bagging和Boosting的MultiBoosting的诞生。参考:Geoffrey I. Webb (2000). MultiBoosting: A Technique for Combining Boosting and Wagging. Machine Learning. Vol.40(No.2)</li>
  <li>LMT(Logistic Model Tree ) 应运而生，它把LR和DT嫁接在一起，实现了两者的优势互补。对比GBDT和DT会发现GBDT较DT有两点好处：1）GBDT本身是集成学习的一种算法，效果可能较DT好；2）GBDT中的DT一般是Regression Tree，所以预测出来的绝对值本身就有比较意义，而LR能很好利用这个值。这是个非常大的优势，尤其是用到广告竞价排序的场景上。</li>
  <li>关于Facebook的GBDT+LR方法，它出发点简单直接，效果也好。但这个朴素的做法之后，有很多可以从多个角度来分析的亮点：可以是简单的stacking，也可以认为LR实际上对GBDT的所有树做了选择集成，还可以GBDT学习了基，甚至可以认为最后的LR实际对树做了稀疏求解，做了平滑。</li>
</ul>

<h2 id="section-2">更多学习资料</h2>
<ul>
  <li><a href="http://suanfazu.com/t/gbdt-die-dai-jue-ce-shu-ru-men-jiao-cheng/135">Gbdt迭代决策树入门教程</a></li>
  <li><a href="http://www.schonlau.net/publication/05stata_boosting.pdf">Boosting Decision Tree入门教程</a></li>
  <li><a href="http://research.microsoft.com/pubs/132652/MSR-TR-2010-82.pdf">LambdaMART用于搜索排序入门教程</a></li>
  <li><a href="http://insidebigdata.com/2014/12/18/ask-data-scientist-ensemble-methods/">文章 Ask a Data Scientist: Ensemble Methods</a> “Ask a Data Scientist.”系列文章之Ensemble Methods，通俗程度可以和昨天介绍的Quora随机森林解释相媲美，但更为详尽，对常用Ensemble框架及其特点也进行了介绍，很好。</li>
  <li><a href="http://cvchina.net/post/107.html">决策树模型组合之随机森林与GBDT</a>
  <a href="http://www.cnblogs.com/LeftNotEasy/archive/2011/03/07/random-forest-and-gbdt.html">机器学习中的算法(1)-决策树模型组合之随机森林与GBDT link2</a>
  模型组合与决策树相关的算法比较多，这些算法最终的结果是生成N棵树，这样可以大大的减少单决策树带来的毛病，有点类似于三个臭皮匠等于一个诸葛亮的做法，虽然这几百棵决策树中的每一棵都很简单，但是他们组合起来确是很强大。</li>
  <li><a href="http://statweb.stanford.edu/~jhf/ftp/trebst.pdf">经典文章 Greedy function approximation : A Gradient Boosting Machine</a></li>
  <li><a href="https://github.com/tqchen/xgboost">xgboost - eXtreme Gradient Boosting (GBDT or GBRT) Library, also support distributed learning</a>
  并行实现推荐 @陈天奇怪 的xgboost，实际例子见@phunter_lau 最近的文章 http://t.cn/RhKAWac</li>
  <li><a href="http://machinelearning.wustl.edu/pmwiki.php/Main/Pgbrt">pGBRT: Parallel Gradient Boosted Regression Trees</a></li>
  <li><a href="http://bigdata.memect.com/?tag=GBDT">更多GBDT</a></li>
  <li><a href="http://www.hankcs.com/ml/decision-tree.html">决策树 用Python实现了决策树的ID3生成算法和C4.5生成算法</a></li>
  <li><a href="http://t.cn/RZBT6Ap">论文 Understanding Random Forests: From Theory to Practice</a>
Louppe, Gilles的博士论文，全面了解随机森林的好材料，推荐！pdf: http://t.cn/RZBTobH 云:http://t.cn/RZBTobT</li>
  <li><a href="http://blog.datadive.net/interpreting-random-forests/">Interpreting random forests</a></li>
  <li><a href="http://toutiao.com/a4055188882/">计算机视觉：随机森林算法在人体识别中的应用</a></li>
  <li>J. Friedman(1999). Greedy Function Approximation: A Gradient Boosting Machine.</li>
  <li>J. Friedman(1999). Stochastic Gradient Boosting.</li>
  <li>J. Friedman, T. Hastie, R. Tibshirani(2000). Additive Logistic Regression - A Statistical View of Boosting.</li>
  <li>T. Hastie, R. Tibshirani, J. Friedman(2008). Chapter 10 of The Elements of Statistical Learning(2e).</li>
</ul>


  </article>

</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">100的技术博客</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col  footer-col-1">
        <ul class="contact-list">
          <li>100的技术博客</li>
          <li><a href="mailto:zero_based@foxmail.com">zero_based@foxmail.com</a></li>
        </ul>
      </div>

      <div class="footer-col  footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/zzbased">
              <span class="icon  icon--github">
                <svg viewBox="0 0 16 16">
                  <path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
                </svg>
              </span>

              <span class="username">zzbased</span>
            </a>
          </li>
          

          
          <li>
            <a href="https://twitter.com/zero_based">
              <span class="icon  icon--twitter">
                <svg viewBox="0 0 16 16">
                  <path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                  c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
                </svg>
              </span>

              <span class="username">zero_based</span>
            </a>
          </li>
          
        </ul>
      </div>

      <div class="footer-col  footer-col-3">
        <p class="text">机器学习，自然语言处理，计算广告学，工作与生活，总结与温习
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
