<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>最优化方法</title>
  <meta name="description" content="">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://yourdomain.com/2015/01/15/%E6%9C%80%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95.html">
  <link rel="alternate" type="application/rss+xml" title="100的技术博客" href="http://yourdomain.com/feed.xml" />
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">100的技术博客</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
        
          
          <a class="page-link" href="/about/">About</a>
          
        
          
        
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">最优化方法</h1>
    <p class="post-meta">Jan 15, 2015</p>
  </header>

  <article class="post-content">
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<h1 id="section">最优化方法</h1>

<p>一般我们接触到的最优化分为两类：</p>

<ul>
  <li>无约束最优化</li>
  <li>有约束最优化</li>
</ul>

<h2 id="section-1">无约束最优化</h2>

<p>通常对于无约束最优化，首先要判断是否为凸函数。</p>

<p><a href="http://www.52nlp.cn/unconstrained-optimization-one">无约束最优化</a></p>

<p><a href="http://www.cnblogs.com/daniel-D/p/3377840.html">机器学习中导数最优化方法</a></p>

<p><a href="http://cseweb.ucsd.edu/~elkan/250B/logreg.pdf">最大似然、逻辑回归和随机梯度训练</a></p>

<h3 id="section-2">梯度下降法</h3>

<h3 id="section-3">牛顿法</h3>

<h3 id="section-4">拟牛顿法</h3>

<h3 id="section-5">共轭梯度法</h3>

<h2 id="section-6">有约束最优化</h2>

<p>一般采用拉格朗日方程，kkt，对偶问题求解。<a href="http://www.moozhi.com/topic/show/54a8a261c555c08b3d59d996">关于拉格朗日乘子法与KKT条件</a></p>

<p>譬如svm里，最大化几何间隔 max y(wx+b)/||w||</p>

<p><a href="http://blog.csdn.net/v_july_v/article/details/7624837">支持向量机</a></p>

<p>首先写出cost function：min [ 1/2*w^2 + max(0,1-y(wx+b)) ]</p>

<p>可以看出，这是一个有约束的问题，那么就可以用到”拉普拉斯+KKT+对偶”来求解了。</p>

<h2 id="section-7">最优化算法的并行化</h2>

<h3 id="logistic-regression">Logistic Regression</h3>

<p>这里主要以Logistic regression为例，讲一讲最优化算法的并行化实现。</p>

<p>先看一下Logistic regression的损失函数：</p>

<p>Logistic函数（或称为Sigmoid函数）为：
<script type="math/tex">sigmoid(z)=f(z)=\frac{1}{1+e^{-z}}</script></p>

<p>sigmoid函数的导数为：
<script type="math/tex">\nabla f(z)=f(z)(1-f(z))</script></p>

<p>对于线性回归来说，其分类函数为：
<script type="math/tex">h(x)=w_0+\sum_{i=1}^p{w_ix_i}=w^Tx</script>
其中x是特征向量，w是特征权重向量，p是特征向量维度。</p>

<p>逻辑回归本质上是一个被logistic函数归一化后的线性回归，即在特征到结果的映射中加入了一层sigmoid函数映射。相比于线性回归，模型输出取值范围为[0，1]，</p>

<p>如果y的取值是0或1，定义事件发生的条件概率为：
<script type="math/tex">P(y=1|x)=\pi(x)=\frac{1}{1+exp(-h(x))}</script></p>

<p>定义事件不发生的条件概率为：
<script type="math/tex">P(y=0|x)=1-P(y=1|x)=\frac{1}{1+exp(h(x))}</script></p>

<p>假设有n个观测样本，分别为：(\(x_1\),\(y_1\))，(\(x_2\),\(y_2\)) … (\(x_n\),\(y_n\))。</p>

<p>得到一个观测值(\(x_i\),\(y_i\))的概率为：
<script type="math/tex">P(y_i)=p_i^{y_i}(1-p_i)^{1-y_i}</script>  其中\(p_i=P(y_i=1|x_i)=\pi(x_i)\)</p>

<p>由于各项观测独立，所以它们的联合分布可以表示为各边际分布的乘积：
<script type="math/tex">l(w)=\prod_{i=1}^n{p_i^{y_i}(1-p_i)^{1-y_i}}</script></p>

<p>对上述函数取对数，根据最大似然估计，得到最优化目标为：
<script type="math/tex">\max{L(w)}=\max{log[l(w)]}=\max{\sum_{i=1}^n{y_i*log[\pi(x_i)] + (1-y_i)*log[1-\pi(x_i)]}}</script></p>

<p>对L(w)求导得：
<script type="math/tex">\nabla L(w) = \sum_{i=1}^n{(\pi(x_i)-y_i)x_i}</script></p>

<p>而如果y的取值是1或-1，则最优化目标为：</p>

<script type="math/tex; mode=display">\max{L(w)}=\max{\sum_{i=1}^n{-log[1+exp(-y_iw^Tx_i)]}}</script>

<p>加上正则项后则是：</p>

<script type="math/tex; mode=display">\min_w \frac{1}{2}||w||^2+C\sum_{i=1}^{n}log[1+exp(-y_iw^Tx_i)]</script>

<h3 id="lrmapreduce">LR的MapReduce并行</h3>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/lbfgs.png" alt="" /></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/lbfgs_two_loops.png" alt="" /></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/lbfgs_two_loops_vf.png" alt="" /></p>

<ul>
  <li><a href="http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/distributed-liblinear/">Distributed LIBLINEAR</a></li>
  <li><a href="http://nips.cc/Conferences/2014/Program/event.php?ID=4831">Large scale learning spotlights</a></li>
</ul>

<h2 id="loss-function">Loss Function</h2>

<p><a href="http://web.mit.edu/lrosasco/www/publications/loss.pdf">loss.pdf</a></p>

<p><a href="https://github.com/JohnLangford/vowpal_wabbit/wiki/Loss-functions">vowpal_wabbit Loss-functions </a></p>

<p><a href="http://en.wikipedia.org/wiki/Loss_function">Loss function wiki</a></p>

<p><a href="http://image.diku.dk/shark/sphinx_pages/build/html/rest_sources/tutorials/concepts/library_design/losses.html">shark loss function</a></p>

<p>adaboost，svm，lr三个算法的关系：</p>

<p>三种算法的分布对应exponential loss（指数损失函数），hinge loss，log loss（对数损失函数），无本质区别。应用凸上界取代0、1损失，即凸松弛技术。从组合优化到凸集优化问题。凸函数，比较容易计算极值点。</p>

<ul>
  <li>关于loss function，记录几点：
    <ul>
      <li>SVC &amp; SVR：对于SVC，由于hinge loss: max(0, 1-yw’x)，SVC会特别关注那些难分类的点（classify with least certainty; support vectors, i.e., sv）；对于SVR，由于epsilon loss: max(0, |y-w’x|-e)，SVR类似的会关注那些residual比较大的点（”outlier”; prediction with least certainty too; sv）</li>
      <li>SVM和ANN 1) SVM的hinge loss func: max(0, 1 - y’x), 当“激活量” y’x 超过阈值1，那么就不计损失即损失为零 2) Deep Sparse Rectifier的rectifier activation func: max(0, x), 看图它在零点处不可导带来了稀疏性，可大量约简神经网络参数规模 3) loss function + penalty regularization</li>
      <li>SVM &amp; Boosting：SVM用loss func来剔除了那些容易的点，实际相当于给样本加了权重0，使得模型更加关注难预测的点；Boosting每次迭代后调整权重：降低容易点的权重，增加难预测点的权重，也使得模型更关注难预测的点</li>
    </ul>
  </li>
  <li>
    <p><a href="http://www.cs.nyu.edu/~yann/talks/lecun-20071207-nonconvex.pdf">Who is afraid of non-convex loss functions?</a></p>

    <p>Yann LeCun在2007年的讲义。目前大部分loss function都是凸函数，譬如lr，svms，exponential-family graphical models，凸函数一个很优秀的性质，但有些时候却是限制，坚持凸性将增大模型规模或者求解最优化问题方法的复杂程度。</p>

    <p>Deep architectures势必将带来非凸的loss function。但是(1)采用一个合适的模型架构(即便将带来loss function非凸)比坚持凸性更加重要；(2) 即使是对于浅层模型，例如SVM，使用非凸的loss function实际上可以带来准确率和性能的提升。</p>

    <p>对于神经网络，conjugate gradient, BFGS, LM-BFGS没有stochastic gradient好用；对于SVM，“batch” quadratic方法没有SMO好用，而SMO又没有on-line methods好用；对于CRF，Iterative scaling没有stochastic gradient好用；虽说stochastic gradient没有很多的理论保证，但经验证明是不错的选择。</p>
  </li>
</ul>

<h2 id="section-8">最优化算法参考资料</h2>

<ul>
  <li>
    <p><a href="http://www.erogol.com/comparison-sgd-vs-momentum-vs-rmsprop-vs-momentumrmsprop/">几点改进随机梯度下降(SGD)过程的tricks</a>
  对梯度本身的探究也是特别有价值的，这里的优化方法，就是让梯度的update更平滑和稳定，记录之前在这个方向上的梯度，做平滑，并防止不同batch带来的突变是有价值的</p>
  </li>
  <li>
    <p><a href="http://stanford.edu/~boyd/cvxbook/">凸优化，convex optimization</a></p>
  </li>
  <li>
    <p>比较全面的优化教材 http://t.cn/Rwx3S6S @黄萱菁 发起的讨论 @Copper_PKU: Optimization for Machine Learning（Wright） @夏睿 Numerical Optimization（Nocedal）@wzxhome 推荐Nesterov @程龚_NJU 推荐Dingzhu Du的 @debiandsc 推荐Bertsekas和Ghaoui的</p>

    <p>好像除了这本大家谈的比较多的只有Introductory Lectures on Convex Optimization by Yurii Nesterov http://t.cn/Rwyz6Nb 其他的我也没印象。</p>

    <p>Nocedal 的Numerical Optimization，L-BFGS 算法提出者 http://t.cn/RwUQ40Q</p>

    <p><a href="http://enpub.fulton.asu.edu/cseml/Fall2008_ConvOpt/book/Intro-nl.pdf">Introductory Lectures on Convex Programming</a></p>

    <p><a href="http://www.bioinfo.org.cn/~wangchao/maa/Numerical_Optimization.pdf">Numerical_Optimization</a></p>

    <p><a href="http://mitpress.mit.edu/books/optimization-machine-learning">Optimization for Machine Learning</a></p>

    <p><a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">Convex Optimization</a></p>

    <p><a href="http://www2.isye.gatech.edu/~nemirovs/Lect_EMCO.pdf">EFFICIENT METHODS IN CONVEX PROGRAMMING</a></p>
  </li>
</ul>

  </article>

</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">100的技术博客</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col  footer-col-1">
        <ul class="contact-list">
          <li>100的技术博客</li>
          <li><a href="mailto:zero_based@foxmail.com">zero_based@foxmail.com</a></li>
        </ul>
      </div>

      <div class="footer-col  footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/zzbased">
              <span class="icon  icon--github">
                <svg viewBox="0 0 16 16">
                  <path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
                </svg>
              </span>

              <span class="username">zzbased</span>
            </a>
          </li>
          

          
          <li>
            <a href="https://twitter.com/zero_based">
              <span class="icon  icon--twitter">
                <svg viewBox="0 0 16 16">
                  <path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                  c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
                </svg>
              </span>

              <span class="username">zero_based</span>
            </a>
          </li>
          
        </ul>
      </div>

      <div class="footer-col  footer-col-3">
        <p class="text">机器学习，自然语言处理，计算广告学，工作与生活，总结与温习
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
