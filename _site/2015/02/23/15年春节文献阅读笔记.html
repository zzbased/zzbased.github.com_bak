<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>15年春节文献阅读笔记</title>
  <meta name="description" content="深度学习  深度学习整体教程">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://yourdomain.com/2015/02/23/15%E5%B9%B4%E6%98%A5%E8%8A%82%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0.html">
  <link rel="alternate" type="application/rss+xml" title="100的技术博客" href="http://yourdomain.com/feed.xml" />
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">100的技术博客</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
        
          
          <a class="page-link" href="/about/">About</a>
          
        
          
        
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">15年春节文献阅读笔记</h1>
    <p class="post-meta">Feb 23, 2015</p>
  </header>

  <article class="post-content">
    <h3 id="section">深度学习</h3>
<ul>
  <li>
    <p>深度学习整体教程</p>

    <ul>
      <li><a href="http://dataunion.org/?p=5395">深度学习：CNN的反向求导及练习</a></li>
      <li><a href="http://quantombone.blogspot.com/2015/01/from-feature-descriptors-to-deep.html">From feature descriptors to deep learning</a></li>
      <li><a href="http://simonwinder.com/2015/01/what-is-deep-learning/">What is Deep Learning?</a></li>
      <li><a href="http://markus.com/deep-learning-101/">Deep Learning 101</a></li>
      <li><a href="http://www.toptal.com/machine-learning/an-introduction-to-deep-learning-from-perceptrons-to-deep-networks">A Deep Learning Tutorial: From Perceptrons to Deep Networks</a></li>
      <li><a href="http://www.iro.umontreal.ca/~bengioy/dlbook/">deep learning book. by Yoshua Bengio</a></li>
      <li>
        <p><a href="http://deeplearning.net/tutorial/deeplearning.pdf">Deep Learning Tutorial. LISA lab, University of Montreal</a>  <a href="http://deeplearning.net/software_links/">software</a>
  基于Theano的深度学习教程，内容很新，包括了多层感知器，卷积神经网络，auto encoder，RBM，Deep Belief Networks，Monte-carlo sampling，循环神经网络，LSTM，RNN-RBM，Miscellaneous等，值得学习。</p>
      </li>
      <li><a href="http://www.cs.toronto.edu/~tijmen/csc321/">Introduction to Neural Networks and Machine Learning by hinton</a> <a href="http://www.cs.toronto.edu/~tijmen/csc321/lecture_notes.shtml">Lecture notes</a></li>
      <li>
        <p><a href="http://blog.csdn.net/zouxy09/article/details/8775360">深度学习-笔记整理系列1-8</a> <a href="http://blog.csdn.net/zouxy09/article/details/9993371">link2</a> 使用自下向上非监督学习（就是从底层开始，一层一层的往顶层训练）；自顶向下的监督学习（就是通过带标签的数据去训练，误差自顶向下传输，对网络进行微调）。还介绍了Autoencoder，SparseCoding，Restricted Boltzmann Machine，Deep BeliefNetworks，CNN等模型。</p>
      </li>
      <li><a href="http://weibo.com/p/1001603799166017998138">谷歌科学家、Hinton亲传弟子Ilya Sutskever的深度学习综述及实际建议</a> 比较喜欢其中关于tricks的建议：包括data, preprocessing, minibatches, gradient normalization, learning rate, weight initialization, data augmentation, dropout和ensemble。</li>
      <li>
        <p><a href="http://www.ee.ucl.ac.uk/sahd2014/resources/LeCun.pdf">LeCun：The Unreasonable Effectiveness of Deep Learning</a> LeCun做的300+页的深度学习slides，太棒了！毋需多做介绍，单看标题、作者应该就能作出判断——看看看，必须的！</p>
      </li>
      <li>
        <p><a href="http://colah.github.io/posts/2015-01-Visualizing-Representations/">Visualizing Representations: Deep Learning and Human Beings</a> 利用深度学习和维数约减，可以对整个Wikipedia进行可视化，文中结合Wikipedia训练得出的例子，全面介绍了深度学习、词向量、段落向量、翻译模型以及深度学习可视化方面的知识，理论结合实践，实属不可多得的好文。</p>
      </li>
      <li>
        <p><a href="http://www.bammf.org">Bay Area Multimedia Forum</a> 邓力，贾扬清，Ronan Collobert, Richard Socher 讲用深度学习处理语音、文本和图像。有slides有视频。</p>
      </li>
      <li><a href="http://t.cn/RZ8Sqe6">博文“A Brief Overview of Deep Learning”</a> 有见解有福利。一些技术总结得不错，例如Practice Advice，有很多干货，谁用谁知道…… 文后还有Bengio的点评及与网友的互动讨论。</li>
      <li><a href="">Neural network with numpy</a> Python下(只)用numpy写神经网络，不错的开始 <a href="https://github.com/FlorianMuellerklein/Machine-Learning/blob/master/BackPropagationNN.py">github code</a></li>
    </ul>
  </li>
  <li>
    <p>ImageNet classification</p>

    <ul>
      <li><a href="http://arxiv.org/pdf/1502.03167v1.pdf">Googles breakthrough paper shows 10*faster neural nets, and beats a human</a> Google的最新论文，用”batch normalization”在ImageNet上得到4.82%( top-5 error)，训练速度也大大加快。</li>
      <li><a href="http://arxiv.org/abs/1502.01852">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a> 微软所创建的基于深度卷积神经网络系统首次在ImageNet图像分类上超越人类，实现4.94% top-5 test error。</li>
    </ul>
  </li>
  <li>Image Caption Generation
    <ul>
      <li>
        <p><a href="http://arxiv.org/abs/1502.03044">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</a> 基于视觉焦点用LSTM自动生成图像内容描述。
  来自Yoshua Bengio教授团队（22 pages，8页正文+n多附图），文中报道的结果比之前Microsoft、Google的结果更好。</p>
      </li>
      <li>
        <p><a href="https://pdollar.wordpress.com/2015/01/21/image-captioning/">Image Captioning</a>  来自于微软的一篇博文，介绍了image caption的最近进展。图像标题生成小综述，文中引用了另一篇进展综述性文章《Rapid Progress in Automatic Image Captioning》http://t.cn/Rzzk2H3 ，列举出最有影响和代表性的进展和成果(论文)，并从数据集、验证(评价)和下一步怎么走三方面进行了讨论，观点很有代表性</p>

        <p><a href="http://t.cn/RZdBZEe">Top Microsoft Machine Learning Posts of 2014</a> 微软14年最佳博文：图像自动描述生成的飞速进展、机器学习欢乐多、Azule ML为用户带来变革、机器学习与文本分析、微软机器学习20年、Vowpal Wabbit快速学习、什么是机器学习、NIPS14机器学习趋势、机器学习与机器视觉等</p>
      </li>
    </ul>
  </li>
  <li>RNN&amp; LSTM
    <ul>
      <li><a href="http://xxx.tau.ac.il/abs/1502.02367">Gated Feedback Recurrent Neural Networks</a>  又有人设计新的RNN了，这回Cho和Bengio都在．这回介绍的GF-RNN说是能比以前Deep RNN的都好。明摆着说LSTM嘛。</li>
      <li><a href="http://t.cn/Rww4fbV">论文 Scaling Recurrent Neural Network Language Models》(2015) W Williams, N Prasad, D Mrva</a> 讨论如何使用GPU训练大型RNN，以及#RNN##语言模型#(RNNLM)在进行扩展时模型大小、训练集规模和运算开销方面的问题；使用维基和新闻语料训练的大规模RNNLM效果明显。</li>
      <li><a href="http://t.cn/RZNfoLt">Passage：用RNN做文本分析的python库</a></li>
      <li><a href="http://t.cn/RZKo42W">深入讨论RNN</a> 非常好的讨论递归神经网络的文章，覆盖了RNN的概念、原理、训练及优化等各个方面内容，强烈推荐！本文作者Nikhil Buduma，也是前两天推荐的《Deep Learning in a Nutshell》的作者，赞！</li>
      <li>深度RNN/LSTM用于结构化学习 0)序列标注<a href="http://t.cn/RZZs9Io">Connectionist Temporal Classification ICML06</a> 1)机器翻译<a href="http://t.cn/RZZs9Jk">Sequence to Sequence NIPS14</a> 2)成分句法<a href="http://t.cn/RZZs9Ia">GRAMMAR AS FOREIGN LANGUAGE</a> 再次用到窃取果实distilling</li>
    </ul>
  </li>
  <li>CNN
    <ul>
      <li>
        <p>Character-based CNN。<a href="http://arxiv.org/abs/1502.01710">论文 Text Understanding from Scratch 2015 Xiang Zhang, Yann LeCun</a>  深度学习在NLP领域最新进展！使用temporal ConvNets对大规模文本语料进行学习，在本体分类、情感分析、文本分类任务中取得了“astonishing performance”，不依赖任何语言知识，中英文均适用。必读！</p>
      </li>
      <li>
        <p><a href="http://arxiv.org/abs/1404.2188">论文 A Convolutional Neural Network for Modelling Sentences. Nal Kalchbrenner等</a> 用动态卷积神经网络(DCNN)对句子进行语义建模，该方法不依赖解析树也不受语种限制，效果也很明显，推荐学习。其项目主页上http://t.cn/RZd9HOE 提供了源码(Matlab) 云:http://t.cn/RZdCx1o</p>
      </li>
    </ul>
  </li>
  <li>Word2vec
    <ul>
      <li>[问答]《What are some interesting Word2Vec results?》http://t.cn/RZrOfYp Quora上的主题，讨论Word2Vec的有趣应用，Omer Levy提到了他在CoNLL2014最佳论文里的分析结果和新方法（稍后单独推荐），Daniel Hammack给出了找特异词的小应用并提供了(Python)代码http://t.cn/zQgLQ20</li>
      <li>Ensemble of Generative and Discriminative Techniques for Sentiment Analysis of Movie Reviews。<a href="https://github.com/mesnilgr/iclr15">code</a></li>
      <li>隐含主题模型LDA的学习过程可为文档每个词分配隐含主题，我组本科生刘扬同学利用LDA为词汇提供的补充信息，提出topical word embeddings，在词汇相似度计算和文本分类上得到一些有趣的结果。<a href="https://github.com/largelymfs/topical_word_embeddings">github code</a>，<a href="http://nlp.csai.tsinghua.edu.cn/~lzy/publications/aaai2015_twe.pdf">论文Topical Word Embeddings</a></li>
    </ul>
  </li>
</ul>

<h3 id="rtb">RTB</h3>
<ul>
  <li>
    <p><a href="http://tutorial.computational-advertising.org/images/WSDM2015.pdf">Real-Time Bidding</a></p>
  </li>
  <li>
    <p>在RTB广告出价过程中，两个重点因素是：1. 广告展示的效益(点击率/转化率)；2. 展示花销。以这两项因素为输入，我们的框架能优化出最优的RTB出价策略。论文：http://t.cn/Rvb5K9d，实验基于@品友互动 RTB数据集http://t.cn/RP3pPQO。</p>
  </li>
</ul>

<h3 id="section-1">计算广告</h3>

<ul>
  <li>
    <p><a href="">Practical Lessons from Predicting Clicks on Ads at Facebook</a></p>

    <ul>
      <li>
        <p><a href="https://breezedeus.github.io/2014/11/19/breezedeus-feature-mining-gbdt.html">利用GBDT模型构造新特征</a></p>
      </li>
      <li>
        <p>特征决定了所有算法效果的上限，而不同的算法只是离这个上限的距离不同而已。</p>
      </li>
      <li>
        <p>这篇论文的思想：先用已有特征训练GBDT模型，然后利用GBDT模型学习到的树来构造新特征，最后把这些新特征加入原有特征一起训练模型。构造的新特征向量是取值0/1的，向量的每个元素对应于GBDT模型中树的叶子结点。当一个样本点通过某棵树最终落在这棵树的一个叶子结点上，那么在新特征向量中这个叶子结点对应的元素值为1，而这棵树的其他叶子结点对应的元素值为0。新特征向量的长度等于GBDT模型里所有树包含的叶子结点数之和。</p>
      </li>
      <li>
        <p>已经有人利用这种方法赢得了Kaggle一个CTR预估比赛的冠军，代码可见https://github.com/guestwalk/kaggle-2014-criteo，里面有这种方法的具体实现。</p>
      </li>
    </ul>
  </li>
  <li>特征处理
    <ul>
      <li>在logistic regression问题中，最基本的问题就是特征处理。</li>
      <li><a href="https://breezedeus.github.io/2014/11/15/breezedeus-feature-processing.html">特征处理入门 by BreezeDeus</a></li>
      <li><a href="http://www.flickering.cn/ads/2014/08/转化率预估-4特征选择－简介/">特征选择 by ubiwang</a></li>
    </ul>
  </li>
  <li>
    <p><a href="https://breezedeus.github.io/2014/11/20/breezedeus-feature-hashing.html">特征hash法</a></p>

    <ul>
      <li>
        <p>特征哈希法的目标是把原始的高维特征向量压缩成较低维特征向量，且尽量不损失原始特征的表达能力。</p>
      </li>
      <li>
        <p>特征降维方法：聚类，PCA，hashing</p>
      </li>
      <li>参考文献：
        <ul>
          <li>Kilian Weinberger et al. Feature Hashing for Large Scale Multitask Learning,2010.  or Feature hashing for large scale multitask learning</li>
          <li>Joshua Attenberg et al. Collaborative Email-Spam Filtering with the Hashing-Trick, 2009.</li>
          <li>Alekh Agarwal, Oliveier Chapelle, Miroslav Dud ́ık, and John Langford. A Reliable Effective Terascale Linear Learning System. Journal of Machine Learning Research, 15:1111–1133, 2014.</li>
          <li><a href="">Large-scale L-BFGS using MapReduce</a> 该论文讲述了运用了hashing技术后，AUC的变化。</li>
        </ul>
      </li>
      <li>
        <p>特征hash法用于 多任务学习(multitask learning)</p>
      </li>
      <li>大数据机器学习专家John Langford总结了一下关于使用hashing trick来处理大数据的重要文章：http://t.cn/RzbMxm5 值得一看。</li>
    </ul>
  </li>
  <li>特征选择
    <ul>
      <li><a href="http://blog.datadive.net/selecting-good-features-part-i-univariate-selection/">Selecting good features</a> 《Selecting good features》特征工程系列文章：Part1.单变量选取http://t.cn/Rw4B2vO Part2.线性模型和正则化http://t.cn/Rw4Blqc Part3.随机森林http://t.cn/Rw4BEAQ Part4.稳定性选择法、递归特征排除法(RFE)及综合比较http://t.cn/Rw4rndV 内容很赞，还有Python代码示例，强烈推荐！</li>
    </ul>
  </li>
  <li>CTR预估
    <ul>
      <li>竞价搜索广告中的点击率预估法: 特征和模型 1）经典LR[WWW07] http://t.cn/zT8FYsf 及L1正则版[ICML07] http://t.cn/RZOSmEl 2）高质量特征+组合LR和BDT[ADKDD14] http://t.cn/RZOSmET 3）加LDA话题特征[ACM-TIST15] http://t.cn/RZOinjO 4）组合ANN和BDT[ICLR15] http://t.cn/RZOSmEY</li>
      <li><a href="http://blog.sina.com.cn/s/blog_6cb8e53d0101oetv.html">并行逻辑回归</a>： 逻辑回归（Logistic Regression，简称LR）是机器学习中十分常用的一种分类算法，在互联网领域得到了广泛的应用，无论是在广告系统中进行CTR预估，推荐系统中的预估转换率，反垃…文字版» http://t.cn/8FpoAyz （新浪长微博» http://t.cn/zOXAaic）</li>
      <li>发表了博文《CTR预估模型，机器学习初探水》http://t.cn/R7tJ59w损失函数；CRT模型；数据模型切分；结果推荐；链路距离计算；推http://t.cn/R7tJ5LU</li>
      <li>NTU关于Criteo CTR Challenge的winning solution出来了：http://t.cn/RhkyR3B，有代码有slide。解决方案是用GBDT学intermediate feature，再配合raw feature，用FM训练CTR模型。其中GBDT学特征是参考了Facebook ADKDD2014文章：http://t.cn/RPrqyRQ</li>
      <li>Evaluation。Precision, Recall, AUCs and ROCs 准确率、召回率、AUC(曲线下面积)和ROC(受试者工作特征曲线)，文中就很多Kaggle竞赛分类任务最终多个评判标准间的关系进行了讨论 http://t.cn/RZYd75X</li>
    </ul>
  </li>
  <li><a href="http://weibo.com/p/1001603803645836675771">汤兴-爱奇艺大脑—视频进化</a></li>
</ul>

<h3 id="sampling--statistic">Sampling &amp; Statistic</h3>
<ul>
  <li>
    <p><a href="http://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/">Probabilistic Programming &amp; Bayesian Methods for Hackers</a></p>

    <p>黑客们的概率编程和贝叶斯方法书，电子版，有代码，介绍为主，减少推导。同时详细介绍了使用PyMC进行MCMC编程的细节。</p>
  </li>
  <li>
    <p><a href="http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=203307140&amp;idx=1&amp;sn=83cf3093d9cd0f6bca8981eae0cecb9e#rd">统计学公开课大盘点</a></p>

    <p>很多统计学课程，看来习题可以从这里面获取了。推荐可汗学院的“Probability and Statistics（概率与统计)”</p>
  </li>
  <li>随机采样方法整理与讲解（MCMC、Gibbs Sampling等） - Bin的专栏 - 博客园 http://t.cn/RZDLdr4</li>
  <li>
    <p>[可视化]《Markov Chains》http://t.cn/RZBE1ME 马尔可夫链的交互可视化解释，和以前发过超赞的那个特征值特征向量的可视化同一系列</p>
  </li>
  <li>
    <p>[图书]《Forecasting, Principles and Practice》http://t.cn/zR4ZGMM 全面介绍预测模型和算法，对证券、电信、交通等时序信号的预测分析都很有用，书中例子为R语言，在线免费阅读</p>
  </li>
  <li>How To Implement These 5 Powerful Probability Distributions In 用python实现5种常用的概率分布：二项式分布、泊松分布、正态分布、Beta分布和指数分布 http://t.cn/RZWta4n</li>
  <li>[文章]《Getting started in data science: My thoughts》http://t.cn/RviewMN 数据科学入门指南——作者从数学、统计学、实验和因果推理、机器学习、软件选择和实践经验积累各方面谈了自己的入门建议。和其他入门指南类文章不同的是，本文没有盲目看好MOOC之类的自学途径，建议都很中肯，推荐阅读</li>
</ul>

<h3 id="nlp">NLP</h3>

<ul>
  <li><a href="https://github.com/arnicas/NLP-in-Python">NLP-in-Python —— 介绍Python下NLP的系列ipn，内容覆盖Reading in Files、Tokenizing, Stemming, POS、Wordclouds、TF-IDF, Clustering, Pattern、Naive Bayes Classification、Naive Bayes in Scikit-Learn</a></li>
</ul>

<h3 id="section-2">最优化算法</h3>

<ul>
  <li>
    <p><a href="http://www.erogol.com/comparison-sgd-vs-momentum-vs-rmsprop-vs-momentumrmsprop/">几点改进随机梯度下降(SGD)过程的tricks</a>
  对梯度本身的探究也是特别有价值的，这里的优化方法，就是让梯度的update更平滑和稳定，记录之前在这个方向上的梯度，做平滑，并防止不同batch带来的突变是有价值的</p>
  </li>
  <li>
    <p><a href="http://stanford.edu/~boyd/cvxbook/">凸优化，convex optimization</a></p>
  </li>
  <li>关于loss function，记录几点：
    <ul>
      <li>SVC &amp; SVR：对于SVC，由于hinge loss: max(0, 1-yw’x)，SVC会特别关注那些难分类的点（classify with least certainty; support vectors, i.e., sv）；对于SVR，由于epsilon loss: max(0, |y-w’x|-e)，SVR类似的会关注那些residual比较大的点（”outlier”; prediction with least certainty too; sv）</li>
      <li>SVM和ANN 1) SVM的hinge loss func: max(0, 1 - y’x), 当“激活量” y’x 超过阈值1，那么就不计损失即损失为零 2) Deep Sparse Rectifier的rectifier activation func: max(0, x), 看图它在零点处不可导带来了稀疏性，可大量约简神经网络参数规模 3) loss function + penalty regularization</li>
      <li>SVM &amp; Boosting：SVM用loss func来剔除了那些容易的点，实际相当于给样本加了权重0，使得模型更加关注难预测的点；Boosting每次迭代后调整权重：降低容易点的权重，增加难预测点的权重，也使得模型更关注难预测的点</li>
    </ul>
  </li>
  <li><a href="http://www.cs.nyu.edu/~yann/talks/lecun-20071207-nonconvex.pdf">Who is afraid of non-convex loss functions?</a></li>
</ul>

<h3 id="section-3">其他机器学习问题</h3>

<ul>
  <li>
    <p><a href="http://blog.csdn.net/u013854886/article/details/38425499">Multi-task learning（多任务学习）简介</a></p>

    <p>目前多任务学习方法大致可以总结为两类，一是不同任务之间共享相同的参数（common parameter），二是挖掘不同任务之间隐藏的共有数据特征（latent feature）</p>
  </li>
  <li>
    <p>迁移学习</p>

    <ul>
      <li><a href="http://blog.csdn.net/jwh_bupt/article/details/8901261">迁移学习与自我学习</a></li>
      <li><a href="http://blog.csdn.net/jwh_bupt/article/details/9276165">迁移学习的相关概念</a></li>
      <li>
        <p><a href="http://www.zhizhihu.com/html/y2011/2902.html">阅读笔记：Boosting for Transfer learning</a></p>
      </li>
      <li>
        <p>当前只有少量新的标记的数据，但是有大量旧的已标记的数据（甚至是其他类别的有效数据），这时通过挑选这些旧数据中的有效的数据，加入到当前的训练数据中，训练新的模型。</p>
      </li>
      <li>
        <p>迁移学习的目标是将从一个环境中学到的知识用来帮助新环境中的学习任务。</p>
      </li>
      <li>
        <p>例子：当教会了电脑学习区分大象和犀牛的能力后，电脑利用这一本领更快或更准确地来学习如何区分飞机和鸟。</p>
      </li>
      <li>参考文献：
        <ul>
          <li>Boosting for transfer learning</li>
          <li>Self-taught learning: transfer learning from unlabeled data</li>
          <li>A survey on transfer learning. IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 22, NO. 10, OCTOBER 2010</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>机器学习通用问题
    <ul>
      <li><a href="http://www.valleytalk.org/wp-content/uploads/2012/11/机器学习那些事.pdf">A few useful things to know about machine learning</a></li>
      <li><a href="https://breezedeus.github.io/2012/10/30/breezedeus-things-about-ml.html">机器学习的12堂课</a></li>
      <li><a href="http://www.douban.com/note/413022836/">机器学习常见的错误</a></li>
      <li>学习=表示(representation)+评价(evaluation)+优化(optimization)；表示即为学习器的假设空间。所选的learner应该具有某种表达形式，通过learner最终获得的模型应该具有给定的这种特征(如分割面为超平面)。选定了这种特性，就相当于为learner选定了一组候选classifier。</li>
    </ul>

    <p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/machine_learning.png" alt="" /></p>

    <ul>
      <li>泛化很重要。将泛化作为目标给机器学习带来一个有趣的结果。与其他大部分优化问题不同,机器学习无法获得希望优化的那个函数! 我们不得不用训练误差来代替测试误差。</li>
      <li>
        <p>过拟合。解决过拟合的思路：交叉验证，正则项，在决定是否增加新的结构时进行诸如卡方测试等统计显著性检验 (statistical significance test), 用来决定类别分布是否会因为增 加这个结构而不同。
  <img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/bias_and_variance.png" alt="" /></p>
      </li>
      <li>直觉不适用于高维空间。</li>
      <li>要学习很多模型, 而不仅仅是一个。model emsemble。</li>
      <li>简单并不意味着准确。应当先选择简单假设,这是因为简单本身就是一个优点,而不是因为所假设的与准确率有什么联系。</li>
      <li>相关并不意味着因果。</li>
      <li>可表示并不代表可学习。</li>
      <li>更多的数据打败更聪明的算法。</li>
      <li>特征工程是关键。</li>
      <li>理论上的保证不像你所想的那样。</li>
    </ul>
  </li>
  <li><a href="http://xilinx.eetrend.com/article/8319">机器学习综述——机器学习理论基础与发展脉络</a>
    <ul>
      <li>如果一个系统能够通过执行某个过程改进它的性能，这就是学习。从大量无序的信息到简洁有序的知识
        <ul>
          <li>从有限观察概括特定问题世界模型的机器学习</li>
          <li>发现观测数据中暗含的各种关系的数据分析</li>
          <li>从观测数据挖掘有用知识的数据挖掘</li>
        </ul>
      </li>
      <li>
        <p>模型假设+模型选择+学习算法；常用的损失函数包括0-1损失、平方误差损失、绝对损失、对数损失等等。</p>

        <p>统计机器学习方法的三个问题都是非常值得研究的，对于模型假设这个问题，如果模型都选择错误，无论后面如何选择模型，也都难以反映数据集的正确分布。因此，首先需要选择对模型做出正确假设，如何选择模型的假设空间是一个学问，除掉交叉验证的方法之外还有不少其他方法。模型选择的关键在于如何设计损失函数，而损失函数通常包括损失项和正则化项，不同的模型选择策略通常选出的模型也非常不同，从而导致模型的预测效果也大大不同。学习算法比较定式，不同的学习算法不仅学习的效率不同，而且学习出来的效果也不一样。</p>
      </li>
      <li>
        <p>SVM方法是通过一个非线性映射p，把样本空间映射到一个高维乃至无穷维的特征空间中（Hilber空间），使得在原来的样本空间中非线性可分的问题转化为在特征空间中的线性可分的问题。分类，回归等问题，很可能在低维样本空间无法线性处理的样本集，在高维特征空间中却可以通过一个线性超平面实现线性划分（或回归）。一般的升维都会带来计算的复杂化，SVM方法巧妙地解决了这个难题：应用核函数的展开定理，就不需要知道非线性映射的显式表达式；由于是在高维特征空间中建立线性学习机，所以与线性模型相比，不但几乎不增加计算的复杂性，而且在某种程度上避免了“维数灾难”．这一切要归功于核函数的展开和计算理论。</p>

        <p>SVM 的最终决策函数只由少数的支持向量所确定,计算的复杂性取决于支持向量的数目,而不是样本空间的维数,这在某种意义上避免了“维数灾难”</p>
      </li>
      <li>增强机器学习，无监督学习，有监督学习，半监督学习，自我学习，迁移学习，多任务学习等等。这些学习方法的基本原理都要理解。</li>
    </ul>
  </li>
  <li>
    <p><a href="http://t.cn/Rw4Wz73">An Introduction to Supervised Machine Learning and Pattern Classification: The Big Picture</a> 很不错的监督机器学习(重点是模式分类)介绍资料 云:http://t.cn/Rw4WIrh</p>
  </li>
  <li>
    <p><a href="http://ask.julyedu.com/question/64">邹博讲EM、GMM</a> 被称赞为：“最清楚的一次GMM”，“老师讲的太好，太多人想听”等等。</p>
  </li>
  <li>
    <p><a href="https://github.com/mrlyk423/relation_extraction">关系提取 relation_extraction</a>。ransE通过h + r = t的目标学习知识图谱表示，效果引人关注。最近我组林衍凯同学针对TransE对1-N、N-1类型关系效果不佳的问题，提出TransR将实体映射到不同关系空间中构建优化目标，效果最高比TransE提升近20%。论文 http://t.cn/RZSpha8 ，实现TransE、TransH和TransR全部开源：http://t.cn/RZe9of4</p>
  </li>
  <li>
    <p>[文章]《Robust Machine Learning》http://t.cn/RZkX55C 针对机器学习算法如何在面对错误样本时更健壮的问题，作者讨论了几种角度的方案：用(相对)阈值过滤；针对性的特征选择；对每个特征建立查处模型，训练、验证、纠偏。联系这两天热议的深度学习对抗样本问题的解决，差不多也是这些角度。</p>
  </li>
  <li>
    <p>[IPN]《Advice for applying Machine Learning》http://t.cn/RZreqPu 为机器学习的使用提供了一些建议，包括可视化分析方法、机器学习算法的选取、过拟合及欠拟合的处理、大数据集的处理、各种损失函数的比较等，很实用。Andrew Ng的《Advice for applying Machine Learning》http://t.cn/RZdaera</p>
  </li>
  <li>
    <p>[文章]《All Models of Learning have Flaws》http://t.cn/zlvnrVW 机器学习模型的缺陷(抱怨)，讨论实际使用时各种模型的限制条件，很实用，推荐阅读</p>
  </li>
  <li>
    <p>[文章]《MACHINE LEARNING WORK-FLOW》机器学习工作流系列文章：1.综述:http://t.cn/R7ZP8C5 2.数据预处理:http://t.cn/RZrS1GN 3.特征抽取:http://t.cn/R7aQPbR 4.完整性检查和数据分割:http://t.cn/R7eAXgy 5.特征预处理:http://t.cn/RzLBuRk 目前就写到Part5，感兴趣可以持续关注该博主</p>
  </li>
  <li>
    <p>[课程]《Machine Learning - A introductory course on machine learning》http://t.cn/RZsmDSu USC的Fei Sha和Yan Liu的机器学习课程，讲义组织得不错，知识覆盖也比较全面，可看作机器学习的速成参考，推荐学习 云:http://t.cn/RZsuLXo</p>
  </li>
  <li>【机器学习入门教材】Max Welling教授在UCI讲授《机器学习》多年，他认为许多教材堆砌数学公式，缺乏对这些公式的解释。他2011年写了一本入门书，公式不多，但是对许多概念提供了符合直觉的解释。Max Welling的机器学习入门书免费下载：http://t.cn/RZiZ7BH 工业界人士、本科生、非CS专业学生均适合。</li>
  <li>[文章]《Machine Learning Overview》机器学习概述系列文章：Part1:http://t.cn/RZ1yEYg Part2.Logistic Regression http://t.cn/RZ1yetX Part3.Decision Trees and Random Forests http://t.cn/RZ1U5Zg Part4还没写，感兴趣可继续关注</li>
  <li>[幻灯]《Tutorial Slides by Andrew W. Moore》http://t.cn/RZmOG7c 由Google计算机科学家、曾任CMU教授的Andrew W. Moore整理的系列教程，内容覆盖统计数据挖掘的各个方面，包括概率基础、统计数据分析基础、经典机器学习算法和数据挖掘算法，是很好的学习材料。简短版目录列表:http://t.cn/zHiLcXa</li>
  <li>一个非常好的电子书在线阅读及下载网站： http://t.cn/RP2JcJ3 ，已经收集书籍超过200多万，很多专业书籍均能下载到电子版，例如Bishop的PRML，Duda的PR经典教科书，Vapnik的统计学习理论的2本经典著作…</li>
  <li><a href="http://www.52ml.net/15845.html">Machine Learning Done Wrong</a> 作者总结了机器学习七种易犯的错误：1.想当然用缺省Loss；2.非线性情况下用线性模型；3.忘记Outlier；4.样本少时用High Viriance模型；5.不做标准化就用L1/L2等正则；6.不考虑线性相关直接用线性模型；7.LR模型中用参数绝对值判断feature重要性。</li>
</ul>

<h3 id="section-4">机器学习基本算法</h3>
<ul>
  <li>
    <p>[文章]《The Trouble with SVMs》http://t.cn/RwvkOx4 非常棒的强调特征选择对分类器重要性的文章。情感分类中，根据互信息对复杂高维特征降维再使用朴素贝叶斯分类器，取得了比SVM更理想的效果，训练和分类时间也大大降低——更重要的是，不必花大量时间在学习和优化SVM上——特征也一样no free lunch</p>
  </li>
  <li>数据挖掘中分类算法小结：(1)决策树；(2) KNN法(K-Nearest Neighbor)；(3) SVM法；(4) VSM法；(5) Bayes法；(6)神经网络。http://t.cn/Rwve07S</li>
  <li>
    <p>[论文]《Understanding Random Forests: From Theory to Practice》http://t.cn/RZBT6Ap Louppe, Gilles的博士论文，全面了解随机森林的好材料，推荐！pdf:http://t.cn/RZBTobH 云:http://t.cn/RZBTobT</p>
  </li>
  <li>LDA
    <ul>
      <li>【LDA入门与Java实现】 这是一篇面向工程师的LDA入门笔记，并且提供一份开箱即用Java实现。本文只记录基本概念与原理，并不涉及公式推导。文中的LDA实现核心部分采用了arbylon的LdaGibbsSampler并力所能及地注解了，在搜狗分类语料库上测试良好，开源在GitHub上。什么… http://t.cn/RZBIEYh</li>
      <li><a href="http://www.7300days.com/index.php/stds/topic/list/id/27/name/Topic%20modeling">@Copper_PKU 推荐的35篇Topic Model论文</a></li>
      <li><a href="http://www.cs.colorado.edu/~jbg/docs/2014_book_chapter_care_and_feeding.pdf">Care and Feeding of Topic Models: Problems, Diagnostics, and Improvements</a>  Dave Blei得意门生Jordan Boyd-Graber（科罗拉多大学助理教授）和博士后David Mimno（康奈尔助理教授）是公认的主题模型(Topic Model)专家。近日他们写了一个简短的主题模型入门介绍，并且讨论了主题模型的问题，评价手段，以及部分改进方法。</li>
    </ul>
  </li>
  <li>CRF
    <ul>
      <li>CRF训练，但标注数据很少。感兴趣的朋友可以参考下Semi-supervised Sequence Labeling for Named Entity Extraction based on Tri-Training:Case Study on Chinese Person Name Extraction</li>
      <li>[视频]《Log-linear Models and Conditional Random Fields》http://t.cn/SUGYtC Charles Elkan讲的对数线性模型和条件随机场，非常棒的教程 讲义:http://t.cn/RZ1kQ6A</li>
      <li>http://t.cn/zO7uh30 推荐这个项目，虽然现在都流行 Deep Learning了， CRF 类方法还是很容易达到一个比较高的 Score， 这个项目 f-score 低了 0.7 % 但是速度 提升了 10倍，隐含的，可以处理更大量的样本数据。</li>
      <li>PPT 来了！机器学习班第15次课，邹博讲条件随机场CRF的PPT 下载地址：http://t.cn/RzE4Oy8，第16次课，邹博讲PCA&amp;SVD的PPT 下载地址：http://t.cn/RzE4OyQ，@sumnous_t 讲社区发现算法的PPT 下载地址：http://t.cn/RzE4OyR。顺便说句，sumnous还曾是算法班周六班的学员，一年下来，进步很大。分享！</li>
    </ul>
  </li>
  <li>SVM
    <ul>
      <li>【分类战车SVM】第五话：核函数（哦，这实在太神奇了！）→ http://t.cn/RZ0JICY</li>
      <li>【SVM之菜鸟实现】之前帖子有bug, 担心有朋友受其害，这里给出正确版，用matlab做伪代码: yp=x<em>w; idx=find(yp.</em>y&lt;1) e=”yp(idx)-y(idx);”   f=”e’<em>e+c</em>w<em>w.”  df=”2(x(idx,:)’</em>e+c*w);” fdf=”” lbfgslbfgs=”” svmsgd=”” em=””&gt;</li>
    </ul>
  </li>
  <li>Boost
    <ul>
      <li>陈天奇的xgboost。<a href="http://weibo.com/p/1001603795687165852957">有监督代码心得</a>，<a href="https://github.com/tqchen/xgboost">xgboost</a> ，<a href="http://weibo.com/p/1001603801281637563132">分布式机器学习</a>，<a href="http://weibo.com/p/1001603795728785912771">模板和张量库</a>，<a href="http://weibo.com/p/1001603795714256832384">迭代器和流水处理</a>，<a href="http://courses.cs.washington.edu/courses/cse546/14au/slides/oct22_recitation_boosted_trees.pdf">tutorial</a></li>
    </ul>
  </li>
</ul>

<h3 id="section-5">图像视觉</h3>

<ul>
  <li>
    <p><a href="http://blog.csdn.net/morewindows/article/details/8225783">OpenCV入门指南</a></p>
  </li>
  <li>
    <p><a href="http://quantombone.blogspot.com/2015/01/from-feature-descriptors-to-deep.html">From feature descriptors to deep learning: 20 years of computer vision</a> 从特征描述子到深度学习——机器视觉20年回顾。通俗易懂，回顾了很多特征描述子的内容，也介绍了很多计算机视觉、机器学习特别是深度学习方面的大牛。</p>
  </li>
  <li>
    <p><a href="http://t.cn/RZkbwoz">图像处理中的全局优化技术</a> 最近打算好好学习一下几种图像处理和计算机视觉中常用的 global optimization (或 energy minimization) 方法，这里总结一下学习心得。</p>
  </li>
  <li>
    <p>Retinex算法详解 - 计算机视觉小菜鸟的专栏 - 博客频道 - CSDN.NET http://t.cn/RZFJt0a</p>
  </li>
  <li>
    <p>本团队雕琢多年的人脸检测库现以MIT协议发布 http://t.cn/RwhgTnB ,供商业和非商业无限制使用,包含正面和多视角人脸检测两个算法.优点:速度快(OpenCV haar+adaboost的2-3倍), 准确度高 (FDDB非公开类评测排名第二），能估计人脸角度. 例子看下图. 希望能帮助到有需要的个人和公司。</p>
  </li>
  <li>
    <p><a href="http://www.guokr.com/article/439945/">计算机视觉：让冰冷的机器看懂多彩的世界</a></p>
  </li>
</ul>

<h3 id="section-6">机器学习框架</h3>

<ul>
  <li>
    <p><a href="http://petuum.github.io">Petuum</a> Petuum涵盖的模型更多（CNN、DNN、K-Mean、Regression、Sparse Coding、Matrix Factorization，LDA、 RF等。例如基于Petuum训练CNN，用CPU集群能达到比用GPU的Caffe高的效率。</p>
  </li>
  <li><a href="http://fastml.com/torch-vs-theano/">Torch vs Theano</a>  还可以比较下Caffe。</li>
  <li>《Caffe-LSTM》GitHub:http://t.cn/Rw4pf3G 基于Caffe实现的LSTM，应用例子的ipn:http://t.cn/Rw4pCPX</li>
  <li><a href="https://github.com/srendle/libfm">开源libFM</a> —— 开源Factorization Machines(FM)工具 GitHub:http://t.cn/Rh0QKfr FM可对任意实值向量进行预测，可看作自动的特征选择/组合方法。参考文献：中文简介http://t.cn/8DkCnjI 开山之作http://t.cn/Rw4eWfP KDD2012的Toturialhttp://t.cn/Rw4DwFe 最新例子文章http://t.cn/Rw4e5nT</li>
  <li>Facebook开源了一组深度学习扩展fbcunn：http://t.cn/RZN9gAM @极客头条 是Torch框架的插件，基于NVidia的GPU，大大提升了原来nn神经网络包的性能，可以用于计算机视觉和自然语言处理等场景</li>
  <li>
    <p>Caffe自推出以来便成为了被广泛使用深度学习研究工具，借助Caffe可以轻松训练用于识别、检测等不同人物的深度网络。ECCV‘14 Tutorial《DIY Deep Learning for Vision: a Hands-On Tutorial with Caffe》简要介绍了如何使用Caffe实现不同网络训练任务</p>

    <p>【Caffe 深度学习框架上手教程】http://t.cn/RZTgg3x Caffe是一个清晰而高效的深度学习框架，其作者是博士毕业于UC Berkeley的 贾扬清，目前在谷歌工作。本文详细介绍了它的优势、架构，网络定义、各层定义、安装与配置，解读了它实现的图像分类模型AlexNet，并演示了CIFAR-10在caffe上进行训练与学习</p>
  </li>
  <li>[开源] Vowpal Wabbit http://t.cn/h40wlI GitHub:http://t.cn/zlzvCpS MS家的开源#机器学习#系统，特色是高性能在线学习，简介:http://t.cn/RwLq6VF 性能比较可参考FastML的《Vowpal Wabbit, Liblinear/SBM and StreamSVM compared》http://t.cn/R742J6H</li>
  <li>【mlpack：可伸缩C++机器学习库】mlpack是一个直观、快速、可伸缩的C++机器学习库，旨在为机器学习研究者提供更广泛的机器学习方法和函数。它的目的是让新用户通过简单、一致的API使用机器学习，同时为专业用户提供C++的高性能和最大灵活性。http://t.cn/RzeNyXi</li>
  <li>Minerva V2 Release: http://t.cn/RzubWsd 用NDArray来写machine learning程序，支持多CPU/GPU。目前GPU跑ImageNet 213张/秒，2GPU跑到403张。parameter server下周开始整合。doc还在完善中</li>
  <li>cxxnet http://t.cn/Rzuv17w 卷积神经网络的一个C++实现。写了个Guide，用deep conv net在kaggle那个17万的比赛里5分钟得到前十的结果（当然马上就会变成zero benchmark）。http://t.cn/RzuPYIR 欢迎转发 @陈天奇怪 @phunter_lau @我爱机器学习 @好东西传送门</li>
  <li><a href="http://www.librec.net">LibRec</a> ——JAVA下的先进推荐算法库，刚刚推出1.2新版本，目前支持的推荐算法包括UserKNN, ItemKNN, RegSVD, PMF, SVD++, BiasedMF, BPMF, SocialMF, TrustMF, SoRec, SoReg, RSTE, TrustSVD等。</li>
  <li>Parameter server. <a href="http://www.cs.cmu.edu/~muli/file/osdi14_talk.pdf">slides</a></li>
</ul>

<h3 id="section-7">大数据</h3>
<ul>
  <li>[幻灯]《Large Scale Machine Learning for Information Retrieval》(2013) Bo Long，Liang Zhang http://t.cn/zRNcu9v 面向信息检索的大规模机器学习，内容包括基于Map-Reduce的分布计算、自动推荐系统、大规模逻辑回归、并行矩阵分解、Bag of Little Bootstraps (BLB)方法等 云:http://t.cn/RZsrmqd</li>
  <li><a href="http://wenku.baidu.com/view/623ba70902020740be1e9b27.html">刘小兵的MPI-LR Olympic</a></li>
</ul>

<h3 id="section-8">系统设计</h3>
<ul>
  <li>Optimal Space-time Tradeoffs for Inverted Indexes，Github http://t.cn/RZgGiiN 倒排的最新压缩设计，作者是去年创新索引压缩算法Partitioned Elias-Fano的发明人，今年继续给出各种情形下的最佳选择，http://t.cn/RZgGiiC</li>
</ul>

<h3 id="section-9">高频交易</h3>
<ul>
  <li><a href="http://queue.acm.org/detail.cfm?id=2534976">文章 Online Algorithms in High-frequency Trading - The challenges faced by competing HFT algorithms》2013</a> 介绍高频交易(HFT)中的在线学习算法，重点解决流动性估计、波动性估计和线性回归问题，HFT算法简单了解可参考<a href="http://www.zhihu.com/question/23667442">知乎主题-高频交易都有哪些著名的算法</a></li>
</ul>

<h3 id="section-10">博客推荐</h3>
<ul>
  <li>《Frequently updated Machine Learning blogs》http://t.cn/RwbHZpy 活跃机器学习博客推荐，真有点怀念Google Reader呢</li>
  <li>《面试经验分享之机器学习、大数据问题》如今，好多机器学习、数据挖掘的知识都逐渐成为常识，要想在竞争中脱颖而出，就必须做到：保持学习热情，关心热点，深入学习，会用，也要理解，在实战中历练总结等等。http://t.cn/RzMtL3j（来自： Blog of 太极雪 ）</li>
</ul>

<h3 id="section-11">其他推荐资料</h3>
<ul>
  <li>
    <p><a href="https://breezedeus.github.io/2015/01/31/breezedeus-review-for-year-2014-tech.html">世纪佳缘用户推荐系统的发展历史</a></p>

    <ul>
      <li>“总结、温习，这两点让人成长。而不是你走得有多快！”</li>
      <li>天真的算法年：item-based kNN。推荐以前看过的item的相似item。可逆（Reciprocal）推荐算法，是什么东西？<a href="http://search.aol.com/aol/search?s_it=topsearchbox.search&amp;v_t=opensearch&amp;q=Reciprocal+recommendation">Reciprocal recommendation</a></li>
      <li>技术为产品服务，而不是直接面向用户；数据质量是地基，保证好的质量很不容易；如何制定正确的优化指标真的很难；业务理解 &gt; 工程实现；数据 &gt; 系统 &gt; 算法；快速试错；</li>
      <li>Dirichlet Process 和 Dirichlet Process Mixture</li>
      <li>Alternating Direction Method of Multipliers(ADMM)</li>
      <li><a href="https://breezedeus.github.io/2014/11/19/breezedeus-feature-mining-gbdt.html">利用GBDT模型构造新特征</a></li>
      <li><a href="https://breezedeus.github.io/2014/11/20/breezedeus-feature-hashing.html">特征哈希（Feature Hashing）</a></li>
      <li>不平衡数据的抽样方法。参考文献：William Fithian, Trevor Hastie, Local Case-Control Sampling Efficient Subsampling in Imbalanced Data Sets, 2014.</li>
      <li><a href="http://www.douban.com/note/484853135/">世纪佳缘推荐系统之我见</a>
        <ul>
          <li>明确推荐评价指标：对于婚恋推荐系统来说，最核心的指标无外乎付费的转换率</li>
          <li>我们倒着来推，把问题转换为识别出最愿意付费的那些用户，然后找到这些用户感兴趣的用户，通过产品引导让这些用户发信</li>
          <li>能不能从数据跳出来对产品提出一些创意性改进从而产生的产品模式和收费模式的变革。</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="http://www.wsdm-conference.org/2015/wp-content/uploads/2015/02/WSDM-2015-PE-Leskovec.pdf">New Directions in Recommender Systems</a>
    <ul>
      <li><a href="http://www.douban.com/note/484692347/">飞林沙-读后总结</a></li>
      <li>需要理解 可替换 和 可补充，这两种推荐形式。</li>
      <li>怎样生成替代品的推荐理由，应该是更好，而不是他们包含同一关键词</li>
      <li>推荐一整套装备</li>
      <li>Inferring Networks from Opinions阅读总结：
        <ul>
          <li>Product Graph：Building networks from product text
            <ul>
              <li>Understand the notions of substitute and complement goods</li>
              <li>Generate explanations of why certain products are preferred</li>
              <li>Recommends baskets of related items</li>
            </ul>
          </li>
          <li>learn x and y are related?
            <ul>
              <li>Attempt 1: Text features；缺点：High-dimensional，Prone to overfitting，Too fine-grained</li>
              <li>Attempt 2: Features from Topics。也就是把第一种方法，用topic vector替换，相当于降维了。</li>
              <li>Attempt 3: Learn ‘good’ topics。Learn to discover topics that explain the graph structure；
                <ul>
                  <li>Idea: Learn both simultaneously；we want to learn to project documents (reviews) into topic space such that related products are nearby；</li>
                  <li>Combining topic models with link prediction；topic和link利用一个目标函数，一起训练。</li>
                  <li>Issue 1: Relationships we want to learn are not symmetric；Solution: We solve this issue by learning “relatedness” in addition to “directedness”</li>
                  <li>Issue 3: The model has a too many parameters；Solution: Product hierarchy；Associate each node in the category tree with a small number of topics</li>
                  <li>整个模型用EM算法来求解，类似于PLSA的EM算法。</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Recommender Systems，Alex Smola 阅读笔记
    <ul>
      <li>Neighborhood methods
        <ul>
          <li>Collaborative filtering；User-based (item base is smaller than user or changes rapidly)；Item-based (user base is small)；</li>
          <li>Normalization/Bias; rate bias-&gt; bui = μ(global) + bu(user bias) + bi(item bias);
  <img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/cf_formula.png" alt="" />
  <img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/item_similarity.png" alt="" /></li>
        </ul>
      </li>
      <li>Matrix Factorization
        <ul>
          <li>SVD，常用优化方法：SGD，alternating optimization；问题是Overfitting without regularization，特别是fewer reviews than dimensions</li>
          <li>Risk Minimization。利用Alternating least squares，比较适合MR。
  <img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/matrix_factorization_risk.png" alt="" />
  <img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/matrix_factorization_risk_addbias.png" alt="" /></li>
          <li>优化方法：Add bias，who rated what， temporal effects … P63</li>
        </ul>
      </li>
      <li>Theoretical Motivation
        <ul>
          <li>Rating matrix is (row, column) exchangeable</li>
        </ul>
      </li>
      <li>Ranking and Session Modeling
        <ul>
          <li>Independent click model</li>
          <li>Logistic click model。Exponential family model for click; user looks at all</li>
          <li>Sequential click model; User traverses list</li>
          <li>Skip click model</li>
          <li>Context skip click model</li>
        </ul>
      </li>
      <li>Features
        <ul>
          <li>social network = friendship + interests</li>
          <li>Latent dense (Bayesian Probabilistic Matrix Factorization)</li>
          <li>Latent sparse (Dirichlet process factorization)</li>
        </ul>
      </li>
      <li>Hashing</li>
    </ul>
  </li>
  <li><a href="https://breezedeus.github.io/2012/11/01/breezedeus-yuanquan-etao.html">个性化推荐技术#总结-袁全</a>
    <ul>
      <li>相关性推荐，点击数据更有用；补充性推荐，购买数据更有用；要根据用户行为意图选择不同的推荐方法。</li>
      <li>对于不同种类的产品，当用户处在同一购物流程时，其理想的相关性推荐/补充性推荐的概率也差别很大。</li>
      <li>Mixture Logistic Regression</li>
    </ul>
  </li>
  <li><a href="https://breezedeus.github.io/2012/11/10/breezedeus-jiangshen.html">面向广告主的推荐，江申@百度</a>
    <ul>
      <li>技术目标要正确；譬如对于拍卖词推荐，其数学目标的烟花过程为：推荐最相关的词-&gt;推荐广告主采用率最高的词-&gt;推荐采用率最高且产生推广效果最佳的词。</li>
      <li>在拍卖词推荐中主要涉及到三种模型：相关性模型、采用率模型和推广效果模型。</li>
      <li>负反馈：按照item已经对user展示的次数指数级降低其权重，避免同一个item多次重复被展示给一个用户。</li>
    </ul>
  </li>
  <li><a href="https://breezedeus.github.io/2012/11/12/breezedeus-wenguozhu.html">个性化推荐技术#总结 稳国柱@豆瓣</a>
    <ul>
      <li>电影推荐：首先把电影按照电影标签进行分组（比如分成动作片，剧情片等）；然后在每个组里面使用CF算法产生推荐结果；最后把每组中获得的推荐按照加权组合的方式组合在一块。</li>
      <li>图书推荐：图书有一定的阶梯性，在大部分的场合，我们需要的并不是与自己相似的用户的推荐，而是与自己相似的专家的推荐。</li>
      <li>电台的音乐推荐：必须使用一个算法系统（其中包含多个算法）来针对不同的用户进行不同的算法调度</li>
    </ul>
  </li>
  <li>
    <p><a href="http://www.douban.com/note/472267231/">年终总结 &amp; 算法数据的思考 by 飞林沙</a></p>
  </li>
  <li>
    <p><a href="http://www.aszxqw.com/work/2014/06/01/tuijian-xitong-de-nadianshi.html">推荐系统的那点事</a> 分析了推荐系统中使用算法的误区，确实规则带来的好处简单有效。 当一个做推荐系统的部门开始重视【数据清理，数据标柱，效果评测，数据统计，数据分析】这些所谓的脏活累活，这样的推荐系统才会有救。</p>
  </li>
  <li>
    <p>WSDM2015 上传了Michaol Franklin和Thorsten Joachims的主题报告slides http://t.cn/R7Jyy0g 还有Jure Leskovec和Tushar Chandra的实践与经验报告 slides <a href="http://www.wsdm-conference.org/2015/practice-and-experience-talks/">Practice and Experience Talks</a></p>
  </li>
  <li>
    <p>[文章]《Computing Recommendations at Extreme Scale with Apache Flink and Google Compute Engine》http://t.cn/RZemQe9 Flink实例！用Flink和GAE做面向大规模数据集的协同推荐，从中可看出Flink的巨大应用潜力，文中引用的材料值得一读（作者说了，细节文章即将推出敬请期待，感兴趣请持续关注）</p>
  </li>
  <li>[幻灯]《Recommender Systems: Super Overview》http://t.cn/R7WtFwY 来自Netflix的Xavier Amatriain在Summer School 2014 @ CMU上长达4小时的报告，共248页，是对推荐系统发展的一次全面综述，其中还包括Netflix在个性化推荐方面的一些经验介绍，强烈推荐! 云盘:http://t.cn/RZuLoSS</li>
  <li>
    <p>【干货丨美团推荐算法实践：机器学习重排序模型成亮点】本文介绍了美团网推荐系统的构建和优化过程中的一些做法，包括数据层、触发层、融合过滤层和排序层五个层次，采用了HBase、Hive、storm、Spark和机器学习等技术。两个优化亮点是将候选集进行融合与引入重排序模型。 http://t.cn/RZrgB5u</p>
  </li>
  <li>
    <p><a href="http://tech.meituan.com/machinelearning-data-feature-process.html">美团推荐团队-机器学习中的数据清洗与特征处理综述</a>。</p>
  </li>
  <li><a href="http://benanne.github.io/2014/08/05/spotify-cnns.html">文章-Recommending music on Spotify with deep learning</a> 基于作者的实习经历讲Spotify的音乐推荐，内容涉及：协同过滤、基于内容的推荐、基于深度学习的品味预测、convnets规模扩展、convnets的学习内容、推荐的具体应用等
    <ul>
      <li>Collaborative filtering：content-agnostic（与内容无关的），容易推荐popular items。另外，new and unpopular songs cannot be recommended，即cold-start problem。</li>
      <li>Content-based：tags, artist and album information, lyrics, text mined from the web (reviews, interviews, …), and the audio signal itself（e.g. the mood of the music）。</li>
      <li>Predicting listening preferences with deep learning。</li>
    </ul>
  </li>
  <li>【推荐系统中所使用的混合技术介绍】http://t.cn/8sKdQFq 系统架构层面一般使用多段组合混合推荐框架，算法层面则使用加权型混合推荐技术，包括LR、RBM、GBDT系列。此外还介绍分级型混合推荐技术，交叉调和技术，瀑布型混合方法，推荐基础特征混合技术，推荐模型混合技术，整体式混合推荐框架等。</li>
</ul>


  </article>

</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">100的技术博客</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col  footer-col-1">
        <ul class="contact-list">
          <li>100的技术博客</li>
          <li><a href="mailto:zero_based@foxmail.com">zero_based@foxmail.com</a></li>
        </ul>
      </div>

      <div class="footer-col  footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/zzbased">
              <span class="icon  icon--github">
                <svg viewBox="0 0 16 16">
                  <path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
                </svg>
              </span>

              <span class="username">zzbased</span>
            </a>
          </li>
          

          
          <li>
            <a href="https://twitter.com/zero_based">
              <span class="icon  icon--twitter">
                <svg viewBox="0 0 16 16">
                  <path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                  c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
                </svg>
              </span>

              <span class="username">zero_based</span>
            </a>
          </li>
          
        </ul>
      </div>

      <div class="footer-col  footer-col-3">
        <p class="text">机器学习，自然语言处理，计算广告学，工作与生活，总结与温习
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
