title: "机器学习技法学习笔记"
---

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>


# 机器学习技巧 学习笔记

有用链接：

- [机器学习基石](https://www.coursera.org/course/ntumlone)
- [机器学习技法](https://class.coursera.org/ntumltwo-001/lecture)
- [beader.me笔记](http://beader.me/mlnotebook/)
- [听课笔记douban](http://www.douban.com/doulist/3440234/)
- [mooc学院](http://mooc.guokr.com/course/610/機器學習基石--Machine-Learning-Foundations-/)


## 第1讲 Linear Support Vector Machines

我们的目标是：最大间隔

求一个点x距离一个平面的距离：

点x到平面上的点x'的向量 x-x'，在平面的法向量上的投影：w*(x-x')/|w|，即|w^T*x+b|/|w|。

最大化这个距离，可以假设 min{y*(wx+b)}=1。那么目标变为：

max 1/|w|  条件是： min{y*(wx+b)}=1

进一步推导，得到最终优化的目标：

min 1/2 w*w^T  subject to y(wx+b)>=1

这就是支持向量机的优化目标，它的损失函数，等同于： max{0, 1-ywx}

注意：函数间隔与几何间隔。

可以将这个优化目标转化到 [二次规划 quadratic programming](http://cn.mathworks.com/discovery/quadratic-programming.html)

![](quadratic_programming.png)

large-margin algorithm的VC维分析。因为large margin的限制，相比于较于PLA，svm的dichotomies会更少。所以从VC维看，相比于PLA，其泛化能力更强。

![](vc_dimension_of_large_margin_algorithm.png)

large-margin hyperplanes：参数最少，所以boundary最简单。
一般的hyperplanes：参数适中，边界简单。
一般的hyperplanes+feature转换(非线性的)：参数较多，边界复杂。
large-margin hyperplanes+feature transform：则可以得到适中的参数个数，复杂的边界。

![](Benefits-of-Large-Margin-Hyperplanes.png)

**扩展阅读**
[支持向量机通俗导论（理解SVM的三层境界）](http://blog.csdn.net/v_july_v/article/details/7624837)

**习题**

![](2chapter1_question1.png)
![](2chapter1_question2.png)

## 第2讲 Dual support vector machine

讨论： Support Vector Classification，Logistic Regression，Support Vector Regression的区别：

![](L2-regularized-L1-and-L2-loss-Support-Vector-Classification.png)

![](L2-regularized-Logistic-Regression.png)

![](L1-regularized-L2-loss-Support-Vector-Classification.png)

![](L1-regularized-Logistic-Regression.png)

![](L2-regularized-L1-and-L2-loss-Support-Vector-Regression.png)

复习一下第1讲，直接求解SVM的original问题，利用QP方法，需要求解 d+1个变量(d指代feature转换后的维度)，N个约束条件。如果我们采用一个非线性变换，维度特别高，就不太可解了，所以我们想SVM without d。所以有 ‘Equivalent’ SVM: based on some dual problem of Original SVM。

这时就要用到lagrange multipliers。这里看下正则化，为什么正则化的表达式是这样的，这是通过lagrange multipliers。

![](Lagrange-Multipliers-regularization.png)

下面是SVM的对偶问题推导过程：

![](Lagrange-Function1.png)

![](lagrange-dual-problem2.png)

这里要提一下KKT条件：

![](kkt_11.png)

![](kkt_12.png)

经过一通推导，我们得到了svm的对偶问题：

![](Dual-Formulation-of-svm.png)

这个对偶问题，就可以用QP来求解了。

求得a后，primal问题的w和b，可以通过下面式子求得：

![](w_b_optim.png)

最后说一个解释：当a_n大于0时，此时该点正好处于边界上，这也就是所谓的支撑向量。

有趣之处在于，对于新点x的预测，只需要计算它与训练数据点的内积即可（表示向量内积），这一点至关重要，是之后使用 Kernel 进行非线性推广的基本前提。此外，所谓 Supporting Vector 也在这里显示出来——事实上，所有非Supporting Vector 所对应的系数都是等于零的，因此对于新点的内积计算实际上只要针对少量的“支持向量”而不是所有的训练数据即可。



**习题**

![](2chapter2_question1.png)
![](2chapter2_question2.png)
![](2chapter2_question3.png)
![](2chapter2_question4.png)

## 第3讲
为什么要把SVM转换到对偶问题，原因有这样几个：1.对偶问题的变量为N个，有时候N远远小于d。2.解释了support vector。 3.比较直观的引入了核函数。

在线性不可分的情况下，支持向量机首先在低维空间中完成计算，然后通过核函数将输入空间映射到高维特征空间，最终在高维特征空间中构造出最优分离超平面，从而把平面上本身不好分的非线性数据分开。

建立非线性学习器分为两步：
首先使用一个非线性映射将数据变换到一个特征空间F，
然后在特征空间使用线性学习器分类。

核函数的优势在于：
一个是映射到高维空间中，然后再根据内积的公式进行计算；
而另一个则直接在原来的低维空间中进行计算，而不需要显式地写出映射后的结果。

![](Kernel-SVM-with-QP.png)

多项式核：

![](Poly-2-Kernel.png)

SVM + Polynomial Kernel: Polynomial SVM

![](Poly-Kernel.png)

高斯核：

![](gaussian_kernel.png)

看一下高斯核参数改变带来的变化：

![](gaussian_kernel2.png)

下面对比一下常用的几种核函数：

![](compare_linear_kernel.png)
![](compare_poly_kernel.png)
![](compare_gaussian_kernel.png)

当然，除了上面三种常用的核函数外，还可以自己构造一些核，只需要这些核满足mercer's condition。不过需要说明的，很难。

![](valid_kernel_2.png)

**习题**

![](2chapter3_question1.png)

![](2chapter3_question2.png)

## 第4讲

使用松弛变量处理 outliers 方法，本讲的内容。


## 第6讲  Blending and Bagging

Aggregation的方法包括：select, mix uniformly, mix non-uniformly, combine;
![](aggregation_1.png)

为什么Aggregation方法是有效的？可以从两方面来看，其一通过Aggregation可以生成复杂的hypotheses，相当于做了feature transform；其二，生成的G(x)更加moderate，例如下图中PLA的uniform mix就是large-margin，相当于做了regularization。

![](aggregation_works.png)

**uniform blending**

如果是classification，则有：

![](uniform_blending_for_classification.png)

如果是regression，则有：

![](uniform_blending_for_regression.png)

从上图还可以看出：任意g的Eout平均大于等于G的Eout。

从上图的公式还可以得出，expected performance of A = expected deviation to consensus +performance of consensus。

![](uniform_blending_reduce_variance.png)

**Linear Blending**

linear blending就像two-level learning。

![](linear_blending.png)

like selection, blending practically done with (Eval instead of Ein) + (gt− from minimum Etrain)

Any blending也叫Stacking。

![](any_blending.png)

**bagging**

aggregation里最重要的一个点就是：diversity。diversity的方法有很多种。

![](diversity_important.png)

下面介绍一种通过data randomness的方法，也叫bootstrapping，即bagging。

![](bootstarpping1.png)

**习题**

![](2chapter7_question1.png)

## 第8讲 Adaptive Boosting

课程的最开始有一个分辨苹果的例子。以后AdaBoost的时候可以借鉴那个例子。其基本思路是：给予上次分错的样本更高的权重。

![](re_weighting_bootstrapping.png)

给每个example不同的weight，类似于给予不同的class的样本不同的weight。回忆一下，有时候我们false reject尽可能低，那对于这一类，我们在error measure给予更高的权重。

![](re_weighting_bootstrapping2.png)

![](false-accept-and-false-reject.png)

具体怎么更新下一次训练的样本权重呢，参考下面的图：

![](scaling_factor.png)

有了样本权重更新公式后，则有一个Preliminary算法：

![](adaboost_preliminary.png)

得到这么多的g后，怎么得到G，也就是aggregation的方法，我们希望在计算g的时候把aggregation的权重也得到。

![](adaboost1.png)

那么完整算法为：
![](adaboost2.png)

下面是一些理论：

![](Theoretical-Guarantee-of-AdaBoost.png)

Decision Stump

![](Decision-Stump.png)

AdaBoost与Decision Stump的结合 -- > AdaBoost-Stump:
efficient feature selection and aggregation

**习题**

![](2chapter8_question1.png)

## 第9讲 Decision Tree

decision tree的位置，模仿人脑决策过程。

![](decision-tree1.png)

decision tree缺点：(1)启发式的规则(前人的巧思)，缺乏理论基础；(2)启发式规则很多，需要selection；(3)没有代表性的算法。

![](Disclaimers-about-Decision-Tree.png)

一个基本的decision tree算法：

![](basic-decision-tree.png)

CART: classification and regression tree。
有两个简单的选择：binary tree；叶子节点是常数。

怎么选择branching，切完后两个子树的纯度最高。

![](Purifying-in-CART.png)

怎么考量"不纯度"

![](Impurity-Functions.png)

最终CART算法如下：

![](cart_algorithm.png)

关于CART算法的演算过程，具体请参考 [决策树算法的计算过程演示](http://mydisk.com/yzlv/webpage/datamining/xiti.html)，[Decision tree learning](http://en.wikipedia.org/wiki/Decision_tree_learning)，[An example of calculating gini gain in CART](http://www.academia.edu/7032069/An_example_of_calculating_gini_gain_in_CART)

几种决策树算法的区别：

C4.5算法是在ID3算法的基础上采用信息增益率的方法选择测试属性。 ID3算法和C4.5算法虽然在对训练样本集的学习中可以尽可能多地挖掘信息，但其生成的决策树分支较大，规模较大。为了简化决策树的规模，提高生成决策树的效率，又出现了根据GINI系数来选择测试属性的决策树算法CART。
CART算法采用一种二分递归分割的技术，与基于信息熵的算法不同，CART算法对每次样本集的划分计算GINI系数，GINI系数，GINI系数越小则划分越合理。CART算法总是将当前样本集分割为两个子样本集，使得生成的决策树的每个非叶结点都只有两个分枝。因此CART算法生成的决策树是结构简洁的二叉树。

Regularization

![](Regularization-by-Pruning.png)

当有categorical features时，CART也可以灵活处理。

![](Branching-on-Categorical-Features.png)

如果有缺失特征的话，怎么办？可以利用surrogate feature。

看一个CART的例子：

![](cart_example.png)


**习题**

![](2chapter9_question1.png)

## 第10讲 random forest
Bagging and Decision Tree，将这两者合在一起，就是Random forest。

random forest (RF) = bagging + fully-grown C&RT decision tree

![](Bagging-and-Decision-Tree.png)

三个优点：

- highly parallel/efficient to learn
- inherit pros of C&RT
- eliminate cons of fully-grown tree

因为是random forest，除了在bootstrapping时利用data randomness，还可以randomly sample d' feature from x。即original RF re-sample new subspace for each b(x) in C&RT。

那么更进一步了，RF = bagging + random-subspace C&RT

random-combination的意思是：随机抽样一些features后，line combination，作为一个新的feature切分点。那么original RF consider d′ random low-dimensional projections for each b(x) in C&RT。

所以，再进一步：RF = bagging + random-combination C&RT

从上面可以看出，randomness是随处不在的。

回顾一下bagging的过程，每次随机抽样一些数据，这样下去，总会有一些样本是一直未被抽中的。未被抽中的概率计算为：

![](numbers_of_oob.png)

有了这些out-of-bag (OOB) examples后，可以将其作为validation set来使用。

![](oob_vs_validation.png)

那么，相比于原来的validation过程，RF可以做self-validation，也就是在训练的过程中，把model选择顺便也做了。

![](model_selection_by_oob.png)

接着看下Feature selection，decision tree正好是一个内建的feature selection过程。

先看下利用linear model做feature importance判别，训练完的模型，weight越大，表示feature越重要。

![](Feature_Selection_by_Importance.png)

而RF可以采用permutation test来做特征选择。所谓permutation test，也就是对某一个特征，对所有样本上该维度的特征值做随机排列，然后在这个样本集上计算RF performance。用原来的performance减去这个新的performance后，就得到该特征的重要性。如下图所示：

![](feature_selection_by_permutation_test.png)

但是在RF上，因为OOB的存在，可以利用Eoob(G)-Eoob^p(G)。Eoob^p(G)是通过在OOB上permute某一维特征值。**这里后续可以再深挖**

![](feature_importance_by_rf.png)

**习题**

![](2chapter10_question1.png)

![](2chapter10_question2.png)

## 第11讲 Gradient Boosted Decision Tree(GBDT)

random forest用一句话来总结，则是：bagging of randomized C&RT trees with automatic validation and feature selection。

比较一下Random forest和AdaBoost Tree。
![](compare_rd_adaboost-tree.png)

但是要做AdaBoost Tree的话，首先需要weighted DTree。这个在LR,SVM等模型上容易做到，但是在DT上很难。所以我们换个思路，如果我们想给某个样本加个weight，可以在sample样本的时候，增大或者减小它的概率即可。

![](Randomized-Base-Algorithm.png)

所以AdaBoost-DTree的组成由下图所示：
![](AdaBoost-DTree1.png)

在adaboost算法中，如果一个g的错误率为0的话，那么这个g的权重将是无限大的。而在决策树的世界里，如果是full-grown的话，在训练数据上，错误率为0是很容易办到的。

那么为了避免这种过拟合的情况存在，我们需要对DT做剪枝。

![](AdaBoost-DTree2.png)

当我们extremely剪枝时，譬如限制树的高度小于等于1，那此时DT就变成了decision stump。所以有了adaboost-stump算法，它是AdaBoost-DTree的一种特例。

2，3，4节 未完待续。

更多具体的内容，请参考单独的文章： [Aggregation模型](http://zzbased.github.io/2015/04/03/Aggregation模型.html)

**习题**

![](2chapter11_question1.png)

## 第12讲 神经网络

**Motivation**

通过"Linear Aggregation of Perceptrons"，可以完成AND，OR，NOT等操作，可以完成 convex set等操作，但是不能完成XOR操作。怎么办？只能multi-layer perceptron。

XOR(g1, g2) = OR(AND(−g1, g2), AND(g1, −g2))

![](perceptron_powerful_limitation.png)

perceptron (simple)
=⇒ aggregation of perceptrons (powerful)
=⇒ multi-layer perceptrons (more powerful)

![](2chapter12_question1.png)

**Neural Network Hypothesis**

output：any linear model can be used；
transformation function of score (signal) s：不用linear，因为多层线性=>whole network linear。也不用阶梯函数(0-1)，因为它不可微。通常的选择有tanh(x)，sigmoid(s)。

tanh(x) = [exp(s)-exp(-s)] / [exp(s)+exp(-s)] = 2sigmoid(2x)-1

![](2chapter12_question2.png)

**Backpropagation**

![](Backpropagation_1.png)

![](Backpropagation_2.png)

![](2chapter12_question3.png)

**Optimization**

当multiple hidden layers，一般都是non-convex。对于最优化来说，不容易求得全局最优解。GD/SGD可能只能求出局部最优解。

对Wij做不同的初始化，可能有不同的局部最优解。所以对初始化值比较敏感。

有效的建议是：不要初始化太大的weights，因为large weight，加上tanh后，将saturate。如果做梯度下降的话，那段区域里有small gradient。所以建议要try some random&small ones。

神经网络的dVC=O(VD)，V表示神经元的个数，D表示weight的个数，也就是edge的数目。

VC维太大，容易overfit。可以加一个L2 regularizer。但是加L2后，带来的只是shrink weights。我们希望可以得到sparse解，那么就可以用L1 regularizer，但L1不可微分。
所以另外一个选择是：weight-elimination（scaled L2），即large weight → median shrink; small weight → median shrink

![](weight-elimination-regularizer.png)

Early Stopping，随着t 增长，VC维越大。所以合适的t 就够了。

![](BP-Early-Stopping.png)

![](2chapter12_question4.png)


## 第13讲 Deep Learning

structural decisions: key issue for applying NNet。模型结构很关键。

![](Challenges-and-Key-Techniques-for-Deep-Learning.png)

hinton 2006提出的：

![](A-Two-Step-Deep-Learning-Framework.png)

![](Information-Preserving-Neural-Network.png)

Auto-encoder的作用：监督学习的话，给予做特征；无监督学习的话，用来做密度预测，也可以用来做异常点检测。

![](Deep-Learning-with-Autoencoders.png)

Regularization in Deep Learning的方法：

- structural decisions/constraints，譬如卷积神经网络，循环神经网络
- weight decay or weight elimination regularizers
- Early stopping
- dropout，dropconnect等
- denosing

![](denosing_auto-encoder.png)

Linear Autoencoder Hypothesis
![](linear-autoencoder-hypothesis.png)

简单点看就是：h(x) = WW^T x

复习一下特征值和特征向量。[特征向量wiki](http://zh.wikipedia.org/wiki/特征向量)

![](optimal_v_linear_autoencoder.png)

![](pca-for-autoencoder.png)

**习题**

![](2chapter13_question1.png)

## 第14讲 Radial Basis Function Network

以前在讲SVM时，有提到RBF kernel(gaussian kernel)，这里回顾一下。高斯核是将x空间变换到z空间的无限维。

![](gaussian_svm.png)

基于高斯核的SVM如下所示，相当于是support vector上的radial hypotheses的线性组合。

![](gaussian_svm2.png)
So，Radial Basis Function (RBF) Network: linear aggregation of radial hypotheses。

将RBF network类比于neural network，output layer是一样的，都是线性组合，不一样是隐藏层(在RBF network里，是distance + gaussian)。

![](rbf_network.png)

基于RBF network来解释SVM：

![](RBF-Network-Hypothesis.png)

此时，需要学习的参数是 u_m(是centers)，b_m(是不同rbf线性组合的系数)。

kernel是Z空间的相关性度量，而RBF是X空间的相关性度量。

![](rbf_vs_kernel.png)

所以RBF network： distance similairty-to-centers as feature transform。

Full RBF Network：是说将所有样本点都参与到运算里(M=N)。

![](full_rbf_network.png)

full rbf network是一个lazy way to decide u_m。

Nearest-Neighbor：

![](Nearest-Neighbor.png)

如果是利用RBF network做regression呢？如下所示。但是这样做了后，Ein(g)=0，这样势必会overfit。
所以需要做正则化。正则化的思路有：(1)类似于kernel ridge regression，加正则项。(2)fewer centers，譬如support vector。constraining number of centers and voting weights。

那怎样才能做到fewer centers呢？通常的方法就是：寻找prototypes。那how to extract prototypes?

![](Regularized-Full-RBF-Network.png)

这里提到一种算法：k-means cluster。k-means的优化思路为：alternating minimization。说到这，EM也属于alternating minimization。

![](k-Means-Algorithm.png)

OK，现在prototypes提取到了，接下来把基于k-means的rbf network写出来。

![](RBF-Network-Using-k-Means.png)

下面是实战，先看一个k-means的例子：

![](k-means-examples.png)

可以看到，k和初始化，在k-means算法里非常关键。

通常随机地从样本中挑k个出来作为k个初始的聚类中心。但这不是个明智的选择。它有可能会导致图像趋于稠密聚集某些区域，因为如果训练样本本身就在某个区域分布非常密，那么我们随机去选择聚类中心的时候，就会出现就在这个数据分布密集的地方被选出了很多的聚类中心。

那么怎么做k-means的初始化呢？

- 多次运行数据集合，选择最小的SSE的分簇结果作为最终结果。该方法依赖于数据集合和簇数量，对分簇结果有比较大影响，所以在某些场景下效果也不是很好。
- 抽取数据集合样本，对样本进行Hierarchical Clustering技术，从中抽取K个Clustering作为初始中心点。该方法工作良好，知识有两点限制条件：抽样数据不能太大，因为Hierarchical Clustering比较耗时间；K值相对于抽样数据比较小才行。
- kmeans++算法。[kmeans++ wiki](http://en.wikipedia.org/wiki/K-means%2B%2B)，[kmenas++中文](http://www.cnblogs.com/shelocks/archive/2012/12/20/2826787.html)。

K-means++的步骤为：

1. 从输入的数据点集合中随机选择一个点作为第一个聚类中心
2. 对于数据集中的每一个点x，计算它与最近聚类中心(指已选择的聚类中心)的距离D(x)
3. 选择一个新的数据点作为新的聚类中心，选择的原则是：D(x)较大的点，被选取作为聚类中心的概率较大
4. 重复2和3直到k个聚类中心被选出来
5. 利用这k个初始的聚类中心来运行标准的k-means算法

更多关于k-means初始化的方法，请参考 [Efficient and Fast Initialization Algorithm for K- means Clustering](http://www.mecs-press.org/ijisa/ijisa-v4-n1/IJISA-V4-N1-3.pdf)

**扩展阅读**

- 发表在Science的论文：基于密度的快速无监督聚类方法 [Clustering by fast search and find of density peaks. A Rodriguez, A Laio (2014) ](http://t.cn/RAASZ4q) 很棒，推荐给没看过的朋友，另有相关中文两篇：http://t.cn/RPoKmOi http://t.cn/RPOs6uK 供参考理解 云:http://t.cn/RAACowz

	对所有坐标点，基于相互距离，提出了两个新的属性，一是局部密度rho，即与该点距离在一定范围内的点的总数，二是到更高密度点的最短距离delta。作者提出，类簇的中心是这样的一类点：它们被很多点围绕（导致局部密度大），且与局部密度比自己大的点之间的距离也很远。

- Canopy 聚类算法的基本原则是：首先应用成本低的近似的距离计算方法高效的将数据分为多个组，这里称为一个Canopy。Canopy 之间可以有重叠的部分。然后采用严格的距离计算方式准确的计算在同一 Canopy 中的点，将他们分配与最合适的簇。Canopy 聚类算法经常用于 K 均值聚类算法的预处理，用来找合适的 k 值和簇中心。[Clustering Algorithm/聚类算法](http://blog.pureisle.net/archives/2045.html)，[Canopy clustering algorithm](http://en.wikipedia.org/wiki/Canopy_clustering_algorithm)。

- [文章 K-means Clustering with scikit-learn](http://t.cn/RAwxOJx) PyData SV 2014上Sarah Guido的报告，Python下用Scikit-Learn做K-means聚类分析的深入介绍，涉及k值选取、参数调优等问题，很实用 GitHub:http://t.cn/RAwxsFS 云(视频+讲义):http://t.cn/RAwJJkG

- [文章 Divining the ‘K’ in K-means Clustering](http://t.cn/RwlDlgq) 用G-means算法确定K-means聚类最佳K值，G-means能很好地处理stretched out clusters(非球面伸展型类簇)

- [RBF的核心论文 Introduction to Radial Basis Function Networks](http://www.cc.gatech.edu/~isbell/tutorials/rbf-intro.pdf)

**习题**

![](2_chapter14_question1.png)

![](2_chapter14_question2.png)


## 第15讲 Matrix Factorization

从"Linear Network" Hypothesis说起，用来做推荐，也就是根据feature x，预测得分y。

![](Linear-Network-Hypothesis.png)

求解上面的linear network，采用squared error。

![](linear_model_for_recommendation.png)

从上面的求解过程，得到Matrix factorization：

![](Matrix-Factorization1.png)

具体下来，应该怎么求解呢？考虑到这里面有两个变量W和V，这时可以采用alternating minimization。

![](Matrix-Factorization-Learning.png)

所以，得到Alternating Least Squares方法。

![](Alternating-Least-Squares.png)

比较一下 linear autoencoder 和 matrix factorization。linear autoencoder
≡ special matrix factorization of complete X

![](Linear-Autoencoder-versus-Matrix-Factorization.png)

上面讲述了 alternating解法，matrix factorization还可以利用Stochastic gradient descent求解。SGD：most popular large-scale matrix factorization algorithm，比alternating速度更快。

![](SGD-for-Matrix-Factorization.png)

举一个SGD的例子。在KDDCup 2011 Track1中，因为该推荐任务与时间系列有关，所以在优化时，没有用stochastic GD算法，而是采用了time-deterministic GD算法，也就是最近的样本最后参与计算，这样可以保证最近的样本拟合得更好。

![](KDDCup-2011-Track1.png)


## Extraction Models总结

将特征转换纳入到我们的学习过程。

Extraction Models： neural network，RBF network，Matrix Factorization。

![](Map-of-Extraction-Models.png)

Extraction Techniques：function gradient descnet，SGD。

无监督学习用于预训练，例如autoencoder，k-means clustering。

![](Map-of-Extraction-Techniques.png)

regularization：

![](Pros-and-Cons-of-Extraction-Models.png)

## 第16讲 Finale 大总结

Exploiting Numerous Features via Kernel：Polynomial Kernel，Gaussian Kernel等。

![](Exploiting-Numerous-Features-via-Kernel.png)

Exploiting Predictive Features via Aggregation：

![](Exploiting-Predictive-Features-via-Aggregation.png)

Exploiting Hidden Features via Extraction：

![](Exploiting-Hidden-Features-via-Extraction.png)

Exploiting Low-Dim. Features via Compression：

![](Exploiting-Low-Dim.Features-via-Compression.png)

**习题**

![](2chapter16_question1.png)
