<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>书签整理笔记</title>
  <meta name="description" content="书签整理笔记">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://yourdomain.com/2014/02/05/%E4%B9%A6%E7%AD%BE%E6%95%B4%E7%90%86%E7%AC%94%E8%AE%B0.html">
  <link rel="alternate" type="application/rss+xml" title="100的情怀 - 技术博客" href="http://yourdomain.com/feed.xml" />
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">100的情怀 - 技术博客</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
        
          
          <a class="page-link" href="/about/">About</a>
          
        
          
        
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">书签整理笔记</h1>
    <p class="post-meta">Feb 5, 2014</p>
  </header>

  <article class="post-content">
    <h2 id="section">书签整理笔记</h2>

<p>一直以来，看到有用的网页资源，总会将其收藏到书签里。但我们不妨扪心自问，我们收藏的书签有多少可以被重新阅读。</p>

<p>趁着春节假期，我打算将自己的2500多条书签收藏全部过一遍，然后整理一下，对于其中有趣的网页，也写点读书笔记，将其记录在这里。</p>

<h3 id="section-1">机器学习</h3>
<ul>
  <li>
    <p><a href="http://adc.alibabatech.org/carnival/history/schedule/2013/detail/main/280?video=1">余凯深度学习视频-ali技术嘉年华</a>
余凯最近关于深度学习讲了很多视频，这个视频的内容比较通俗，几乎没啥深层次的技术，都是比较宽泛的，但对于新学着来说，听听还是不错的。1）kernel svm, boosting相当于单个隐层的浅层学习模型。2）语音原来都是gmm-hmm-lm，现在都用dnn-hmm-lm. 3) deeplearning building blocks: rbms, autoencode, sparse encoding. 4） unsupervised pre-learning有两个作用，一让优化更容易，二减少过拟合。但它不是必须的，当有足够数据的时候，fine tune就足够了。</p>
  </li>
  <li><a href="fastml.com">fastml</a>  这是一个关于机器学习方面的博客，偏deep learning。里面有很多有趣的文章，之前我做图像检索时就从这里吸取了一些经验。</li>
  <li><a href="http://fastml.com/yesterday-a-kaggler-today-a-kaggle-master-a-wrap-up-of-the-cats-and-dogs-competition/">overfeat kaggle</a> 这篇文章主要讲利用decaf和overfeat提取图像的特征，然后利用其他分类算法来实现图像分类器。有趣的结论有两个：一是对raw feature做标准化后效果更差。二是利用不同特征组合，不同分类算法，训练多个分类模型，然后采用voting的策略做出最终分类，也可以利用boosting的思想。</li>
  <li><a href="http://fastml.com/go-non-linear-with-vowpal-wabbit/">vowpal wabbit, non-linear</a>   <a href="http://hunch.net/~vw/">vw</a>旨在设计一个快速的，扩展性好的学习算法。它包括online gradient descent，conjugate gradient (CG), mini-batch, and data-dependent learning rates。它还包括一些非线性模型，譬如单隐藏层神经网络，N-grams，Quadratic and cubic features。</li>
  <li><a href="http://fastml.com/running-things-on-a-gpu/">running things on gpu</a>  gpu适合用来做矩阵运算，所以在机器学习特别是深度学习上得到很大的运用。GPUs differ from CPUs in that they are optimized for throughput instead of latency。Cudamat and Theano是两个深度学习的python gpu库，文中测试了这两个库。最后，文章还列出了其他gpu库，包括：pylearn, torch, cuda-convnet, deepnet, cuv, nnforge, caffe, gpumlib, gtsvm, cusvm, gpusvm, gpu-libsvm。其中torch，cuda-convnet, caffe, nnforge这几个库我已经测试使用过，其他的有时间可以再测试看看。 PS: 使用<a href="https://pypi.python.org/pypi/joblib">joblib</a>使python程序可以简单运行在多核上。</li>
  <li><a href="http://fastml.com/intro-to-random-forests/">random forest</a>  trees ensembles有两种基本形式：bagged trees and boosted trees。bagged trees中各个树的建立都是独立的，而boosted trees中新树的建立要考虑旧树的缺点。random forest属于bagged trees. 除了效率和速度，random forest还很方便使用，它没有很多的参数需要调试，最重要的参数是树的个数，不需要对数据做转换和尺度变换。最后要提的是：a random forest generates an internal unbiased estimate of the generalization error as the forest building progresse，known as out-of-bag error, or OOBE.</li>
  <li><a href="http://fastml.com/regularizing-neural-networks-with-dropout-and-with-dropconnect/">dropout and with DropConnect</a>  Dropout is certainly one of the bigger steps forward in neural network development. It adresses the main problem in machine learning, that is overfitting. It does so by “dropping out” some unit activations in a given layer, that is setting them to zero.DropConnect by Li Wan et al., takes the idea a step further. Instead of zeroing unit activations, it zeroes the weights. 一个是将激励元的输出置0，另一个则是将连接的weight置为0.  DropConnect seems to offer a similiar performance to dropout in terms of accuracy, although the details are somewhat complicated due to a large number of model hyperparameters involved - things such as network architecture and the length of training.</li>
</ul>

<h3 id="section-2">系统架构</h3>

<ul>
  <li><a href="http://cxwangyi.github.io/2014/01/20/distributed-machine-learning/">分布式机器学习的故事</a>
(1)大数据时代的并行和传统的并行是不一样的，主要着重于数据并行。并行编程框架主要有mpi,mapreduce,bsp。MPI的缺陷是没有fault recovery。而mapreduce的缺陷是每轮迭代都要访问分布式文件系统，效率不够高。(2)rephil模型是一个神经元网络模型。LDA和plsa这类概率模型的主要构造单元都是指数分布(exponential distributions)。而internet上的实际数据基本都不是指数分布，而是长尾分布。plsa和lda都割掉了数据的尾巴，而rephil没有，它是一个能理解百态的模型。</li>
  <li>
    <p><a href="http://cxwangyi.github.io/2013/04/09/asynchronous-parameter-updating-with-gradient-based-methods/">异步sgd</a>
downpour sgd：在大数据中，如何使用sgd。将训练数据划分为n份，在每份数据子集上run a copy of the model，然后通过中央参数服务器更新参数，参数也被分为n份，各自独立更新。This approach is asynchronous in two distinct aspects: the model replicas run independently of each other, and the parameter server shards also run independently of one another.
相比于传统的sgd，异步sgd效果为什么会好，益总这里给出了他的解释：The swamp of bees optimize collaboratively and covers a region like region-based optimization, where the region is composed of a set of points. This, I think, is the reason that parallel asynchronous SGD works better than traditional gradient-base optimization algorithms.</p>
  </li>
  <li>
    <p><a href="http://astyle.sourceforge.net/astyle.html">代码格式调整工具</a>
pong搞了一个google风格的astyle ,还没来得及用，年后去试一试。</p>
  </li>
  <li>
    <p><a href="http://www.cocos2d-x.org/">cocos2d游戏引擎</a>, <a href="http://libgdx.badlogicgames.com/">Java game development framework</a>
游戏引擎，这里有两例，有空的时候尝试开发一些移动端的程序。</p>
  </li>
  <li>
    <p><a href="http://coolshell.cn/articles/10910.html">分布式系统的事务处理</a>   <a href="http://blog.csdn.net/sparkliang/article/details/5279393">一致性hash算法</a>
通常用两种手段来扩展我们的数据服务：数据分区(一致性hash)与数据镜像。要想让数据有高可用性，就得写多份数据。这就涉及到一致性模型。1）master-slave 2）master-master 3)Two/Three Phase Commit
这个协议的缩写又叫2PC，中文叫两阶段提交。4) Two generals problem. 两将军问题。5） paxos算法  <a href="http://blog.csdn.net/baiduforum/article/details/7007741">描述1</a> <a href="http://zh.wikipedia.org/zh/Paxos算法#.E5.AE.9E.E4.BE.8B">描述2</a></p>
  </li>
  <li>
    <p><a href="http://coolshell.cn/articles/7490.html">性能调优攻略</a>
2年前的文章，但现在来看，依然一点都不过时。的确是一篇雄文，还需要多读几遍。记录几点有感触的。1）系统性能定义：throughput , latency。2）对于多核cpu，CPU0至关重要，因为cpu各核之间的调度是由cpu0来完成的。3）stl函数复杂度的不同，抽空需要把effective stl再看看。4）网络调优，keepalive参数，TIME_WAIT状态，tcp receive window size＝吞吐量  * 回路时间，udp mtu，小心dns lookup系统调用。5）aio方式。一种是产生信号，另一种是基于线程的回调函数。6）多核cpu的numa技术。7）多表查询，exists,in,join.<a href="http://explainextended.com/2009/06/16/in-vs-join-vs-exists/">对比</a> 8) tcp_tw_recycle最好不要打开，因为企业上网都是用nat后的，同一公网ip会有不同的timestamp.（linux 和 mac）。这样就造成了企业nat后面的用户有的能打开网站，有的打不开的问题。</p>
  </li>
  <li>
    <p><a href="http://programmers.stackexchange.com/questions/49550/which-hashing-algorithm-is-best-for-uniqueness-and-speed/145633#145633">Hash函数对比测试</a>
文中对比了murmur, fnv-1a, fnv-1, dbj2a, dbj2, sdbm, superfasthash, crc32, loselose。测试了三种场景。综合来看，murmur2的hash time最少，性能最好，collisions一般。冲突最好的是crc32。在实际项目中，要根据当前的数据场景来挑选合适的hash函数，这个选择有可能对整体性能影响较大。</p>
  </li>
  <li>
    <p><a href="http://www.csdn.net/article/2013-12-04/2817706--YARN">yarn</a> 最近yarn很火，我们公司的集群现在都逐渐从hadoop迁移到yarn(hadoop2.0)。这篇文章讲述阿里云梯的部署。阿里巴巴的Hadoop集群，即云梯集群，分为存储与计算两个模块，计算模块既有MRv1，也有YARN集群，它们共享一个存储HDFS集 群。云梯YARN集群上既支持MapReduce，也支持Spark、MPI、RHive、RHadoop等计算模型。为了访问数据的便捷性，阿里的存 储集群是一个单一的大集群，引入YARN不应迫使HDFS集群拆分。
该文章同时也讲到了spark，这个也算是大数据处理领域的新贵。他不仅在Batch、Interactive处理上可以取代map reduce和Impala，在Streaming上也可以取代storm.<a href="http://www.csdn.net/article/2014-01-27/2818282-Spark-Streaming-big-data">这篇文章</a>有相关的论述。</p>
  </li>
  <li>
    <p><a href="https://github.com/SamyPesse/How-to-Make-a-Computer-Operating-System">Make a Computer Operating System</a>  写一个操作系统，挺有趣的，有空的时候看看。</p>
  </li>
  <li><a href="http://webpy.org/">python web framework webpy</a> <a href="http://bottlepy.org/docs/dev/index.html">bottle</a> <a href="http://flask.pocoo.org/">flask</a> <a href="https://www.djangoproject.com/">django</a>
python的web框架有很多，上面这四个是我所接触过的，这其中，我用flask最多。这里有篇<a href="https://blog.tonyseek.com/post/discuss-about-flask-framework/">文章1</a> <a href="http://feilong.me/2011/01/talk-about-python-web-framework">文章2</a>比较了flask和其他。</li>
</ul>


  </article>

</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">100的情怀 - 技术博客</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col  footer-col-1">
        <ul class="contact-list">
          <li>100的情怀 - 技术博客</li>
          <li><a href="mailto:zero_based@foxmail.com">zero_based@foxmail.com</a></li>
        </ul>
      </div>

      <div class="footer-col  footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/zzbased">
              <span class="icon  icon--github">
                <svg viewBox="0 0 16 16">
                  <path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
                </svg>
              </span>

              <span class="username">zzbased</span>
            </a>
          </li>
          

          
          <li>
            <a href="https://twitter.com/zero_based">
              <span class="icon  icon--twitter">
                <svg viewBox="0 0 16 16">
                  <path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                  c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
                </svg>
              </span>

              <span class="username">zero_based</span>
            </a>
          </li>
          
        </ul>
      </div>

      <div class="footer-col  footer-col-3">
        <p class="text">机器学习，自然语言处理，计算广告学，工作与生活，总结与温习
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
